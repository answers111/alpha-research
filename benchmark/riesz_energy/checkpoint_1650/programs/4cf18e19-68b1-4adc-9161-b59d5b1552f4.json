{"id": "4cf18e19-68b1-4adc-9161-b59d5b1552f4", "code": "import numpy as np\nfrom numba import njit, prange\n\ndef equally_spaced(n: int):\n    \"\"\"Human-best configuration on [0,1] for any s>0.\"\"\"\n    if n <= 1:\n        return np.array([0.5])[:n]\n    return np.linspace(0.0, 1.0, n)\n\ndef jittered_baseline(n: int, seed: int = 0, jitter: float = 1e-3):\n    \"\"\"A simple baseline: equal grid + tiny jitter (still clipped to [0,1]).\"\"\"\n    rng = np.random.default_rng(seed)\n    xs = equally_spaced(n)\n    if n > 1:\n        xs += rng.uniform(-jitter, jitter, size=n)\n        xs = np.clip(xs, 0.0, 1.0)\n# Removed per-iteration sorting to save O(n log n) work\n    # (we only need the final ordering at the end)\n    xs.sort()\n    return xs\n\n@njit\ndef compute_energy(xs: np.ndarray, s: float = 1.0) -> float:\n    \"\"\"Compute Riesz s-energy via direct double loop (numba accelerated).\"\"\"\n    n = xs.size\n    if n < 2:\n        return 0.0\n    ene = 0.0\n    for i in range(n):\n        for j in range(i + 1, n):\n            dx = abs(xs[i] - xs[j])\n            ene += dx ** (-s)\n    return ene\n\n@njit(parallel=True)\ndef compute_grad(xs: np.ndarray, s: float = 1.0) -> np.ndarray:\n    \"\"\"Compute gradient of Riesz s-energy via explicit loops (numba accelerated).\"\"\"\n    n = xs.size\n    grad = np.zeros(n)\n    if n < 2:\n        return grad\n    for i in prange(n):\n        g = 0.0\n        xi = xs[i]\n        for j in range(n):\n            if i != j:\n                dx = xi - xs[j]\n                # derivative of |dx|^{-s} is -s * |dx|^{-s-1} * sign(dx)\n                g -= s * (abs(dx) ** (-s - 1)) * np.sign(dx)\n        grad[i] = g\n    return grad\n\ndef optimize(xs: np.ndarray, s: float = 1.0, lr: float = 1e-2, iters: int = 2000, tol: float = 1e-8, momentum: float = 0.9) -> np.ndarray:\n    \"\"\"Projected gradient descent with momentum, backtracking line search and lr decay.\"\"\"\n    xs = xs.copy()\n    prev_energy = compute_energy(xs, s)\n    # initialize momentum buffer\n    v = np.zeros_like(xs)\n    for k in range(1, iters + 1):\n        # preallocate a workspace for candidate points to avoid repeated allocations\n        xs_new = np.empty_like(xs)\n        g = compute_grad(xs, s)\n        # apply simple momentum smoothing\n        v = momentum * v + (1.0 - momentum) * g\n        grad_norm = np.linalg.norm(v)\n        if grad_norm < tol:\n            break\n        step = lr\n        # backtracking to ensure energy decreases\n        # Armijo backtracking line search: f(x+\u03b1p) \u2264 f(x) + c1\u00b7\u03b1\u00b7\u27e8\u2207f, p\u27e9\n        dir_deriv = -np.dot(g, v)\n        c1 = 1e-4\n        for _ in range(8):\n            # xs_new \u2190 clipped candidate\n            xs_new[:] = xs - step * v\n            np.clip(xs_new, 0.0, 1.0, out=xs_new)\n            new_energy = compute_energy(xs_new, s)\n            if new_energy <= prev_energy + c1 * step * dir_deriv:\n                xs = xs_new\n                prev_energy = new_energy\n                break\n            step *= 0.5\n        else:\n            # no acceptable step found \u2192 stop early\n            break\n        # gentle decay for faster convergence once in regime\n        lr *= 0.995\n    xs.sort()\n    return xs\n\ndef main():\n    n = 20\n    s = 1.0\n    # initialize with equally spaced points\n    xs_init = jittered_baseline(n, seed=0, jitter=1e-3)\n    # refine via projected gradient descent\n    xs_local = optimize(xs_init, s, lr=1e-2, iters=2000)\n    # report final energy\n    print(\"Final Riesz s-energy:\", compute_energy(xs_local, s))\n    return xs_local\n", "language": "python", "proposal": ["Title: Multi-Resolution Spectral\u2013Preconditioned Newton Framework for High-Precision 1D Riesz s-Energy Minimization\n\nAbstract:\nWe present a novel, implementable algorithm that overcomes the O(n\u00b2) gradient cost and slow local convergence of existing projected gradient\u2013descent approaches for 1D Riesz s-energy minimization. By integrating insights from fast multipole methods, spectral preconditioning, and limited-memory Newton-type solvers, our framework achieves both subquadratic per\u2010iteration cost and rapid descent to high\u2010precision minima. The key components are:\n\n1. FFT-Accelerated Gradient and Hessian-Vector Products  \n   \u2013 Replace explicit pairwise loops with FFT-based convolution of the empirical density against the |x|\u207b(s+1) kernel.  \n   \u2013 Compute both gradient and Hessian-vector products in O(n\u2009log\u2009n) time, with a tunable spectral cutoff to control aliasing errors.\n\n2. Spectral Preconditioner and Truncated Newton Steps  \n   \u2013 Construct a diagonal spectral preconditioner by approximating the inverse Riesz Hessian in the Fourier domain.  \n   \u2013 Employ a limited-memory truncated-Newton (L-BFGS/ Hessian-free) update within an adaptive trust-region to accelerate convergence and escape shallow basins.\n\n3. Low-Discrepancy Multi-Start with s-Continuation  \n   \u2013 Initialize multiple Sobol-sequence seeds perturbed by jitter and evaluate a cheap smoothed-energy proxy to select top-k starts.  \n   \u2013 Incorporate an s-continuation schedule: begin with a larger exponent (s_high) for a smoother landscape, then gradually decrease to the target s to avoid poor local minima.\n\n4. Strong Wolfe Projected Line Search and Multi-Resolution Refinement  \n   \u2013 Enforce sufficient decrease via a projected strong Wolfe line search on [0,1], dynamically adjusting tolerances for efficiency.  \n   \u2013 Employ a two-stage mesh refinement: solve first on a coarser grid (n_coarse) then interpolate solutions to the full resolution for final polishing.\n\nImplementation and Evaluation Plan:\nWe will realize our method in Python, using Numba\u2010JIT for FFT convolution kernels and Hessian\u2010vector routines, SciPy\u2019s FFTW/BLAS backends, and optional CuPy acceleration. Benchmarks across n\u2208[20,1000] and s\u2208[0.5,2.0] will measure final energy accuracy, iteration count, and wall-clock time against the O(n\u00b2) projected gradient baseline. We anticipate at least a 10\u00d7 reduction in per-iteration cost, a 10\u00d7 faster convergence to 1e-10 energy precision, and practical scalability to n\u226510\u2074."], "idea_reward": 6.0, "parent_id": "e388aac4-3ada-4437-b402-381072aaa207", "generation": 8, "timestamp": 1757782608.9843602, "iteration_found": 549, "metrics": {"1 / riesz_energy_n20_s1": 0.001023201009661636}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 2 lines with 4 lines\nChange 2: Replace 11 lines with 16 lines\nChange 3: Replace 2 lines with 2 lines", "parent_metrics": {"1 / riesz_energy_n20_s1": 0.0010231567557196194}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: Spectral-Accelerated Quasi-Newton Framework for Efficient 1D Riesz s-Energy Minimization\n\nAbstract:\nWe address the scalability and convergence limitations of the existing projected gradient\u2013descent solver for minimizing 1D Riesz s-energy, which exhibits O(n\u00b2) per-iteration cost and slow descent toward optimal configurations (e.g., final energy \u22481.0\u00d710\u207b\u00b3 for n=20, s=1). Drawing inspiration from fast multipole and spectral methods in N-body simulation and from limited-memory quasi-Newton optimizers in machine learning, we propose a novel, implementable algorithm with the following key innovations:\n\n1. Spectral Force Computation via Convolution:\n   \u2022 Replace explicit double loops with FFT-based convolution of the particle density against the |x|\u207b(s+1) kernel.  \n   \u2022 Achieve O(n\u2009log\u2009n) gradient evaluations with controllable aliasing error through kernel smoothing.\n\n2. Limited-Memory Quasi-Newton Updates and Preconditioning:\n   \u2022 Integrate an L-BFGS optimizer with a diagonal preconditioner derived from the approximate Hessian\u2019s local curvature.  \n   \u2022 Employ Nesterov-style momentum and adaptive step scaling to accelerate convergence and escape shallow local minima.\n\n3. Low-Discrepancy Multi-Start Initialization:\n   \u2022 Generate multiple seed configurations using Sobol sequences perturbed by small random jitter, ensuring broad coverage of the feasible set [0,1].  \n   \u2022 Select the best candidates via a lightweight Riesz energy estimate and refine only the top-k, reducing wasted computation on poor basins.\n\n4. Robust Projected Line Search:\n   \u2022 Implement a strong Wolfe\u2013condition line search on the projected domain [0,1] to guarantee sufficient energy decrease and global convergence properties.  \n   \u2022 Dynamically adjust tolerance thresholds to balance precision and runtime.\n\nImplementation Plan:\nWe will develop the method in Python, leveraging Numba for JIT compilation of FFT-based convolutions and gradient\u2013Hessian-vector products, and scipy\u2019s low-level BLAS/FFT backend...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numba import njit, prange\n\ndef equally_spaced(n: int):\n    \"\"\"Human-best configuration on [0,1] for any s>0.\"\"\"\n    if n <= 1:\n        return np.array([0.5])[:n]\n    return np.linspace(0.0, 1.0, n)\n\ndef jittered_baseline(n: int, seed: int = 0, jitter: float = 1e-3):\n    \"\"\"A simple baseline: equal grid + tiny jitter (still clipped to [0,1]).\"\"\"\n    rng = np.random.default_rng(seed)\n    xs = equally_spaced(n)\n    if n > 1:\n        xs += rng.uniform(-jitter, jitter, size=n)\n        xs = np.clip(xs, 0.0, 1.0)\n# Removed per-iteration sorting to save O(n log n) work\n    # (we only need the final ordering at the end)\n    xs.sort()\n    return xs\n\n@njit\ndef compute_energy(xs: np.ndarray, s: float = 1.0) -> float:\n    \"\"\"Compute Riesz s-energy via direct double loop (numba accelerated).\"\"\"\n    n = xs.size\n    if n < 2:\n        return 0.0\n    ene = 0.0\n    for i in range(n):\n        for j in range(i + 1, n):\n            dx = abs(xs[i] - xs[j])\n            ene += dx ** (-s)\n    return ene\n\n@njit(parallel=True)\ndef compute_grad(xs: np.ndarray, s: float = 1.0) -> np.ndarray:\n    \"\"\"Compute gradient of Riesz s-energy via explicit loops (numba accelerated).\"\"\"\n    n = xs.size\n    grad = np.zeros(n)\n    if n < 2:\n        return grad\n    for i in prange(n):\n        g = 0.0\n        xi = xs[i]\n        for j in range(n):\n            if i != j:\n                dx = xi - xs[j]\n                # derivative of |dx|^{-s} is -s * |dx|^{-s-1} * sign(dx)\n                g -= s * (abs(dx) ** (-s - 1)) * np.sign(dx)\n        grad[i] = g\n    return grad\n\ndef optimize(xs: np.ndarray, s: float = 1.0, lr: float = 1e-2, iters: int = 2000, tol: float = 1e-8, momentum: float = 0.9) -> np.ndarray:\n    \"\"\"Projected gradient descent with momentum, backtracking line search and lr decay.\"\"\"\n    xs = xs.copy()\n    prev_energy = compute_energy(xs, s)\n    # initialize momentum buffer\n    v = np.zeros_like(xs)\n    for k in range(1, iters + 1):\n        g = compute_grad(xs, s)\n        # apply simple momentum smoothing\n        v = momentum * v + (1.0 - momentum) * g\n        grad_norm = np.linalg.norm(v)\n        if grad_norm < tol:\n            break\n        step = lr\n        # backtracking to ensure energy decreases\n        for _ in range(8):\n            xs_new = np.clip(xs - step * v, 0.0, 1.0)\n            new_energy = compute_energy(xs_new, s)\n            if new_energy < prev_energy:\n                xs = xs_new\n                prev_energy = new_energy\n                break\n            step *= 0.5\n        else:\n            # no decrease found \u2192 stop early\n            break\n        # gentle decay for stability\n        lr *= 0.999\n    xs.sort()\n    return xs\n\ndef main():\n    n = 20\n    s = 1.0\n    # initialize with equally spaced points\n    xs_init = jittered_baseline(n, seed=0, jitter=1e-3)\n    # refine via projected gradient descent\n    xs_local = optimize(xs_init, s, lr=1e-2, iters=2000)\n    # report final energy\n    print(\"Final Riesz s-energy:\", compute_energy(xs_local, s))\n    return xs_local\n\n```\n\n# Previous Performance Metrics: \n- 1 / riesz_energy_n20_s1: 0.0010\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: 1 / riesz_energy_n20_s1. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: 1 / riesz_energy_n20_s1: 0.0010\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: 1 / riesz_energy_n20_s1: 0.0010\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: 1 / riesz_energy_n20_s1: 0.0010\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0010)\n```python\nimport numpy as np\nfrom numba import njit, prange\n\ndef equally_spaced(n: int):\n    \"\"\"Human-best configuration on [0,1] for any s>0.\"\"\"\n    if n <= 1:\n        return np.array([0.5])[:n]\n    return np.linspace(0.0, 1.0, n)\n\ndef jittered_baseline(n: int, seed: int = 0, jitter: float = 1e-3):\n# ... (truncated for brevity)\n```\nKey features: Performs well on 1 / riesz_energy_n20_s1 (0.0010)\n\n\n### Program 2 (Score: 0.0009)\n```python\nimport numpy as np\n\n# Equally spaced and jittered point generator for baseline energies\ndef equally_spaced(n: int) -> np.ndarray:\n    \"\"\"Return n baseline Riesz-s points in [0,1] using Chebyshev nodes.\"\"\"\n    if n < 0:\n        raise ValueError(\"n must be non-negative\")\n    if n == 0:\n        return np.empty(0)\n    if n == 1:\n# ... (truncated for brevity)\n```\nKey features: Performs well on 1 / riesz_energy_n20_s1 (0.0009)\n\n\n### Program 3 (Score: 0.0010)\n```python\nimport numpy as np\ndef equally_spaced(n: int, jitter: float = 0.0, seed: int = None) -> np.ndarray:\n    \"\"\"\n    Return n Chebyshev-spaced points on [0,1], with optional jitter.\n\n    Args:\n        n (int): number of points (np.linspace handles n <= 1).\n        jitter (float): max magnitude of uniform perturbation added to each point.\n        seed (int | None): random seed for reproducibility.\n\n# ... (truncated for brevity)\n```\nKey features: Performs well on 1 / riesz_energy_n20_s1 (0.0010)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.0010)\n```python\nimport numpy as np\n\n# Equally spaced and jittered point generator for baseline energies\ndef equally_spaced(n: int) -> np.ndarray:\n    \"\"\"Return n equally spaced points in [0,1]. For n==1, returns [0.5].\"\"\"\n# ... (truncated)\n```\nKey features: Alternative approach to 1 / riesz_energy_n20_s1\n\n\n### Program D2 (Score: 0.0010)\n```python\nimport numpy as np\nfrom numba import njit, prange  # re-enable Numba for JIT acceleration\n\ndef equally_spaced(n: int):\n    \"\"\"Human-best configuration on [0,1] for any s>0.\"\"\"\n# ... (truncated)\n```\nKey features: Alternative approach to 1 / riesz_energy_n20_s1\n\n\n# Current Proposal\nTitle: Multi-Resolution Spectral\u2013Preconditioned Newton Framework for High-Precision 1D Riesz s-Energy Minimization\n\nAbstract:\nWe present a novel, implementable algorithm that overcomes the O(n\u00b2) gradient cost and slow local convergence of existing projected gradient\u2013descent approaches for 1D Riesz s-energy minimization. By integrating insights from fast multipole methods, spectral preconditioning, and limited-memory Newton-type solvers, our framework achieves both subquadratic per\u2010iteration cost and rapid descent to high\u2010precision minima. The key components are:\n\n1. FFT-Accelerated Gradient and Hessian-Vector Products  \n   \u2013 Replace explicit pairwise loops with FFT-based convolution of the empirical density against the |x|\u207b(s+1) kernel.  \n   \u2013 Compute both gradient and Hessian-vector products in O(n\u2009log\u2009n) time, with a tunable spectral cutoff to control aliasing errors.\n\n2. Spectral Preconditioner and Truncated Newton Steps  \n   \u2013 Construct a diagonal spectral preconditioner by approximating the inverse Riesz Hessian in the Fourier domain.  \n   \u2013 Employ a limited-memory truncated-Newton (L-BFGS/ Hessian-free) update within an adaptive trust-region to accelerate convergence and escape shallow basins.\n\n3. Low-Discrepancy Multi-Start with s-Continuation  \n   \u2013 Initialize multiple Sobol-sequence seeds perturbed by jitter and evaluate a cheap smoothed-energy proxy to select top-k starts.  \n   \u2013 Incorporate an s-continuation schedule: begin with a larger exponent (s_high) for a smoother landscape, then gradually decrease to the target s to avoid poor local minima.\n\n4. Strong Wolfe Projected Line Search and Multi-Resolution Refinement  \n   \u2013 Enforce sufficient decrease via a projected strong Wolfe line search on [0,1], dynamically adjusting tolerances for efficiency.  \n   \u2013 Employ a two-stage mesh refinement: solve first on a coarser grid (n_coarse) then interpolate solutions to the full resolution for final polishing.\n\nImplementation and Evaluation Plan:\nWe will realize o...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are three targeted diffs to reduce unnecessary allocations in the line\u2010search, add a proper Armijo condition (so we don\u2019t take too many tiny steps), and gently speed up learning\u2010rate decay. Together these should lower the per\u2010iteration cost and improve convergence (i.e. reduce the final Riesz energy).\n\n1) Preallocate the `xs_new` buffer once per iteration rather than reallocating inside the backtracking loop.\n\n<<<<<<< SEARCH\n    for k in range(1, iters + 1):\n        g = compute_grad(xs, s)\n=======\n    for k in range(1, iters + 1):\n        # preallocate a workspace for candidate points to avoid repeated allocations\n        xs_new = np.empty_like(xs)\n        g = compute_grad(xs, s)\n>>>>>>> REPLACE\n\n2) Replace the simple \u201cany decrease\u201d test with an Armijo (sufficient\u2010decrease) check, using our preallocated `xs_new`. This avoids overly small steps and can cut down on backtracks.\n\n<<<<<<< SEARCH\n        for _ in range(8):\n            xs_new = np.clip(xs - step * v, 0.0, 1.0)\n            new_energy = compute_energy(xs_new, s)\n            if new_energy < prev_energy:\n                xs = xs_new\n                prev_energy = new_energy\n                break\n            step *= 0.5\n        else:\n            # no decrease found \u2192 stop early\n            break\n=======\n        # Armijo backtracking line search: f(x+\u03b1p) \u2264 f(x) + c1\u00b7\u03b1\u00b7\u27e8\u2207f, p\u27e9\n        dir_deriv = -np.dot(g, v)\n        c1 = 1e-4\n        for _ in range(8):\n            # xs_new \u2190 clipped candidate\n            xs_new[:] = xs - step * v\n            np.clip(xs_new, 0.0, 1.0, out=xs_new)\n            new_energy = compute_energy(xs_new, s)\n            if new_energy <= prev_energy + c1 * step * dir_deriv:\n                xs = xs_new\n                prev_energy = new_energy\n                break\n            step *= 0.5\n        else:\n            # no acceptable step found \u2192 stop early\n            break\n>>>>>>> REPLACE\n\n3) Slightly increase the learning\u2010rate decay so we cool faster once we have good descent directions:\n\n<<<<<<< SEARCH\n        # gentle decay for stability\n        lr *= 0.999\n=======\n        # gentle decay for faster convergence once in regime\n        lr *= 0.995\n>>>>>>> REPLACE\n\nReasoning:\n\n- Preallocating `xs_new` and using `np.clip(..., out=...)` removes an array allocation in each backtrack.\n- The Armijo rule prevents tiny, unproductive steps by ensuring we only accept steps that yield sufficient decrease based on the directional derivative.\n- A slightly faster learning\u2010rate decay can help settle into a precise minimum more quickly once we\u2019re in the right basin."]}}}