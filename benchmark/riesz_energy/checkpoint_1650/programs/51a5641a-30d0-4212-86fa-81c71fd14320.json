{"id": "51a5641a-30d0-4212-86fa-81c71fd14320", "code": "import numpy as np\nfrom numba import njit, prange\n\ndef equally_spaced(n: int) -> np.ndarray:\n    \"\"\"Return n equally spaced points in [0,1], using midpoint for n<=1.\"\"\"\n    if n <= 1:\n        return np.array([0.5])[:n]\n    return np.linspace(0.0, 1.0, n)\n\ndef jittered_baseline(n: int, seed: int = 0, jitter: float = 1e-3) -> np.ndarray:\n    \"\"\"Return n equally spaced points in [0,1] with small random jitter.\"\"\"\n    rng = np.random.default_rng(seed)\n    xs = equally_spaced(n)\n    if n > 1:\n        xs = np.clip(xs + rng.uniform(-jitter, jitter, size=n), 0.0, 1.0)\n        xs.sort()\n    return xs\n\n@njit(cache=True, parallel=True, fastmath=True)\ndef compute_energy(xs: np.ndarray, s: float = 1.0) -> float:\n    \"\"\"Compute Riesz s-energy via direct double loop (numba accelerated).\"\"\"\n    n = xs.size\n    if n < 2:\n        return 0.0\n    ene = 0.0\n    for i in prange(n):\n        # Only sum j>i to avoid double counting\n        for j in range(i + 1, n):\n            dx = xs[j] - xs[i]\n            # use absolute distances so xs need not be sorted\n            ene += dx ** (-s)\n    return ene\n\n@njit(cache=True, parallel=True, fastmath=True)\ndef compute_grad(xs: np.ndarray, s: float = 1.0) -> np.ndarray:\n    \"\"\"Compute gradient of Riesz s-energy via two-range loops (numba accelerated).\"\"\"\n    n = xs.size\n    grad = np.zeros(n)\n    if n < 2:\n        return grad\n    for i in prange(n):\n        g = 0.0\n        xi = xs[i]\n        # combine contributions from all other points (unsorted support)\n        # contributions from points before i (sorted xs)\n        for j in range(i):\n            dx = xi - xs[j]\n            g -= s * dx ** (-s - 1)\n        # contributions from points after i (sorted xs)\n        for j in range(i + 1, n):\n            dx = xs[j] - xi\n            g += s * dx ** (-s - 1)\n        grad[i] = g\n    return grad\n\ndef optimize(xs: np.ndarray, s: float = 1.0, lr: float = 1e-2, iters: int = 2000, tol: float = 1e-8, momentum: float = 0.9) -> np.ndarray:\n    \"\"\"Nesterov-accelerated projected gradient descent with adaptive backtracking.\"\"\"\n    xs = xs.copy()\n    xs_prev = xs.copy()\n    prev_energy = compute_energy(xs, s)\n    for k in range(1, iters + 1):\n        # Nesterov momentum extrapolation\n        y = xs + momentum * (xs - xs_prev)\n        # ensure sorted y for operator consistency\n        y = np.sort(y)\n        # compute gradient at extrapolated point\n        g = compute_grad(y, s)\n        grad_norm = np.linalg.norm(g)\n        if grad_norm < tol:\n            break\n        step = lr\n        # backtracking to ensure energy decreases\n        for _ in range(8):\n            xs_new = np.clip(y - step * g, 0.0, 1.0)\n            xs_new.sort()\n            # no sort needed; compute_energy now uses abs(dx)\n            new_energy = compute_energy(xs_new, s)\n            if new_energy < prev_energy:\n                xs_prev = xs\n                xs = xs_new\n                prev_energy = new_energy\n                break\n            step *= 0.5\n        else:\n            # no decrease found: reduce momentum and learning rate, then retry\n            momentum *= 0.5\n            lr *= 0.5\n            continue\n        # gentle decay of learning rate\n        lr *= 0.995\n    # no final sort needed\n    return xs\n\ndef main():\n    n = 20\n    s = 1.0\n    # initialize with equally spaced points\n    xs_init = jittered_baseline(n, seed=0, jitter=1e-3)\n    # refine via projected gradient descent\n    xs_local = optimize(xs_init, s, lr=1e-2, iters=500, momentum=0.9)\n    # report final energy\n    print(\"Final Riesz s-energy:\", compute_energy(xs_local, s))\n    return xs_local\n\nif __name__ == \"__main__\":\n    main()\n", "language": "python", "proposal": ["Title: Hierarchical Hessian-Free Quasi-Newton with FMM-Accelerated Operators, Meta-Learned Preconditioning, and Bayesian Trust-Region Spectral Restarts for Sub-10\u207b\u2078 Convergence in 1D Riesz s-Energy Minimization\n\nAbstract:\nState-of-the-art first-order and limited-memory quasi-Newton methods for 1D Riesz s-energy problems stall around residual \u223c1\u00d710\u207b\u00b3 for n=20, incur O(n\u00b2) interaction costs, and require laborious hyperparameter tuning. We propose an implementable, fully automated framework that bridges this precision-performance gap by uniting four key innovations:\n\n1. Hierarchical FMM-Accelerated Hessian-Free Newton-CG  \n   \u2013 Compute far-field energy, gradient, and Hessian-vector products in O(n\u2009log\u2009n) via GPU-optimized FMM kernels.  \n   \u2013 Embed these operators into a Newton-CG loop that discards explicit Hessian storage, reducing per-iteration cost by 10\u00d7\u201350\u00d7 and enabling direct second-order steps to sub-micron precision.\n\n2. Meta-Learned Multi-Scale Preconditioners  \n   \u2013 Train a lightweight graph neural network on low- and mid-scale Riesz optima to predict diagonal and block-Schur complement preconditioners across arbitrary n.  \n   \u2013 Inject these priors into the CG inner solver, slashing CG iterations by up to 70% and removing hand-tuned damping parameters.\n\n3. Randomized Spectral Anderson Acceleration with Adaptive s-Continuation  \n   \u2013 Sketch local Hessian spectra on the fly using randomized SVD to extract dominant eigenmodes.  \n   \u2013 Apply Anderson mixing along these modes and drive an adaptive s-continuation schedule via a low-overhead PID controller that automatically enlarges s-steps in well-conditioned regimes.\n\n4. Gaussian-Process-Guided Trust-Region Multi-Start with Spectral Restart  \n   \u2013 Maintain a low-rank GP surrogate in the evolving Krylov subspace to propose informed trust-region radii and restart locations.  \n   \u2013 Use eigenvalue clustering criteria to trigger spectral restarts, ensuring diverse global searches and avoiding redundant local traps.\n\nOur JAX/PyTorch prototype, leveraging pyFMM backends and in-kernel GNN modules, is expected to:\n\u2013 Reach residuals <1\u00d710\u207b\u2078 for n=20 (\u223c1,000\u00d7 improvement),  \n\u2013 Scale near-linearly to n=10\u2076 on distributed GPU clusters,  \n\u2013 Self-tune all algorithmic parameters without manual intervention.  \n\nThis modular, fully implementable approach establishes a new accuracy-efficiency frontier for 1D Riesz energy minimization and directly extends to higher-dimensional repulsive systems."], "idea_reward": 6.67, "parent_id": "8800a8e1-551a-4f78-a979-4ccc17e0925b", "generation": 14, "timestamp": 1757828199.6213374, "iteration_found": 1608, "metrics": {"1 / riesz_energy_n20_s1": 0.0010245053059113726}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: 'ene += abs(dx) ** (-s)' to 'ene += dx ** (-s)'\nChange 2: Replace 6 lines with 8 lines\nChange 3: Replace 2 lines with 4 lines\nChange 4: Replace xs_new = np.clip(y - step * g, 0.0, 1.0) with 2 lines", "parent_metrics": {"1 / riesz_energy_n20_s1": 0.0010245053059113724}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: Meta\u2010Preconditioned Hessian\u2010Free Quasi\u2010Newton with FMM\u2010Accelerated Operators and Spectral Restart for Sub-Microprecision 1D Riesz s-Energy Minimization\n\nAbstract:\nState\u2010of\u2010the\u2010art first\u2010order solvers for 1D Riesz s\u2010energy problems stall at residual ~1\u00d710\u207b\u00b3 for n=20, incur quadratic costs in energy/gradient evaluations, and rely on hand\u2010tuned hyperparameters. We propose a novel, fully implementable framework that closes this precision\u2010performance gap by uniting four key innovations:\n\n1. FMM-Accelerated Hessian\u2010Vector Products  \n   \u2022 Replace direct O(n\u00b2) loops with a customized CUDA/PyCUDA FMM kernel, delivering exact energy, gradient, and Hessian\u2010vector actions in O(n\u2009log\u2009n).  \n   \u2022 Embed these fast operators into a Hessian\u2010free conjugate\u2010gradient (CG) solver within each quasi\u2010Newton step, eliminating explicit matrix storage and reducing per\u2010iteration cost by 10\u00d7\u201350\u00d7.\n\n2. Meta-Learned Diagonal and Block-Diagonal Curvature Priors  \n   \u2022 Train a lightweight graph neural network on low-dimensional Riesz optima to predict both diagonal and block-diagonal preconditioners for arbitrary n.  \n   \u2022 Integrate these priors into a limited\u2010memory BFGS update, accelerating local curvature capture and cutting iteration counts by up to 80%.\n\n3. Spectrally Adaptive s-Continuation with Online Eigenvalue Control  \n   \u2022 Use randomized block Lanczos sketches on the fly to estimate the local Hessian spectrum and identify spectral gaps.  \n   \u2022 Drive an adaptive s-continuation schedule via a low-overhead controller (e.g., PID or lightweight RL), automatically enlarging s steps when the condition number is favorable and slowing when nearing stiff regimes.\n\n4. Bayesian Trust-Region Multi-Start with Spectral Restart  \n   \u2022 Maintain a low\u2010rank Gaussian\u2010process surrogate in the evolving Krylov subspace to propose informed restarts and adjust trust-region radii.  \n   \u2022 Implement spectral restart criteria based on eigenvalue clustering to avoid redundant explorations and ensure diverse glob...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numba import njit, prange\n\ndef equally_spaced(n: int) -> np.ndarray:\n    \"\"\"Return n equally spaced points in [0,1], using midpoint for n<=1.\"\"\"\n    if n <= 1:\n        return np.array([0.5])[:n]\n    return np.linspace(0.0, 1.0, n)\n\ndef jittered_baseline(n: int, seed: int = 0, jitter: float = 1e-3) -> np.ndarray:\n    \"\"\"Return n equally spaced points in [0,1] with small random jitter.\"\"\"\n    rng = np.random.default_rng(seed)\n    xs = equally_spaced(n)\n    if n > 1:\n        xs = np.clip(xs + rng.uniform(-jitter, jitter, size=n), 0.0, 1.0)\n        xs.sort()\n    return xs\n\n@njit(cache=True, parallel=True, fastmath=True)\ndef compute_energy(xs: np.ndarray, s: float = 1.0) -> float:\n    \"\"\"Compute Riesz s-energy via direct double loop (numba accelerated).\"\"\"\n    n = xs.size\n    if n < 2:\n        return 0.0\n    ene = 0.0\n    for i in prange(n):\n        # Only sum j>i to avoid double counting\n        for j in range(i + 1, n):\n            dx = xs[j] - xs[i]\n            # use absolute distances so xs need not be sorted\n            ene += abs(dx) ** (-s)\n    return ene\n\n@njit(cache=True, parallel=True, fastmath=True)\ndef compute_grad(xs: np.ndarray, s: float = 1.0) -> np.ndarray:\n    \"\"\"Compute gradient of Riesz s-energy via two-range loops (numba accelerated).\"\"\"\n    n = xs.size\n    grad = np.zeros(n)\n    if n < 2:\n        return grad\n    for i in prange(n):\n        g = 0.0\n        xi = xs[i]\n        # combine contributions from all other points (unsorted support)\n        for j in range(n):\n            if j == i:\n                continue\n            dx = xi - xs[j]\n            # derivative: -s * |dx|^{-s-1} * sign(dx)\n            g -= s * (dx / abs(dx)) * (abs(dx) ** (-s - 1))\n        grad[i] = g\n    return grad\n\ndef optimize(xs: np.ndarray, s: float = 1.0, lr: float = 1e-2, iters: int = 2000, tol: float = 1e-8, momentum: float = 0.9) -> np.ndarray:\n    \"\"\"Nesterov-accelerated projected gradient descent with adaptive backtracking.\"\"\"\n    xs = xs.copy()\n    xs_prev = xs.copy()\n    prev_energy = compute_energy(xs, s)\n    for k in range(1, iters + 1):\n        # Nesterov momentum extrapolation\n        y = xs + momentum * (xs - xs_prev)\n        # compute gradient at extrapolated point\n        g = compute_grad(y, s)\n        grad_norm = np.linalg.norm(g)\n        if grad_norm < tol:\n            break\n        step = lr\n        # backtracking to ensure energy decreases\n        for _ in range(8):\n            xs_new = np.clip(y - step * g, 0.0, 1.0)\n            # no sort needed; compute_energy now uses abs(dx)\n            new_energy = compute_energy(xs_new, s)\n            if new_energy < prev_energy:\n                xs_prev = xs\n                xs = xs_new\n                prev_energy = new_energy\n                break\n            step *= 0.5\n        else:\n            # no decrease found: reduce momentum and learning rate, then retry\n            momentum *= 0.5\n            lr *= 0.5\n            continue\n        # gentle decay of learning rate\n        lr *= 0.995\n    # no final sort needed\n    return xs\n\ndef main():\n    n = 20\n    s = 1.0\n    # initialize with equally spaced points\n    xs_init = jittered_baseline(n, seed=0, jitter=1e-3)\n    # refine via projected gradient descent\n    xs_local = optimize(xs_init, s, lr=1e-2, iters=500, momentum=0.9)\n    # report final energy\n    print(\"Final Riesz s-energy:\", compute_energy(xs_local, s))\n    return xs_local\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Previous Performance Metrics: \n- 1 / riesz_energy_n20_s1: 0.0010\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: 1 / riesz_energy_n20_s1. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: 1 / riesz_energy_n20_s1: 0.0010\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: 1 / riesz_energy_n20_s1: 0.0010\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: 1 / riesz_energy_n20_s1: 0.0010\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0010)\n```python\nimport numpy as np\nfrom numba import njit, prange\nfrom scipy.optimize import minimize\n\ndef equally_spaced(n: int) -> np.ndarray:\n    \"\"\"Return n equally spaced points on [0,1] using numpy.linspace.\"\"\"\n    # np.linspace gracefully handles n=0 (\u2192 []), n=1 (\u2192 [0.0]), n>1\n    return np.linspace(0.0, 1.0, n)\n\ndef jittered_baseline(n: int, seed: int = 0, jitter: float = 1e-3):\n# ... (truncated for brevity)\n```\nKey features: Performs well on 1 / riesz_energy_n20_s1 (0.0010)\n\n\n### Program 2 (Score: 0.0010)\n```python\nimport numpy as np\nfrom numba import njit, prange\nfrom scipy.optimize import minimize\n\ndef equally_spaced(n: int) -> np.ndarray:\n    \"\"\"Return n equally spaced points on [0,1]. For n<=1, place at midpoint.\"\"\"\n    if n <= 1:\n        return np.array([0.5])[:n]\n    # numpy.linspace handles n>1 reliably\n    return np.linspace(0.0, 1.0, n)\n# ... (truncated for brevity)\n```\nKey features: Performs well on 1 / riesz_energy_n20_s1 (0.0010)\n\n\n### Program 3 (Score: 0.0010)\n```python\nimport numpy as np\nfrom numba import njit, prange\n\ndef equally_spaced(n: int) -> np.ndarray:\n    \"\"\"Return n equally spaced points on [0,1].\"\"\"\n    return np.linspace(0.0, 1.0, n)\n\ndef jittered_baseline(n: int, seed: int = 0, jitter: float = 1e-3):\n    \"\"\"A simple baseline: equal grid + tiny jitter (still clipped to [0,1]).\"\"\"\n    rng = np.random.default_rng(seed)\n# ... (truncated for brevity)\n```\nKey features: Performs well on 1 / riesz_energy_n20_s1 (0.0010)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.0010)\n```python\nimport numpy as np\nfrom numba import njit, prange\n\ndef equally_spaced(n: int) -> np.ndarray:\n    \"\"\"Return n equally spaced points in [0,1], using midpoint for n<=1.\"\"\"\n# ... (truncated)\n```\nKey features: Alternative approach to 1 / riesz_energy_n20_s1\n\n\n### Program D2 (Score: 0.0010)\n```python\nimport numpy as np\nfrom numba import njit, prange\nfrom scipy.optimize import minimize\n\ndef equally_spaced(n: int) -> np.ndarray:\n# ... (truncated)\n```\nKey features: Alternative approach to 1 / riesz_energy_n20_s1\n\n\n# Current Proposal\nTitle: Hierarchical Hessian-Free Quasi-Newton with FMM-Accelerated Operators, Meta-Learned Preconditioning, and Bayesian Trust-Region Spectral Restarts for Sub-10\u207b\u2078 Convergence in 1D Riesz s-Energy Minimization\n\nAbstract:\nState-of-the-art first-order and limited-memory quasi-Newton methods for 1D Riesz s-energy problems stall around residual \u223c1\u00d710\u207b\u00b3 for n=20, incur O(n\u00b2) interaction costs, and require laborious hyperparameter tuning. We propose an implementable, fully automated framework that bridges this precision-performance gap by uniting four key innovations:\n\n1. Hierarchical FMM-Accelerated Hessian-Free Newton-CG  \n   \u2013 Compute far-field energy, gradient, and Hessian-vector products in O(n\u2009log\u2009n) via GPU-optimized FMM kernels.  \n   \u2013 Embed these operators into a Newton-CG loop that discards explicit Hessian storage, reducing per-iteration cost by 10\u00d7\u201350\u00d7 and enabling direct second-order steps to sub-micron precision.\n\n2. Meta-Learned Multi-Scale Preconditioners  \n   \u2013 Train a lightweight graph neural network on low- and mid-scale Riesz optima to predict diagonal and block-Schur complement preconditioners across arbitrary n.  \n   \u2013 Inject these priors into the CG inner solver, slashing CG iterations by up to 70% and removing hand-tuned damping parameters.\n\n3. Randomized Spectral Anderson Acceleration with Adaptive s-Continuation  \n   \u2013 Sketch local Hessian spectra on the fly using randomized SVD to extract dominant eigenmodes.  \n   \u2013 Apply Anderson mixing along these modes and drive an adaptive s-continuation schedule via a low-overhead PID controller that automatically enlarges s-steps in well-conditioned regimes.\n\n4. Gaussian-Process-Guided Trust-Region Multi-Start with Spectral Restart  \n   \u2013 Maintain a low-rank GP surrogate in the evolving Krylov subspace to propose informed trust-region radii and restart locations.  \n   \u2013 Use eigenvalue clustering criteria to trigger spectral restarts, ensuring diverse global searches and avoiding redundant local traps.\n\nOu...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are four small, targeted diffs that (1) eliminate expensive `abs`/`sign` operations by enforcing a sorted\u2010points invariant, (2) sort the extrapolated and updated points so the invariant holds, and (3) simplify the Hessian\u2010free gradient loops. Together these changes remove branches and costly absolute\u2010value calls, which should reduce per\u2010iteration overhead and improve the energy convergence.\n\n1) In `compute_energy`, drop the `abs` now that `xs` is always sorted:\n\n<<<<<<< SEARCH\n            ene += abs(dx) ** (-s)\n=======\n            ene += dx ** (-s)\n>>>>>>> REPLACE\n\n2) In `compute_grad`, replace the full\u2010loop + `abs`/`sign` with two directed loops over sorted `xs`:\n\n<<<<<<< SEARCH\n        for j in range(n):\n            if j == i:\n                continue\n            dx = xi - xs[j]\n            # derivative: -s * |dx|^{-s-1} * sign(dx)\n            g -= s * (dx / abs(dx)) * (abs(dx) ** (-s - 1))\n=======\n        # contributions from points before i (sorted xs)\n        for j in range(i):\n            dx = xi - xs[j]\n            g -= s * dx ** (-s - 1)\n        # contributions from points after i (sorted xs)\n        for j in range(i + 1, n):\n            dx = xs[j] - xi\n            g += s * dx ** (-s - 1)\n>>>>>>> REPLACE\n\n3) In `optimize`, sort the extrapolated point `y` before calling `compute_grad`, so the gradient logic is valid:\n\n<<<<<<< SEARCH\n        # compute gradient at extrapolated point\n        g = compute_grad(y, s)\n=======\n        # ensure sorted y for operator consistency\n        y = np.sort(y)\n        # compute gradient at extrapolated point\n        g = compute_grad(y, s)\n>>>>>>> REPLACE\n\n4) Also in `optimize`, sort each projected update `xs_new` so that subsequent energy/gradient calls see sorted arrays:\n\n<<<<<<< SEARCH\n            xs_new = np.clip(y - step * g, 0.0, 1.0)\n=======\n            xs_new = np.clip(y - step * g, 0.0, 1.0)\n            xs_new.sort()\n>>>>>>> REPLACE\n\nReasoning:\n- By keeping `xs` sorted at all times, we no longer need `abs(dx)` or `sign(dx)` in either energy or gradient, which removes branch mis\u2010predictions and expensive math calls.\n- The two\u2010range\u2010loop gradient is clearer, avoids the `if j==i` check in every iteration, and exploits the sorted invariant to know the sign of each `dx`.\n- Sorting `y` and `xs_new` inside `optimize` maintains the invariant automatically after each extrapolation and update.\n- Overall, you should see reduced per\u2010iteration cost in the `njit`\u2010compiled loops and improved throughput on the Riesz\u2010energy metric."]}}}