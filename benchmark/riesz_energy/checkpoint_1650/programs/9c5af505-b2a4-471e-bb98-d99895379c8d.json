{"id": "9c5af505-b2a4-471e-bb98-d99895379c8d", "code": "import numpy as np\nfrom numba import njit, prange\n\ndef equally_spaced(n: int) -> np.ndarray:\n    \"\"\"Return n equally spaced points in [0,1] via numpy.linspace.\"\"\"\n    # np.linspace handles n=0 (\u2192 []), n=1 (\u2192 [0.0]), and n>1\n    return np.linspace(0.0, 1.0, n)\n\ndef jittered_baseline(n: int, seed: int = 0, jitter: float = 1e-3) -> np.ndarray:\n    \"\"\"Return n equally spaced points in [0,1] with small random jitter.\"\"\"\n    rng = np.random.default_rng(seed)\n    xs = equally_spaced(n)\n    if n > 1:\n        xs = np.clip(xs + rng.uniform(-jitter, jitter, size=n), 0.0, 1.0)\n        xs.sort()\n    return xs\n\n@njit(cache=True, fastmath=True, parallel=True)\ndef compute_energy(xs: np.ndarray, s: float = 1.0) -> float:\n    \"\"\"Compute Riesz s-energy via direct double loop (numba accelerated, parallel).\"\"\"\n    n = xs.size\n    ene = 0.0\n    for i in prange(n):\n        for j in range(i + 1, n):\n            ene += (xs[j] - xs[i]) ** (-s)\n    return ene\n\n@njit(cache=True, fastmath=True, parallel=True)\ndef compute_grad(xs: np.ndarray, s: float = 1.0) -> np.ndarray:\n    \"\"\"Compute gradient of Riesz s-energy via one double loop (numba accelerated, parallel).\"\"\"\n    n = xs.size\n    grad = np.zeros(n)\n    for i in prange(n):\n        for j in range(i + 1, n):\n            dx = xs[j] - xs[i]\n            coeff = s * dx ** (-s - 1)\n            grad[i] += coeff\n            grad[j] -= coeff\n    return grad\n\ndef optimize(xs: np.ndarray, s: float = 1.0, lr: float = 1e-2, iters: int = 2000, tol: float = 1e-8, momentum: float = 0.9) -> np.ndarray:\n    \"\"\"Nesterov-accelerated projected gradient descent with adaptive backtracking.\"\"\"\n    energy_fn = compute_energy\n    xs = xs.copy()\n    xs_prev = xs.copy()\n    prev_energy = energy_fn(xs, s)\n    for k in range(1, iters + 1):\n        # Nesterov momentum extrapolation\n        y = xs + momentum * (xs - xs_prev)\n        # compute gradient at extrapolated point\n        g = compute_grad(y, s)\n        grad_norm = np.linalg.norm(g)\n        if grad_norm < tol:\n            break\n        step = lr\n        # backtracking to ensure energy decreases\n        for _ in range(8):\n            xs_new = np.clip(y - step * g, 0.0, 1.0)\n            xs_new.sort()\n            new_energy = energy_fn(xs_new, s)\n            if new_energy < prev_energy:\n                xs_prev = xs\n                xs = xs_new\n                prev_energy = new_energy\n                break\n            step *= 0.5\n        else:\n            # no decrease found: reduce momentum and learning rate, then retry\n            momentum *= 0.5\n            lr *= 0.5\n            continue\n        # gentle decay of learning rate\n        lr *= 0.995\n    xs.sort()\n    return xs\n\ndef main():\n    n = 20\n    s = 1.0\n    # initialize with equally spaced points\n    xs_init = jittered_baseline(n, seed=0, jitter=1e-3)\n    # refine via projected gradient descent\n    xs_local = optimize(xs_init, s, lr=1e-2, iters=500, momentum=0.9)\n    # report final energy\n    print(\"Final Riesz s-energy:\", compute_energy(xs_local, s))\n    return xs_local\n\nif __name__ == \"__main__\":\n    main()\n", "language": "python", "proposal": ["Research Abstract\n\nWe propose a novel framework for the efficient and accurate minimization of Riesz s-energy on the unit interval that overcomes the quadratic-time complexity and slow local convergence of existing numba-based projected gradient methods. Our approach addresses three key limitations of the current proposal\u2014O(n^2) energy/gradient evaluations, sensitivity to initialization, and first-order stagnation\u2014by integrating (1) a hierarchical fast multipole method (FMM) to approximate pairwise interactions in O(n log n) time with controlled error; (2) low-discrepancy, multi-scale jittered initializations based on scrambled Sobol sequences to improve global coverage and reduce dependence on jitter amplitude; and (3) a bound-constrained quasi-Newton solver (L-BFGS-B) augmented by diagonal Hessian preconditioning and adaptive trust-region line search to achieve superlinear local convergence while respecting the [0,1] ordering constraint via simple projection and a differentiable sorting surrogate.  \n\nWe will implement this pipeline in JAX to leverage auto-differentiation, just\u2010in\u2010time compilation, and GPU acceleration, ensuring scalability to n up to 10^4. We will benchmark against the baseline (n = 20, s = 1) to demonstrate at least a 30% reduction in final Riesz energy and a 5\u00d7 speed-up in wall-clock time, and extend experiments over s \u2208 [0.5,2.0] and a range of n. This research will produce an open-source, modular library for high-performance Riesz energy optimization, with broad applicability to numerical integration, mesh generation, and point distribution problems."], "idea_reward": 5.5, "parent_id": "1c78717c-cc39-4f34-9b0c-0a9003f1dc16_migrant_4_migrant_0_migrant_1_migrant_2", "generation": 14, "timestamp": 1757809589.2418923, "iteration_found": 1185, "metrics": {"1 / riesz_energy_n20_s1": 0.0010245053059113095}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 5 lines with 4 lines\nChange 2: Replace 3 lines with 3 lines\nChange 3: Replace 3 lines with 3 lines\nChange 4: Replace 3 lines with 3 lines\nChange 5: Replace 6 lines with 6 lines", "parent_metrics": {"1 / riesz_energy_n20_s1": 0.001024505305911373}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \n\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numba import njit, prange\n\ndef equally_spaced(n: int) -> np.ndarray:\n    \"\"\"Return n equally spaced points in [0,1], using midpoint for n<=1.\"\"\"\n    if n <= 1:\n        return np.array([0.5])[:n]\n    return np.linspace(0.0, 1.0, n)\n\ndef jittered_baseline(n: int, seed: int = 0, jitter: float = 1e-3) -> np.ndarray:\n    \"\"\"Return n equally spaced points in [0,1] with small random jitter.\"\"\"\n    rng = np.random.default_rng(seed)\n    xs = equally_spaced(n)\n    if n > 1:\n        xs = np.clip(xs + rng.uniform(-jitter, jitter, size=n), 0.0, 1.0)\n        xs.sort()\n    return xs\n\n@njit(cache=True, fastmath=True)\ndef compute_energy(xs: np.ndarray, s: float = 1.0) -> float:\n    \"\"\"Compute Riesz s-energy via direct double loop (numba accelerated).\"\"\"\n    n = xs.size\n    ene = 0.0\n    for i in range(n):\n        for j in range(i + 1, n):\n            ene += (xs[j] - xs[i]) ** (-s)\n    return ene\n\n@njit(cache=True, fastmath=True)\ndef compute_grad(xs: np.ndarray, s: float = 1.0) -> np.ndarray:\n    \"\"\"Compute gradient of Riesz s-energy via one double loop (numba accelerated).\"\"\"\n    n = xs.size\n    grad = np.zeros(n)\n    for i in range(n):\n        for j in range(i + 1, n):\n            dx = xs[j] - xs[i]\n            coeff = s * dx ** (-s - 1)\n            grad[i] += coeff\n            grad[j] -= coeff\n    return grad\n\ndef optimize(xs: np.ndarray, s: float = 1.0, lr: float = 1e-2, iters: int = 2000, tol: float = 1e-8, momentum: float = 0.9) -> np.ndarray:\n    \"\"\"Nesterov-accelerated projected gradient descent with adaptive backtracking.\"\"\"\n    energy_fn = compute_energy\n    xs = xs.copy()\n    xs_prev = xs.copy()\n    prev_energy = energy_fn(xs, s)\n    for k in range(1, iters + 1):\n        # Nesterov momentum extrapolation\n        y = xs + momentum * (xs - xs_prev)\n        # compute gradient at extrapolated point\n        g = compute_grad(y, s)\n        grad_norm = np.linalg.norm(g)\n        if grad_norm < tol:\n            break\n        step = lr\n        # backtracking to ensure energy decreases\n        for _ in range(8):\n            xs_new = np.clip(y - step * g, 0.0, 1.0)\n            xs_new.sort()\n            new_energy = energy_fn(xs_new, s)\n            if new_energy < prev_energy:\n                xs_prev = xs\n                xs = xs_new\n                prev_energy = new_energy\n                break\n            step *= 0.5\n        else:\n            # no decrease found: reduce momentum and learning rate, then retry\n            momentum *= 0.5\n            lr *= 0.5\n            continue\n        # gentle decay of learning rate\n        lr *= 0.995\n    xs.sort()\n    return xs\n\ndef main():\n    n = 20\n    s = 1.0\n    # initialize with equally spaced points\n    xs_init = jittered_baseline(n, seed=0, jitter=1e-3)\n    # refine via projected gradient descent\n    xs_local = optimize(xs_init, s, lr=1e-2, iters=500, momentum=0.9)\n    # report final energy\n    print(\"Final Riesz s-energy:\", compute_energy(xs_local, s))\n    return xs_local\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Previous Performance Metrics: \n- 1 / riesz_energy_n20_s1: 0.0010\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: 1 / riesz_energy_n20_s1. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: 1 / riesz_energy_n20_s1: 0.0010\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: 1 / riesz_energy_n20_s1: 0.0010\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: 1 / riesz_energy_n20_s1: 0.0010\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0010)\n```python\nimport numpy as np\nfrom numba import njit, prange\nfrom scipy.optimize import minimize\n\ndef equally_spaced(n: int) -> np.ndarray:\n    \"\"\"Return n equally spaced points on [0,1] using numpy.linspace.\"\"\"\n    # np.linspace gracefully handles n=0 (\u2192 []), n=1 (\u2192 [0.0]), n>1\n    return np.linspace(0.0, 1.0, n)\n\ndef jittered_baseline(n: int, seed: int = 0, jitter: float = 1e-3):\n# ... (truncated for brevity)\n```\nKey features: Performs well on 1 / riesz_energy_n20_s1 (0.0010)\n\n\n### Program 2 (Score: 0.0010)\n```python\nimport numpy as np\nfrom numba import njit, prange\nimport math\n\n@njit(parallel=True, fastmath=True)\ndef compute_energy_s1(xs: np.ndarray) -> float:\n    \"\"\"Specialized Riesz-1 energy: uses 1/dx instead of dx**(-s).\"\"\"\n    n = xs.size\n    if n < 2:\n        return 0.0\n# ... (truncated for brevity)\n```\nKey features: Performs well on 1 / riesz_energy_n20_s1 (0.0010)\n\n\n### Program 3 (Score: 0.0010)\n```python\nimport numpy as np\nfrom numba import njit, prange\n\ndef equally_spaced(n: int) -> np.ndarray:\n    \"\"\"Return n equally spaced points on [0,1].\"\"\"\n    # numpy.linspace handles n=0 and n=1 gracefully and is simpler to maintain\n    return np.linspace(0.0, 1.0, n)\n\ndef jittered_baseline(n: int, seed: int = 0, jitter: float = 1e-3):\n    \"\"\"A simple baseline: equal grid + tiny jitter (still clipped to [0,1]).\"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on 1 / riesz_energy_n20_s1 (0.0010)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.0008)\n```python\nimport numpy as np\nfrom scipy.stats.qmc import Sobol\ndef equally_spaced(n: int) -> np.ndarray:\n    \"\"\"Return n equally spaced points in [0,1]. Explicitly handle n=0 and n=1.\"\"\"\n    if n < 0:\n# ... (truncated)\n```\nKey features: Alternative approach to 1 / riesz_energy_n20_s1\n\n\n### Program D2 (Score: 0.0010)\n```python\nimport numpy as np\nfrom numba import njit, prange\nfrom scipy.optimize import minimize\n\ndef equally_spaced(n: int) -> np.ndarray:\n# ... (truncated)\n```\nKey features: Alternative approach to 1 / riesz_energy_n20_s1\n\n\n# Current Proposal\nResearch Abstract\n\nWe propose a novel framework for the efficient and accurate minimization of Riesz s-energy on the unit interval that overcomes the quadratic-time complexity and slow local convergence of existing numba-based projected gradient methods. Our approach addresses three key limitations of the current proposal\u2014O(n^2) energy/gradient evaluations, sensitivity to initialization, and first-order stagnation\u2014by integrating (1) a hierarchical fast multipole method (FMM) to approximate pairwise interactions in O(n log n) time with controlled error; (2) low-discrepancy, multi-scale jittered initializations based on scrambled Sobol sequences to improve global coverage and reduce dependence on jitter amplitude; and (3) a bound-constrained quasi-Newton solver (L-BFGS-B) augmented by diagonal Hessian preconditioning and adaptive trust-region line search to achieve superlinear local convergence while respecting the [0,1] ordering constraint via simple projection and a differentiable sorting surrogate.  \n\nWe will implement this pipeline in JAX to leverage auto-differentiation, just\u2010in\u2010time compilation, and GPU acceleration, ensuring scalability to n up to 10^4. We will benchmark against the baseline (n = 20, s = 1) to demonstrate at least a 30% reduction in final Riesz energy and a 5\u00d7 speed-up in wall-clock time, and extend experiments over s \u2208 [0.5,2.0] and a range of n. This research will produce an open-source, modular library for high-performance Riesz energy optimization, with broad applicability to numerical integration, mesh generation, and point distribution problems.\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Here are a few small, targeted changes that should help both readability and runtime performance without altering the overall structure of your approach.  We enable parallel loops in Numba, switch the outer loops to `prange` to get multithreaded speed\u2010ups, and simplify the `equally_spaced` helper for clarity.\n\nPlease apply **all** of the following diffs:\n\n<<<<<<< SEARCH\ndef equally_spaced(n: int) -> np.ndarray:\n    \"\"\"Return n equally spaced points in [0,1], using midpoint for n<=1.\"\"\"\n    if n <= 1:\n        return np.array([0.5])[:n]\n    return np.linspace(0.0, 1.0, n)\n=======\ndef equally_spaced(n: int) -> np.ndarray:\n    \"\"\"Return n equally spaced points in [0,1] via numpy.linspace.\"\"\"\n    # np.linspace handles n=0 (\u2192 []), n=1 (\u2192 [0.0]), and n>1\n    return np.linspace(0.0, 1.0, n)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n@njit(cache=True, fastmath=True)\ndef compute_energy(xs: np.ndarray, s: float = 1.0) -> float:\n    \"\"\"Compute Riesz s-energy via direct double loop (numba accelerated).\"\"\"\n=======\n@njit(cache=True, fastmath=True, parallel=True)\ndef compute_energy(xs: np.ndarray, s: float = 1.0) -> float:\n    \"\"\"Compute Riesz s-energy via direct double loop (numba accelerated, parallel).\"\"\"\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    for i in range(n):\n        for j in range(i + 1, n):\n            ene += (xs[j] - xs[i]) ** (-s)\n=======\n    for i in prange(n):\n        for j in range(i + 1, n):\n            ene += (xs[j] - xs[i]) ** (-s)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n@njit(cache=True, fastmath=True)\ndef compute_grad(xs: np.ndarray, s: float = 1.0) -> np.ndarray:\n    \"\"\"Compute gradient of Riesz s-energy via one double loop (numba accelerated).\"\"\"\n=======\n@njit(cache=True, fastmath=True, parallel=True)\ndef compute_grad(xs: np.ndarray, s: float = 1.0) -> np.ndarray:\n    \"\"\"Compute gradient of Riesz s-energy via one double loop (numba accelerated, parallel).\"\"\"\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    for i in range(n):\n        for j in range(i + 1, n):\n            dx = xs[j] - xs[i]\n            coeff = s * dx ** (-s - 1)\n            grad[i] += coeff\n            grad[j] -= coeff\n=======\n    for i in prange(n):\n        for j in range(i + 1, n):\n            dx = xs[j] - xs[i]\n            coeff = s * dx ** (-s - 1)\n            grad[i] += coeff\n            grad[j] -= coeff\n>>>>>>> REPLACE\n\nExplanation of why these help:\n\n-  Adding `parallel=True` to the Numba decorator plus using `prange` lets Numba distribute the outer loop over multiple threads.  \n-  This speeds up both energy and gradient computations, which dominate runtime, especially for larger `n`.  \n-  Simplifying `equally_spaced` removes special\u2010case Python branching and leans on the guaranteed behavior of `np.linspace`, improving maintainability."]}}}