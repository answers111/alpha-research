{"id": "2eaabfd9-1e30-4d2a-ace3-cfe8f78787f8", "code": "import numpy as np\nflatnonzero = np.flatnonzero\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n# cache to avoid redundant ratio computations\n_ratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # check cache for redundant computations\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in _ratio_cache:\n        return _ratio_cache[key]\n    # sparse indexing\n    idxA = flatnonzero(A_ind)\n    idxB = flatnonzero(B_ind)\n    # build sets of sums and diffs\n    sums = {i + j for i in idxA for j in idxB}\n    diffs = {i - j for i in idxA for j in idxB}\n    if not diffs:\n        _ratio_cache[key] = -1.0\n        return -1.0\n    ratio = len(sums) / len(diffs)\n    _ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = flatnonzero(ind)\n    zeros = flatnonzero(~ind)\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        C_ind = current_A.copy()\n        D_ind = current_B.copy()\n        # propose a balanced add/remove move for A or B using helper\n        if rng.random() < 0.5:\n            C_ind = propose_move(C_ind)\n        else:\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < math.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: Adaptive Hierarchical Surrogate\u2010Guided Evolution with Variance\u2010Aware Reinforcement Learning for Scalable Sum\u2010to\u2010Difference Ratio Optimization\n\nAbstract:  \nWe identify three key limitations of the current multi\u2010fidelity GNN plus PPO\u2010guided evolutionary framework: (1) surrogate models are trained offline on synthetic instances and suffer from domain shift as N grows; (2) the PPO scheduler treats move selection as a black box and cannot plan multi\u2010step transitions; (3) covariance adaptation and lazy convolution address locality but lack a principled mechanism to trade evaluation cost against search benefit. We propose four innovations to overcome these weaknesses while preserving the strengths of prior art.\n\n1. Self\u2010Supervised Hierarchical Surrogate Ensemble  \n   \u2013 We embed solver trajectories into a contrastively pre\u2010trained graph\u2010attention network that learns transferable representations across N, seeding both low\u2010 and high\u2010fidelity surrogates.  \n   \u2013 At search time, an online calibration layer uses Gaussian\u2010process uncertainty estimates to dynamically reweight fidelity levels, minimizing wasted high\u2010cost evaluations.  \n\n2. Model\u2010Based Variance\u2010Aware Reinforcement Scheduler  \n   \u2013 We extend PPO to a model\u2010based actor\u2010critic that jointly learns a lightweight transition model of local moves, enabling one\u2010step lookahead and intrinsic rewards for uncertainty reduction.  \n   \u2013 A variance\u2010aware critic penalizes over\u2010confident proposals, thus prioritizing regions of the landscape where surrogate error is highest.  \n\n3. Multi\u2010Scale Correlated Move Generation  \n   \u2013 We generalize low\u2010rank covariance adaptation by maintaining a hierarchy of pattern kernels at different scales (single\u2010bit, block\u2010flip, and FFT\u2010inferred spectral modes).  \n   \u2013 A cross\u2010entropy mechanism adaptively allocates sampling budget among scales, preserving high\u2010order structure while enabling occasional global jumps.  \n\n4. Dynamic Sparse\u2010Fenwick\u2010FFT Convolution Pipeline  \n   \u2013 We introduce a streaming Fenwick\u2010FFT hybrid that maintains \u201cactive frontiers\u201d of indices changed since last full FFT, enabling O(log\u2009N) amortized updates on GPU and CPU backends.  \n   \u2013 A budgeted multi\u2010armed bandit determines when to schedule cheap incremental updates versus full\u2010scale recalculation.  \n\nImplementation & Validation:  \nOur prototype uses PyTorch for graph and RL modules, JAX with custom CUDA/Fenwick kernels for convolution, and a Python\u2010C++ bridge for real\u2010time fidelity scheduling. We will benchmark on N \u2208 {50,100,200,500,1000,2000,5000} against hill\u2010climbing, tabu, CMA\u2010ES, and the prior state\u2010of\u2010the\u2010art, targeting a robust improvement to \u22651.45 in sum\u2010to\u2010difference ratio with sublinear scaling."], "idea_reward": 6.0, "parent_id": "8bc01c2d-e251-4dc8-a405-55d4b646a365", "generation": 12, "timestamp": 1757789713.4072134, "iteration_found": 678, "metrics": {"score": 1.3846153846153846}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace import numpy as np with 2 lines\nChange 2: Replace 2 lines with 4 lines\nChange 3: Replace 11 lines with 18 lines\nChange 4: Replace 4 lines with 4 lines\nChange 5: Replace 2 lines with 2 lines", "parent_metrics": {"score": 1.3846153846153846}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: Multi\u2010Fidelity Graph\u2010Surrogate and Reinforcement\u2010Guided Evolution for Scalable Sum\u2010to\u2010Difference Ratio Maximization\n\nAbstract:  \nMaximizing the sum\u2010to\u2010difference ratio of two binary indicator vectors underpins many additive\u2010combinatorics challenges but remains hindered by costly full convolutions, memoryless move proposals, and premature convergence. We propose a unified metaheuristic that addresses these limitations through four synergistic advances:\n\n1. Multi\u2010Fidelity Graph Neural Surrogate  \n   \u2022 A light\u2010weight graph neural network (GNN) is pre\u2010trained on synthetic instances of varying N to predict \u0394ratio for both single and block\u2010flip proposals.  \n   \u2022 We maintain an ensemble of low\u2010 and high\u2010fidelity surrogates: low\u2010fidelity GNNs provide O(1) rapid screening while a high\u2010fidelity sparse\u2010FFT surrogate refines top candidates at O(k\u2009log\u2009N).  \n   \u2022 Bayesian active sampling selects proposals with greatest predictive uncertainty for true evaluation, bounding overall surrogate error.\n\n2. Reinforcement\u2010Learning\u2010Based Move Scheduler  \n   \u2022 A policy network observes population diversity metrics and surrogate\u2010predicted gains to allocate search effort adaptively among uniform flips, block\u2010swaps, and crossover events.  \n   \u2022 Trained via proximal policy optimization (PPO) on small\u2010scale benchmarks, the scheduler learns to escape deep local optima 5\u00d7 faster than static bandits.\n\n3. Correlated Bit\u2010Flip Proposals via Covariance Adaptation  \n   \u2022 We estimate bit\u2010flip correlations from elite trajectories and update a low\u2010rank covariance matrix to sample multibit moves that preserve high\u2010order \u201cpattern kernels.\u201d  \n   \u2022 This dynamic neighborhood adapts to landscape curvature, enabling larger, targeted jumps when beneficial.\n\n4. Lazy Incremental Convolution with Fenwick\u2010FFT Hybrid  \n   \u2022 We integrate Fenwick\u2010tree data structures for point updates and sparse FFTs for batch corrections, reducing amortized per\u2010flip update time to O(log\u2009N).  \n   \u2022 GPU\u2010accelerated JAX kernels hand...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # perform convolution directly on boolean arrays (bool\u21920/1 implicitly)\n    sums_conv = np.convolve(A_ind, B_ind)\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via correlation for clarity and efficiency\n    diffs_conv = np.correlate(A_ind, B_ind, mode='full')\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(~ind)\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        C_ind = current_A.copy()\n        D_ind = current_B.copy()\n        # propose a balanced add/remove move for A or B using helper\n        if rng.random() < 0.5:\n            C_ind = propose_move(C_ind)\n        else:\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < math.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.3846\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5161\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5161)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5161)\n\n\n### Program 2 (Score: 1.3000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Cache to avoid redundant ratio computations\n_ratio_cache = {}\n# Prebind convolution for faster repeated calls\nconv = np.convolve\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.3000)\n\n\n### Program 3 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    S = np.convolve(A_ind, B_ind)\n    sizeS = np.count_nonzero(S)\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: -1.0000)\n```python\nimport numpy as np\n\n# For reproducibility\nrng = np.random.default_rng(42)\n\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n# Current Proposal\nTitle: Adaptive Hierarchical Surrogate\u2010Guided Evolution with Variance\u2010Aware Reinforcement Learning for Scalable Sum\u2010to\u2010Difference Ratio Optimization\n\nAbstract:  \nWe identify three key limitations of the current multi\u2010fidelity GNN plus PPO\u2010guided evolutionary framework: (1) surrogate models are trained offline on synthetic instances and suffer from domain shift as N grows; (2) the PPO scheduler treats move selection as a black box and cannot plan multi\u2010step transitions; (3) covariance adaptation and lazy convolution address locality but lack a principled mechanism to trade evaluation cost against search benefit. We propose four innovations to overcome these weaknesses while preserving the strengths of prior art.\n\n1. Self\u2010Supervised Hierarchical Surrogate Ensemble  \n   \u2013 We embed solver trajectories into a contrastively pre\u2010trained graph\u2010attention network that learns transferable representations across N, seeding both low\u2010 and high\u2010fidelity surrogates.  \n   \u2013 At search time, an online calibration layer uses Gaussian\u2010process uncertainty estimates to dynamically reweight fidelity levels, minimizing wasted high\u2010cost evaluations.  \n\n2. Model\u2010Based Variance\u2010Aware Reinforcement Scheduler  \n   \u2013 We extend PPO to a model\u2010based actor\u2010critic that jointly learns a lightweight transition model of local moves, enabling one\u2010step lookahead and intrinsic rewards for uncertainty reduction.  \n   \u2013 A variance\u2010aware critic penalizes over\u2010confident proposals, thus prioritizing regions of the landscape where surrogate error is highest.  \n\n3. Multi\u2010Scale Correlated Move Generation  \n   \u2013 We generalize low\u2010rank covariance adaptation by maintaining a hierarchy of pattern kernels at different scales (single\u2010bit, block\u2010flip, and FFT\u2010inferred spectral modes).  \n   \u2013 A cross\u2010entropy mechanism adaptively allocates sampling budget among scales, preserving high\u2010order structure while enabling occasional global jumps.  \n\n4. Dynamic Sparse\u2010Fenwick\u2010FFT Convolution Pipeline  \n   \u2013 We introduce a streamin...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Here are a few targeted changes to reduce redundant work and speed up ratio evaluations. We\n\n1.  Prebind `flatnonzero` for faster lookup.  \n2.  Introduce a small cache to avoid re\u2010computing the ratio for the same state.  \n3.  Replace the two NumPy convolutions with sparse set\u2010based computations (much faster when the indicator vectors are sparse).  \n4.  Eliminate one unnecessary copy when accepting a new \u201ccurrent\u201d solution.\n\nAll changes are in the exact SEARCH/REPLACE diff format:\n\n<<<<<<< SEARCH\nimport numpy as np\n=======\nimport numpy as np\nflatnonzero = np.flatnonzero\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n# For reproducibility\nrng = default_rng(42)\n=======\n# For reproducibility\nrng = default_rng(42)\n# cache to avoid redundant ratio computations\n_ratio_cache = {}\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # perform convolution directly on boolean arrays (bool\u21920/1 implicitly)\n    sums_conv = np.convolve(A_ind, B_ind)\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via correlation for clarity and efficiency\n    diffs_conv = np.correlate(A_ind, B_ind, mode='full')\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n=======\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # check cache for redundant computations\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in _ratio_cache:\n        return _ratio_cache[key]\n    # sparse indexing\n    idxA = flatnonzero(A_ind)\n    idxB = flatnonzero(B_ind)\n    # build sets of sums and diffs\n    sums = {i + j for i in idxA for j in idxB}\n    diffs = {i - j for i in idxA for j in idxB}\n    if not diffs:\n        _ratio_cache[key] = -1.0\n        return -1.0\n    ratio = len(sums) / len(diffs)\n    _ratio_cache[key] = ratio\n    return ratio\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(~ind)\n=======\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = flatnonzero(ind)\n    zeros = flatnonzero(~ind)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # accept as new current\n        current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n=======\n        # accept as new current\n        current_A, current_B, current_ratio = C_ind, D_ind, ratio\n>>>>>>> REPLACE\n\nExplanation:\n\n- By caching the ratio for each unique `(A_ind, B_ind)`, we avoid recomputing expensive convolutions on repeat states.  \n- Switching to set\u2010based sums/diffs is often much faster when the indicator vectors are relatively sparse.  \n- Prebinding `flatnonzero` and removing one `.copy()` cut down Python overhead.  \n\nThese changes should raise your score by reducing per\u2010iteration cost."]}}}