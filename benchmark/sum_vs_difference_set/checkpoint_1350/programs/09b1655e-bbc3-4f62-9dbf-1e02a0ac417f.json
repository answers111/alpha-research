{"id": "09b1655e-bbc3-4f62-9dbf-1e02a0ac417f", "code": "import numpy as np\nfrom numpy.random import default_rng\nfrom numpy import frombuffer\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\nimport functools\nfrom numpy.fft import fft, ifft\n# Removed custom ratio_cache; using lru_cache for compute_ratio\n\n# Cache for computed ratios to avoid redundant convolution calls\n# (removed) ratio_cache = {}\n\n# For reproducibility and performance\nrng = default_rng(42)\n# Pre-bind NumPy and RNG functions for faster access\n# Removed convolution & correlation prebind; using set-based computation in compute_ratio\n# Pre\u2010bind RNG functions for clarity\nrand = rng.random\nchoice = rng.choice\nflat_nonzero = np.flatnonzero\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\n# Replace with set-based computation for clarity and speed\n@functools.lru_cache(maxsize=50000)\ndef _compute_ratio_cached(a_bytes: bytes, b_bytes: bytes) -> float:\n    \"\"\"Cached internal ratio computation using FFT-based convolution/correlation.\"\"\"\n    A_ind = frombuffer(a_bytes, dtype=bool)\n    B_ind = frombuffer(b_bytes, dtype=bool)\n    # infeasible if either is empty\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    n = A_ind.size\n    M = 2 * n\n    # FFT-based convolution for sums\n    fftA = fft(A_ind, M)\n    fftB = fft(B_ind, M)\n    conv = np.rint(np.real(ifft(fftA * fftB)))\n    # FFT-based correlation for differences\n    corr = np.rint(np.real(ifft(fftA * np.conj(fftB))))\n    num_sums = int(np.count_nonzero(conv))\n    num_diffs = int(np.count_nonzero(corr))\n    return -1.0 if num_diffs == 0 else num_sums / num_diffs\n\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum-to-difference ratio using cached FFT evaluation.\"\"\"\n    return _compute_ratio_cached(A_ind.tobytes(), B_ind.tobytes())\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a local move: swap, block\u2010flip, or segment\u2010reversal.\"\"\"\n    N = ind.size\n    r = rand()\n    if r < 0.5:\n        # single\u2010bit swap\n        ones = flat_nonzero(ind)\n        zeros = flat_nonzero(~ind)\n        if ones.size and zeros.size:\n            i_remove = choice(ones)\n            i_add = choice(zeros)\n            ind[i_remove] = False\n            ind[i_add] = True\n    elif r < 0.75:\n        # random block flip (length up to N//4)\n        max_len = max(1, N // 4)\n        start = choice(N)\n        length = choice(max_len) + 1\n        end = (start + length) % N\n        if start < end:\n            ind[start:end] = ~ind[start:end]\n        else:\n            ind[start:] = ~ind[start:]\n            ind[:end] = ~ind[:end]\n    else:\n        # segment reversal\n        i, j = sorted(choice(N, size=2, replace=False))\n        ind[i:j] = ind[i:j][::-1]\n    return ind\n\ndef get_local_best(current_A: np.ndarray, current_B: np.ndarray, batch_size: int = 10):\n    \"\"\"Generate batch proposals and return the best local move and its ratio.\"\"\"\n    local_best_ratio = -1.0\n    local_best_A = current_A\n    local_best_B = current_B\n    for _ in range(batch_size):\n        if rand() < 0.5:\n            C_ind = propose_move(current_A.copy())\n            D_ind = current_B.copy()\n        else:\n            C_ind = current_A.copy()\n            D_ind = propose_move(current_B.copy())\n        ratio = compute_ratio(C_ind, D_ind)\n        if ratio > local_best_ratio:\n            local_best_ratio = ratio\n            local_best_A = C_ind\n            local_best_B = D_ind\n    return local_best_ratio, local_best_A, local_best_B\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind = A_ind.copy()\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.99985       # slower cooling for deeper exploration\n\n    batch_size = 50\n    max_iter = 10000\n    # Pre\u2010bind inner\u2010loop functions for speed\n    get_best = get_local_best\n    for _ in range(max_iter):\n        local_best_ratio, local_best_A, local_best_B = get_best(current_A, current_B, batch_size)\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rand() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Research Abstract\n\nBackground  \nMaximizing the ratio |A\u2009+\u2009B|/|A\u2009\u2212\u2009B| for two binary indicator vectors A,\u2009B\u2282\u2124N is a hard combinatorial problem. A recent hill-climbing\u2009+\u2009simulated-annealing (HC\u2009+\u2009SA) implementation achieves a best score of 1.5455 on N\u2009=\u200930, but suffers from three main bottlenecks:  \n  1. Search stagnation due to fixed\u2010schedule local moves (single\u2010bit swaps) and no dynamic operator adaptation.  \n  2. Expensive O(N\u2009log\u2009N) FFT\u2010based ratio evaluations that dominate runtime and limit sampling.  \n  3. Unbounded caching with simple LRU on raw bit\u2010vectors, leading to memory bloat and limited reuse.\n\nInspired by advances in surrogate\u2010assisted evolutionary computation and adaptive operator selection, we propose a scalable, implementable framework that addresses each weakness and targets a \u2265\u200920% improvement in ratio on N\u2009=\u200930, with clear pathways to larger N.\n\nProposed Framework  \n1. Surrogate\u2010Guided Evaluation  \n   \u2022 Train a lightweight regression model (random forest or shallow neural net) to predict |A\u2009+\u2009B|/|A\u2009\u2212\u2009B| from compact features (e.g., low\u2010order autocorrelations, cardinalities).  \n   \u2022 Use the surrogate for >\u200990% of candidate screening, calling the exact FFT\u2010based evaluator only on the top 5% predicted moves to refine and update the surrogate online.  \n   \u2022 This reduces exact\u2010evaluation calls by an order of magnitude while maintaining estimation error <\u20095%.\n\n2. Adaptive Operator Selection (AOS)  \n   \u2022 Implement a multi\u2010armed bandit scheme (e.g., \u03b5-greedy or UCB) to choose among move operators: single\u2010bit swap, block flips, segment reversals, multi\u2010point crossover.  \n   \u2022 Continuously track operator rewards (improvement magnitude, acceptance rate) and adapt selection probabilities, preventing stagnation and focusing effort where gains are likely.  \n   \u2022 Embed AOS into both global (population\u2010based) and local (memetic intensification) phases.\n\n3. Island\u2010Model Memetic Evolution  \n   \u2022 Maintain p parallel \u201cislands,\u201d each evolving a subpopulation of (A,B) pairs with its own temperature and operator mix.  \n   \u2022 Periodically migrate top individuals between islands to share diverse building blocks while preserving local intensification.  \n   \u2022 This asynchronous island model scales across CPU cores and GPU streams, enabling linear speed\u2010up.\n\n4. Efficient Caching and Symmetry Reduction  \n   \u2022 Hash each candidate pair using a 64-bit Zobrist code to index a fixed\u2010size cache with a 4-way associative replacement policy, bounding memory use.  \n   \u2022 Exploit known symmetry groups (shifts, reversals, complementations) to canonicalize each (A,B) before evaluation or caching, reducing redundant exploration by \u2248\u200950%.\n\nImplementation Plan  \n\u2022 Core evaluation: exact convolution/correlation in C++ with FFTW, wrapped via pybind11; surrogate model and AOS logic in Python using NumPy and scikit\u2010learn.  \n\u2022 Parallelization: Python multiprocessing for islands; optional CUDA kernels for batch surrogate inference.  \n\u2022 Experimental protocol: compare against the HC\u2009+\u2009SA baseline on N\u2009=\u200930 (10\u2009K runs each), track score distributions, convergence curves, operator usage statistics, and cache hit rates.\n\nExpected Outcomes  \nBy uniting surrogate filtering, adaptive operator selection, and an island\u2010model memetic architecture with efficient caching and symmetry exploitation, we expect to surpass the 1.5455 benchmark by \u2265\u200920% on N\u2009=\u200930, reduce exact\u2010evaluation cost by \u2248\u200990%, and demonstrate linear scalability to N\u2009=\u200950. The deliverables will include an open\u2010source optimization library, documented experimental benchmarks, and insights into surrogate\u2010assisted search for extremal combinatorial constructions."], "idea_reward": 6.5, "parent_id": "1b25bc85-80b0-42d4-b0de-7688e739c6c7", "generation": 21, "timestamp": 1757825077.9852853, "iteration_found": 1283, "metrics": {"score": 1.5}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 2 lines with 3 lines\nChange 2: '@functools.lru_cache(maxsize=10000)' to '@functools.lru_cache(maxsize=50000)'\nChange 3: Replace 2 lines with 2 lines\nChange 4: Replace 10 lines with 29 lines", "parent_metrics": {"score": 1.5454545454545454}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nResearch Abstract\n\nBackground  \nMaximizing the ratio |A + B|\u2009/\u2009|A \u2212 B| for two binary indicator vectors A, B\u2282\u2124N is a challenging combinatorial optimization problem. A recent hill\u2010climbing + simulated\u2010annealing implementation achieves a best score of 1.5455 on N = 30 but exhibits three key limitations:  \n1. Local\u2010search stagnation due to single\u2010bit swaps and a fixed annealing schedule,  \n2. O(k\u00b2) set\u2010based ratio computation that becomes a bottleneck as the search intensifies,  \n3. Unbounded cache\u2010growth and inefficient keying of cached ratios.  \n\nProposed Improvements  \nWe propose a hybrid memetic\u2010optimization framework that integrates global and local search components, fast evaluation routines, and structural insights from known difference\u2010set theory:  \n1. FFT\u2010Accelerated Ratio Evaluation  \n   \u2022 Represent A, B as bit\u2010packed arrays and compute convolution A*B and correlation A*reverse(B) via FFT in O(N log N) time.  \n   \u2022 Replace Python\u2010tuple caching with fixed\u2010size hash of bit\u2010vectors and an LRU policy to bound memory.  \n\n2. Population\u2010Based Global Search  \n   \u2022 Maintain a population of (A, B) pairs and apply multi\u2010point crossover inspired by genetic algorithms, exchanging contiguous blocks of bits to explore the search space more broadly.  \n   \u2022 Incorporate parallel tempering: run several subpopulations at different \u201ctemperatures,\u201d allowing occasional swaps of individuals to escape local optima.  \n\n3. Adaptive Local Intensification  \n   \u2022 For each candidate, perform an adaptive simulated\u2010annealing phase that (a) uses a nonmonotonic cooling schedule, (b) allows multi\u2010bit \u201cblock\u201d flips and tabu\u2010guided perturbations to encourage deeper exploration, and (c) adjusts acceptance probabilities based on recent improvement history.  \n   \u2022 Dynamically vary batch sizes and move\u2010types (swap, add/remove, block\u2010reversal) according to local curvature of the objective.  \n\n4. Symmetry Reduction and Seeding  \n   \u2022 Leverage known congruence classes and difference\u2010set symmetries to pr...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\nimport functools\nfrom numpy.fft import fft, ifft\n# Removed custom ratio_cache; using lru_cache for compute_ratio\n\n# Cache for computed ratios to avoid redundant convolution calls\n# (removed) ratio_cache = {}\n\n# For reproducibility and performance\nrng = default_rng(42)\n# Pre-bind NumPy and RNG functions for faster access\n# Removed convolution & correlation prebind; using set-based computation in compute_ratio\n# Pre\u2010bind RNG functions for clarity\nrand = rng.random\nchoice = rng.choice\nflat_nonzero = np.flatnonzero\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\n# Replace with set-based computation for clarity and speed\n@functools.lru_cache(maxsize=10000)\ndef _compute_ratio_cached(a_bytes: bytes, b_bytes: bytes) -> float:\n    \"\"\"Cached internal ratio computation using FFT-based convolution/correlation.\"\"\"\n    A_ind = np.frombuffer(a_bytes, dtype=bool)\n    B_ind = np.frombuffer(b_bytes, dtype=bool)\n    # infeasible if either is empty\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    n = A_ind.size\n    M = 2 * n\n    # FFT-based convolution for sums\n    fftA = fft(A_ind, M)\n    fftB = fft(B_ind, M)\n    conv = np.rint(np.real(ifft(fftA * fftB)))\n    # FFT-based correlation for differences\n    corr = np.rint(np.real(ifft(fftA * np.conj(fftB))))\n    num_sums = int(np.count_nonzero(conv))\n    num_diffs = int(np.count_nonzero(corr))\n    return -1.0 if num_diffs == 0 else num_sums / num_diffs\n\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum-to-difference ratio using cached FFT evaluation.\"\"\"\n    return _compute_ratio_cached(A_ind.tobytes(), B_ind.tobytes())\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a simple single\u2010bit swap to maintain constant cardinality.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(~ind)\n    if ones.size and zeros.size:\n        i_remove = choice(ones)\n        i_add = choice(zeros)\n        ind[i_remove] = False\n        ind[i_add] = True\n    return ind\n\ndef get_local_best(current_A: np.ndarray, current_B: np.ndarray, batch_size: int = 10):\n    \"\"\"Generate batch proposals and return the best local move and its ratio.\"\"\"\n    local_best_ratio = -1.0\n    local_best_A = current_A\n    local_best_B = current_B\n    for _ in range(batch_size):\n        if rand() < 0.5:\n            C_ind = propose_move(current_A.copy())\n            D_ind = current_B.copy()\n        else:\n            C_ind = current_A.copy()\n            D_ind = propose_move(current_B.copy())\n        ratio = compute_ratio(C_ind, D_ind)\n        if ratio > local_best_ratio:\n            local_best_ratio = ratio\n            local_best_A = C_ind\n            local_best_B = D_ind\n    return local_best_ratio, local_best_A, local_best_B\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind = A_ind.copy()\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.99985       # slower cooling for deeper exploration\n\n    batch_size = 50\n    max_iter = 10000\n    # Pre\u2010bind inner\u2010loop functions for speed\n    get_best = get_local_best\n    for _ in range(max_iter):\n        local_best_ratio, local_best_A, local_best_B = get_best(current_A, current_B, batch_size)\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rand() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.5455\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 2 (Score: 0.9828)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math  # for simulated\u2010annealing acceptance\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors via set operations\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B| using set operations. \n# ... (truncated for brevity)\n```\nKey features: Performs well on score (0.9828)\n\n\n### Program 3 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility and performance\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n### Program D2 (Score: 1.4333)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math  # added for simulated annealing acceptance\n\n# For reproducibility\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nResearch Abstract\n\nBackground  \nMaximizing the ratio |A\u2009+\u2009B|/|A\u2009\u2212\u2009B| for two binary indicator vectors A,\u2009B\u2282\u2124N is a hard combinatorial problem. A recent hill-climbing\u2009+\u2009simulated-annealing (HC\u2009+\u2009SA) implementation achieves a best score of 1.5455 on N\u2009=\u200930, but suffers from three main bottlenecks:  \n  1. Search stagnation due to fixed\u2010schedule local moves (single\u2010bit swaps) and no dynamic operator adaptation.  \n  2. Expensive O(N\u2009log\u2009N) FFT\u2010based ratio evaluations that dominate runtime and limit sampling.  \n  3. Unbounded caching with simple LRU on raw bit\u2010vectors, leading to memory bloat and limited reuse.\n\nInspired by advances in surrogate\u2010assisted evolutionary computation and adaptive operator selection, we propose a scalable, implementable framework that addresses each weakness and targets a \u2265\u200920% improvement in ratio on N\u2009=\u200930, with clear pathways to larger N.\n\nProposed Framework  \n1. Surrogate\u2010Guided Evaluation  \n   \u2022 Train a lightweight regression model (random forest or shallow neural net) to predict |A\u2009+\u2009B|/|A\u2009\u2212\u2009B| from compact features (e.g., low\u2010order autocorrelations, cardinalities).  \n   \u2022 Use the surrogate for >\u200990% of candidate screening, calling the exact FFT\u2010based evaluator only on the top 5% predicted moves to refine and update the surrogate online.  \n   \u2022 This reduces exact\u2010evaluation calls by an order of magnitude while maintaining estimation error <\u20095%.\n\n2. Adaptive Operator Selection (AOS)  \n   \u2022 Implement a multi\u2010armed bandit scheme (e.g., \u03b5-greedy or UCB) to choose among move operators: single\u2010bit swap, block flips, segment reversals, multi\u2010point crossover.  \n   \u2022 Continuously track operator rewards (improvement magnitude, acceptance rate) and adapt selection probabilities, preventing stagnation and focusing effort where gains are likely.  \n   \u2022 Embed AOS into both global (population\u2010based) and local (memetic intensification) phases.\n\n3. Island\u2010Model Memetic Evolution  \n   \u2022 Maintain p parallel \u201cislands,\u201d each evolving a subpopulation of (A,B) ...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Here are four targeted patches to (1) enrich the local\u2010move operator, (2) enlarge the FFT\u2010ratio cache, and (3) shave off a couple of attribute lookups. Each change is incremental and should help both exploration (higher score) and raw throughput.\n\nPatch 1: pull in `frombuffer` at top\u2010level to avoid repeated `np.` lookups  \n<<<<<<< SEARCH\nimport numpy as np\nfrom numpy.random import default_rng\n=======\nimport numpy as np\nfrom numpy.random import default_rng\nfrom numpy import frombuffer\n>>>>>>> REPLACE\n\nPatch 2: grow the LRU cache so we less frequently recompute FFTs  \n<<<<<<< SEARCH\n@functools.lru_cache(maxsize=10000)\n=======\n@functools.lru_cache(maxsize=50000)\n>>>>>>> REPLACE\n\nPatch 3: use the top\u2010level `frombuffer` inside the cached FFT routine  \n<<<<<<< SEARCH\n    A_ind = np.frombuffer(a_bytes, dtype=bool)\n    B_ind = np.frombuffer(b_bytes, dtype=bool)\n=======\n    A_ind = frombuffer(a_bytes, dtype=bool)\n    B_ind = frombuffer(b_bytes, dtype=bool)\n>>>>>>> REPLACE\n\nPatch 4: diversify `propose_move` with multi\u2010bit/block flips and reversals  \n<<<<<<< SEARCH\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a simple single\u2010bit swap to maintain constant cardinality.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(~ind)\n    if ones.size and zeros.size:\n        i_remove = choice(ones)\n        i_add = choice(zeros)\n        ind[i_remove] = False\n        ind[i_add] = True\n    return ind\n=======\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a local move: swap, block\u2010flip, or segment\u2010reversal.\"\"\"\n    N = ind.size\n    r = rand()\n    if r < 0.5:\n        # single\u2010bit swap\n        ones = flat_nonzero(ind)\n        zeros = flat_nonzero(~ind)\n        if ones.size and zeros.size:\n            i_remove = choice(ones)\n            i_add = choice(zeros)\n            ind[i_remove] = False\n            ind[i_add] = True\n    elif r < 0.75:\n        # random block flip (length up to N//4)\n        max_len = max(1, N // 4)\n        start = choice(N)\n        length = choice(max_len) + 1\n        end = (start + length) % N\n        if start < end:\n            ind[start:end] = ~ind[start:end]\n        else:\n            ind[start:] = ~ind[start:]\n            ind[:end] = ~ind[:end]\n    else:\n        # segment reversal\n        i, j = sorted(choice(N, size=2, replace=False))\n        ind[i:j] = ind[i:j][::-1]\n    return ind\n>>>>>>> REPLACE\n\nExplanation of benefits:\n\n- Increasing the cache size to 50 000 entries sharply cuts down on repeat FFT evaluations, boosting throughput.\n- Binding `frombuffer` at module scope and calling it directly inside the hot loop saves a few Python attribute lookups per call.\n- Enriching `propose_move` with block flips and segment reversals injects deeper perturbations, which should help escape local optima and raise the overall achieved ratio."]}}}