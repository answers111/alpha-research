{"id": "1b25bc85-80b0-42d4-b0de-7688e739c6c7", "code": "import numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\nimport functools\nfrom numpy.fft import fft, ifft\n# Removed custom ratio_cache; using lru_cache for compute_ratio\n\n# Cache for computed ratios to avoid redundant convolution calls\n# (removed) ratio_cache = {}\n\n# For reproducibility and performance\nrng = default_rng(42)\n# Pre-bind NumPy and RNG functions for faster access\n# Removed convolution & correlation prebind; using set-based computation in compute_ratio\n# Pre\u2010bind RNG functions for clarity\nrand = rng.random\nchoice = rng.choice\nflat_nonzero = np.flatnonzero\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\n# Replace with set-based computation for clarity and speed\n@functools.lru_cache(maxsize=10000)\ndef _compute_ratio_cached(a_bytes: bytes, b_bytes: bytes) -> float:\n    \"\"\"Cached internal ratio computation using FFT-based convolution/correlation.\"\"\"\n    A_ind = np.frombuffer(a_bytes, dtype=bool)\n    B_ind = np.frombuffer(b_bytes, dtype=bool)\n    # infeasible if either is empty\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    n = A_ind.size\n    M = 2 * n\n    # FFT-based convolution for sums\n    fftA = fft(A_ind, M)\n    fftB = fft(B_ind, M)\n    conv = np.rint(np.real(ifft(fftA * fftB)))\n    # FFT-based correlation for differences\n    corr = np.rint(np.real(ifft(fftA * np.conj(fftB))))\n    num_sums = int(np.count_nonzero(conv))\n    num_diffs = int(np.count_nonzero(corr))\n    return -1.0 if num_diffs == 0 else num_sums / num_diffs\n\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum-to-difference ratio using cached FFT evaluation.\"\"\"\n    return _compute_ratio_cached(A_ind.tobytes(), B_ind.tobytes())\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a simple single\u2010bit swap to maintain constant cardinality.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(~ind)\n    if ones.size and zeros.size:\n        i_remove = choice(ones)\n        i_add = choice(zeros)\n        ind[i_remove] = False\n        ind[i_add] = True\n    return ind\n\ndef get_local_best(current_A: np.ndarray, current_B: np.ndarray, batch_size: int = 10):\n    \"\"\"Generate batch proposals and return the best local move and its ratio.\"\"\"\n    local_best_ratio = -1.0\n    local_best_A = current_A\n    local_best_B = current_B\n    for _ in range(batch_size):\n        if rand() < 0.5:\n            C_ind = propose_move(current_A.copy())\n            D_ind = current_B.copy()\n        else:\n            C_ind = current_A.copy()\n            D_ind = propose_move(current_B.copy())\n        ratio = compute_ratio(C_ind, D_ind)\n        if ratio > local_best_ratio:\n            local_best_ratio = ratio\n            local_best_A = C_ind\n            local_best_B = D_ind\n    return local_best_ratio, local_best_A, local_best_B\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind = A_ind.copy()\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.99985       # slower cooling for deeper exploration\n\n    batch_size = 50\n    max_iter = 10000\n    # Pre\u2010bind inner\u2010loop functions for speed\n    get_best = get_local_best\n    for _ in range(max_iter):\n        local_best_ratio, local_best_A, local_best_B = get_best(current_A, current_B, batch_size)\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rand() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Research Abstract\n\nBackground  \nMaximizing the ratio |A + B|\u2009/\u2009|A \u2212 B| for two binary indicator vectors A, B\u2282\u2124N is a challenging combinatorial optimization problem. A recent hill\u2010climbing + simulated\u2010annealing implementation achieves a best score of 1.5455 on N = 30 but exhibits three key limitations:  \n1. Local\u2010search stagnation due to single\u2010bit swaps and a fixed annealing schedule,  \n2. O(k\u00b2) set\u2010based ratio computation that becomes a bottleneck as the search intensifies,  \n3. Unbounded cache\u2010growth and inefficient keying of cached ratios.  \n\nProposed Improvements  \nWe propose a hybrid memetic\u2010optimization framework that integrates global and local search components, fast evaluation routines, and structural insights from known difference\u2010set theory:  \n1. FFT\u2010Accelerated Ratio Evaluation  \n   \u2022 Represent A, B as bit\u2010packed arrays and compute convolution A*B and correlation A*reverse(B) via FFT in O(N log N) time.  \n   \u2022 Replace Python\u2010tuple caching with fixed\u2010size hash of bit\u2010vectors and an LRU policy to bound memory.  \n\n2. Population\u2010Based Global Search  \n   \u2022 Maintain a population of (A, B) pairs and apply multi\u2010point crossover inspired by genetic algorithms, exchanging contiguous blocks of bits to explore the search space more broadly.  \n   \u2022 Incorporate parallel tempering: run several subpopulations at different \u201ctemperatures,\u201d allowing occasional swaps of individuals to escape local optima.  \n\n3. Adaptive Local Intensification  \n   \u2022 For each candidate, perform an adaptive simulated\u2010annealing phase that (a) uses a nonmonotonic cooling schedule, (b) allows multi\u2010bit \u201cblock\u201d flips and tabu\u2010guided perturbations to encourage deeper exploration, and (c) adjusts acceptance probabilities based on recent improvement history.  \n   \u2022 Dynamically vary batch sizes and move\u2010types (swap, add/remove, block\u2010reversal) according to local curvature of the objective.  \n\n4. Symmetry Reduction and Seeding  \n   \u2022 Leverage known congruence classes and difference\u2010set symmetries to prune redundant solutions and seed the population with algebraically structured initial pairs (e.g., cyclic difference sets, Sidon\u2010type constructions).  \n\nImplementation Plan  \n\u2022 Core kernels (FFT\u2010based evaluation, multi\u2010bit mutations) will be implemented in Numba/Cython for CPU acceleration and optionally CUDA for GPU.  \n\u2022 The global search controller will exploit Python\u2019s multiprocessing to run tempered subpopulations in parallel.  \n\u2022 A rigorous experimental protocol will track improvement trajectories, convergence rates, and statistical significance of gains over the baseline 1.5455 score.  \n\nExpected Outcomes  \nBy uniting FFT\u2010based evaluation, memetic search strategies, adaptive intensification, and algebraic symmetry reduction, we expect to exceed the current best ratio by at least 20% on N = 30 and to scale the approach to larger N. The project will deliver an open\u2010source optimization library and new insights into extremal combinatorial set constructions."], "idea_reward": 7.5, "parent_id": "abf68216-a09c-43fe-b206-2919b5d3184f_migrant_0_migrant_1", "generation": 20, "timestamp": 1757823467.4110777, "iteration_found": 1257, "metrics": {"score": 1.5454545454545454}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace exp = math.exp with 4 lines\nChange 2: 'ratio_cache = {}' to '# (removed) ratio_cache = {}'\nChange 3: Replace 19 lines with 23 lines", "parent_metrics": {"score": 1.5454545454545454}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \n\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\n# Cache for computed ratios to avoid redundant convolution calls\nratio_cache = {}\n\n# For reproducibility and performance\nrng = default_rng(42)\n# Pre-bind NumPy and RNG functions for faster access\n# Removed convolution & correlation prebind; using set-based computation in compute_ratio\n# Pre\u2010bind RNG functions for clarity\nrand = rng.random\nchoice = rng.choice\nflat_nonzero = np.flatnonzero\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\n# Replace with set-based computation for clarity and speed\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum-to-difference ratio |A+B|/|A\u2212B|. Return -1.0 if infeasible.\"\"\"\n    # extract indices of ones\n    onesA = flat_nonzero(A_ind)\n    onesB = flat_nonzero(B_ind)\n    if onesA.size == 0 or onesB.size == 0:\n        return -1.0\n    # caching based on index tuples\n    key = (tuple(onesA), tuple(onesB))\n    if key in ratio_cache:\n        return ratio_cache[key]\n    # compute unique sums and differences\n    sums = {i + j for i in onesA for j in onesB}\n    diffs = {i - j for i in onesA for j in onesB}\n    num_sums = len(sums)\n    num_diffs = len(diffs)\n    result = -1.0 if num_diffs == 0 else num_sums / num_diffs\n    ratio_cache[key] = result\n    return result\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a simple single\u2010bit swap to maintain constant cardinality.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(~ind)\n    if ones.size and zeros.size:\n        i_remove = choice(ones)\n        i_add = choice(zeros)\n        ind[i_remove] = False\n        ind[i_add] = True\n    return ind\n\ndef get_local_best(current_A: np.ndarray, current_B: np.ndarray, batch_size: int = 10):\n    \"\"\"Generate batch proposals and return the best local move and its ratio.\"\"\"\n    local_best_ratio = -1.0\n    local_best_A = current_A\n    local_best_B = current_B\n    for _ in range(batch_size):\n        if rand() < 0.5:\n            C_ind = propose_move(current_A.copy())\n            D_ind = current_B.copy()\n        else:\n            C_ind = current_A.copy()\n            D_ind = propose_move(current_B.copy())\n        ratio = compute_ratio(C_ind, D_ind)\n        if ratio > local_best_ratio:\n            local_best_ratio = ratio\n            local_best_A = C_ind\n            local_best_B = D_ind\n    return local_best_ratio, local_best_A, local_best_B\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind = A_ind.copy()\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.99985       # slower cooling for deeper exploration\n\n    batch_size = 50\n    max_iter = 10000\n    # Pre\u2010bind inner\u2010loop functions for speed\n    get_best = get_local_best\n    for _ in range(max_iter):\n        local_best_ratio, local_best_A, local_best_B = get_best(current_A, current_B, batch_size)\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rand() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.5455\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 2 (Score: 1.3846)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.3846)\n\n\n### Program 3 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5455)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.5161)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nResearch Abstract\n\nBackground  \nMaximizing the ratio |A + B|\u2009/\u2009|A \u2212 B| for two binary indicator vectors A, B\u2282\u2124N is a challenging combinatorial optimization problem. A recent hill\u2010climbing + simulated\u2010annealing implementation achieves a best score of 1.5455 on N = 30 but exhibits three key limitations:  \n1. Local\u2010search stagnation due to single\u2010bit swaps and a fixed annealing schedule,  \n2. O(k\u00b2) set\u2010based ratio computation that becomes a bottleneck as the search intensifies,  \n3. Unbounded cache\u2010growth and inefficient keying of cached ratios.  \n\nProposed Improvements  \nWe propose a hybrid memetic\u2010optimization framework that integrates global and local search components, fast evaluation routines, and structural insights from known difference\u2010set theory:  \n1. FFT\u2010Accelerated Ratio Evaluation  \n   \u2022 Represent A, B as bit\u2010packed arrays and compute convolution A*B and correlation A*reverse(B) via FFT in O(N log N) time.  \n   \u2022 Replace Python\u2010tuple caching with fixed\u2010size hash of bit\u2010vectors and an LRU policy to bound memory.  \n\n2. Population\u2010Based Global Search  \n   \u2022 Maintain a population of (A, B) pairs and apply multi\u2010point crossover inspired by genetic algorithms, exchanging contiguous blocks of bits to explore the search space more broadly.  \n   \u2022 Incorporate parallel tempering: run several subpopulations at different \u201ctemperatures,\u201d allowing occasional swaps of individuals to escape local optima.  \n\n3. Adaptive Local Intensification  \n   \u2022 For each candidate, perform an adaptive simulated\u2010annealing phase that (a) uses a nonmonotonic cooling schedule, (b) allows multi\u2010bit \u201cblock\u201d flips and tabu\u2010guided perturbations to encourage deeper exploration, and (c) adjusts acceptance probabilities based on recent improvement history.  \n   \u2022 Dynamically vary batch sizes and move\u2010types (swap, add/remove, block\u2010reversal) according to local curvature of the objective.  \n\n4. Symmetry Reduction and Seeding  \n   \u2022 Leverage known congruence classes and difference\u2010set symmetries to pr...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are three targeted diffs that replace the O(k\u00b2) set\u2010based ratio computation and unbounded dict cache with an FFT-based evaluation cached via functools.lru_cache.  This reduces the computation from quadratic to O(N\u2009log\u2009N) and bounds memory for caching.\n\n1) Remove the old dict cache and import the FFT routines + functools:\n\n<<<<<<< SEARCH\nexp = math.exp\n=======\nexp = math.exp\nimport functools\nfrom numpy.fft import fft, ifft\n# Removed custom ratio_cache; using lru_cache for compute_ratio\n>>>>>>> REPLACE\n\n2) Delete the old unbounded dict cache:\n\n<<<<<<< SEARCH\nratio_cache = {}\n=======\n# (removed) ratio_cache = {}\n>>>>>>> REPLACE\n\n3) Replace the entire compute_ratio function with an FFT-backed, lru_cache-decorated version:\n\n<<<<<<< SEARCH\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum-to-difference ratio |A+B|/|A\u2212B|. Return -1.0 if infeasible.\"\"\"\n    # extract indices of ones\n    onesA = flat_nonzero(A_ind)\n    onesB = flat_nonzero(B_ind)\n    if onesA.size == 0 or onesB.size == 0:\n        return -1.0\n    # caching based on index tuples\n    key = (tuple(onesA), tuple(onesB))\n    if key in ratio_cache:\n        return ratio_cache[key]\n    # compute unique sums and differences\n    sums = {i + j for i in onesA for j in onesB}\n    diffs = {i - j for i in onesA for j in onesB}\n    num_sums = len(sums)\n    num_diffs = len(diffs)\n    result = -1.0 if num_diffs == 0 else num_sums / num_diffs\n    ratio_cache[key] = result\n    return result\n=======\n@functools.lru_cache(maxsize=10000)\ndef _compute_ratio_cached(a_bytes: bytes, b_bytes: bytes) -> float:\n    \"\"\"Cached internal ratio computation using FFT-based convolution/correlation.\"\"\"\n    A_ind = np.frombuffer(a_bytes, dtype=bool)\n    B_ind = np.frombuffer(b_bytes, dtype=bool)\n    # infeasible if either is empty\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    n = A_ind.size\n    M = 2 * n\n    # FFT-based convolution for sums\n    fftA = fft(A_ind, M)\n    fftB = fft(B_ind, M)\n    conv = np.rint(np.real(ifft(fftA * fftB)))\n    # FFT-based correlation for differences\n    corr = np.rint(np.real(ifft(fftA * np.conj(fftB))))\n    num_sums = int(np.count_nonzero(conv))\n    num_diffs = int(np.count_nonzero(corr))\n    return -1.0 if num_diffs == 0 else num_sums / num_diffs\n\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum-to-difference ratio using cached FFT evaluation.\"\"\"\n    return _compute_ratio_cached(A_ind.tobytes(), B_ind.tobytes())\n>>>>>>> REPLACE\n\nReasoning:\n- FFT convolutions drop the nested\u2010loops O(k\u00b2) overhead to O(N\u2009log\u2009N).  \n- Using functools.lru_cache with a fixed maxsize bounds memory and avoids repeated FFTs on identical bit\u2010patterns.  \n- Packing the boolean arrays to bytes for the cache key is fast and deterministic.  \n- Overall this should substantially accelerate ratio evaluations and improve the search\u2019s score."]}}}