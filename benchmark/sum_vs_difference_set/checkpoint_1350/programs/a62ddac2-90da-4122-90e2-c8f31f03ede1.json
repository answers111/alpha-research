{"id": "a62ddac2-90da-4122-90e2-c8f31f03ede1", "code": "import numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\nimport functools\n# Replace FFT path with direct convolution for small N\nconvolve = np.convolve\n# Pre-bind fast correlation path\ncorrelate = np.correlate\n# Removed custom ratio_cache; using lru_cache for compute_ratio\n\n# Cache for computed ratios to avoid redundant convolution calls\n# (removed) ratio_cache = {}\n\n# For reproducibility and performance\nrng = default_rng(42)\n# Pre-bind NumPy and RNG functions for faster access\n# Removed convolution & correlation prebind; using set-based computation in compute_ratio\n# Pre\u2010bind RNG functions for clarity\nrand = rng.random\nchoice = rng.choice\nflat_nonzero = np.flatnonzero\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\n# Replace with set-based computation for clarity and speed\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum-to-difference ratio using direct convolution (fast for N~30).\"\"\"\n    # quick infeasible check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # bool\u2192int8 view lets np.convolve work in native C\n    A = A_ind.view(np.int8)\n    B = B_ind.view(np.int8)\n    conv = convolve(A, B, mode='full')\n    corr = correlate(A, B, mode='full')\n    num_sums = int(count_nonzero(conv))\n    num_diffs = int(count_nonzero(corr))\n    return -1.0 if num_diffs == 0 else num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a simple single\u2010bit swap to maintain constant cardinality.\"\"\"\n    ones = flat_nonzero(ind)\n    zeros = flat_nonzero(~ind)\n    if ones.size and zeros.size:\n        i_remove = choice(ones)\n        i_add = choice(zeros)\n        ind[i_remove] = False\n        ind[i_add] = True\n    return ind\n\ndef get_local_best(current_A: np.ndarray, current_B: np.ndarray, batch_size: int = 10):\n    \"\"\"Generate batch proposals and return the best local move and its ratio.\"\"\"\n    local_best_ratio = -1.0\n    local_best_A = current_A\n    local_best_B = current_B\n    for _ in range(batch_size):\n        if rand() < 0.5:\n            C_ind = propose_move(current_A.copy())\n            D_ind = current_B.copy()\n        else:\n            C_ind = current_A.copy()\n            D_ind = propose_move(current_B.copy())\n        ratio = compute_ratio(C_ind, D_ind)\n        if ratio > local_best_ratio:\n            local_best_ratio = ratio\n            local_best_A = C_ind\n            local_best_B = D_ind\n    return local_best_ratio, local_best_A, local_best_B\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind = A_ind.copy()\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.99985       # slower cooling for deeper exploration\n\n    batch_size = 50\n    max_iter = 10000\n    # Pre\u2010bind inner\u2010loop functions for speed\n    get_best = get_local_best\n    for _ in range(max_iter):\n        local_best_ratio, local_best_A, local_best_B = get_best(current_A, current_B, batch_size)\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rand() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Research Abstract\n\nBackground  \nMaximizing the ratio |A+B|\u2009/\u2009|A\u2212B| for two binary indicator vectors of length N remains a highly non\u2010convex, combinatorial challenge. Existing hill\u2010climbing and memetic\u2010evolutionary schemes routinely stall in local optima, incur O(N\u00b2) or worse per\u2010evaluation costs, and rely on static heuristics that limit scaling beyond N\u2009=\u200930 (best known ratio\u2009\u2248\u20091.5455).\n\nProposed Framework  \nWe introduce an adaptive, multi\u2010fidelity, machine\u2010learned memetic framework with five synergistic innovations to overcome these limitations:\n\n1. Hybrid Evaluation Pipeline  \n \u2022 Dynamically select among bitwise\u2010accelerated transforms (AVX\u2010512 popcounts), radix\u20102 FFTs, and CUDA\u2010offloaded convolutions based on N and real\u2010time profiling, reducing average per\u2010evaluation cost from O(N\u2009log\u2009N) to near O(1) for small updates.  \n \u2022 Maintain incremental sum/difference counters so single\u2010bit or block\u2010move updates incur O(1) time and exploit a fixed\u2010size, LRU\u2010cuckoo cache augmented by low\u2010dimensional locality\u2010sensitive hashes to prune redundant evaluations.\n\n2. Hierarchical Bayesian Surrogates with Acquisition\u2010Driven Screening  \n \u2022 Train a multi\u2010fidelity Gaussian\u2010process ensemble to predict both ratio mean and uncertainty for proposed (A,B) pairs.  \n \u2022 Use an expected\u2010improvement acquisition function to pre\u2010filter candidates, cutting exact evaluations by up to 90% while focusing search on high\u2010gain regions.\n\n3. Deep Policy\u2010Guided Move Generation  \n \u2022 Model A,B as nodes in a bipartite graph and learn a Graph Neural Network policy via reinforcement learning on historical search trajectories.  \n \u2022 The policy proposes variable\u2010length multi\u2010bit \u201cblock\u201d or \u201csegment\u201d moves that capture higher\u2010order interactions, enabling systematic escapes from local optima.\n\n4. Distributed Asynchronous Island Search with Adaptive Migration  \n \u2022 Deploy multiple MPI\u2010coordinated islands at varied \u201ctemperatures,\u201d exchanging solutions based on a diversity\u2010weighted migration schedule.  \n \u2022 Dynamically resize subpopulations and adjust migration frequency by monitoring inter\u2010island entropy and convergence velocity.\n\n5. Structured Seeding and Symmetry Reduction  \n \u2022 Generate initial populations using known cyclic and Sidon\u2010type difference\u2010set constructions.  \n \u2022 Apply group\u2010action symmetries and graph\u2010isomorphism checks to eliminate isomorphic solution classes before search.\n\nImplementation Plan  \nCore compute kernels will be implemented in Rust with AVX\u2010512 intrinsics and CUDA backends, exposed to Python via FFI for orchestration. Learning components will use PyTorch; distributed search leverages MPI. A comprehensive benchmarking suite will track per\u2010iteration cost, convergence curves, and statistical significance against the 1.5455 baseline.\n\nExpected Outcomes  \nWe anticipate improving the N=30 best ratio by \u226525%, achieving robust scalability to N\u2265100 with near\u2010linear speedup, and delivering an open\u2010source library that fuses ultra\u2010efficient bitwise kernels with machine\u2010learned search control."], "idea_reward": 6.33, "parent_id": "8ae17349-02ab-45ed-99da-f03591da1f50", "generation": 22, "timestamp": 1757826749.0908868, "iteration_found": 1311, "metrics": {"error": -1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 6 lines with 9 lines\nChange 2: Replace 2 lines with 4 lines\nChange 3: Replace 4 lines with 4 lines\nChange 4: Replace 7 lines with 7 lines", "parent_metrics": {"score": 1.5454545454545454}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nResearch Abstract\n\nBackground  \nMaximizing the ratio |A\u2009+\u2009B|\u2009/\u2009|A\u2009\u2212\u2009B| for two binary indicator vectors of length N is a highly non-convex combinatorial problem.  Recent hill-climbing with single-bit moves and a fixed cooling schedule attains a best ratio of 1.5455 at N=30, but routinely stalls in local optima, spends O(N\u00b2) time on set-based counts, and suffers unbounded cache growth with inefficient keying.\n\nProposed Framework  \nWe introduce a scalable, hybrid memetic\u2010evolutionary optimizer that combines (1) FFT\u2010accelerated exact evaluation, (2) surrogate\u2010assisted pre-screening, (3) population-based global search with adaptive neighborhood strategies, and (4) algebraic symmetry reduction and structured seeding.  Our four key innovations are:\n\n1. FFT- and Bitwise-Accelerated Evaluation  \n \u2022 Represent indicator vectors as bit-packed arrays; compute convolution and correlation via radix-2 FFT in O(N log N) or via GPU-accelerated bitwise transforms in O(N/word_size).  \n \u2022 Replace unbounded Python-tuple caching with a fixed\u2010size, LRU-cuckoo hash table on 128-bit fingerprints, bounding memory and guaranteeing O(1) lookup.\n\n2. Surrogate-Guided Candidate Filtering  \n \u2022 Train a lightweight gradient-boosted regression tree on previously evaluated (A,B) \u2192 ratio data to predict high\u2010value regions.  \n \u2022 Use the surrogate to pre-filter large batches of multi-bit \u201cblock\u201d and \u201csegment\u201d proposals before incurring the exact FFT cost, reducing costly evaluations by 50\u201380%.\n\n3. Multi-Island Memetic Search with Adaptive Intensification  \n \u2022 Maintain several subpopulations (\u201cislands\u201d) at different temperatures, exchanging individuals asynchronously (parallel tempering) to escape local traps.  \n \u2022 Within each island, apply variable-neighborhood local search: single/ multi-bit swaps, block reversals, and tabu-guided perturbations, selecting move types based on recent improvement curvature.  \n \u2022 Dynamically adapt the cooling schedule (nonmonotonic reheating on stagnation) and batch sizes i...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\nimport functools\n# Replace FFT path with direct convolution for small N\nconvolve = np.convolve\n# Removed custom ratio_cache; using lru_cache for compute_ratio\n\n# Cache for computed ratios to avoid redundant convolution calls\n# (removed) ratio_cache = {}\n\n# For reproducibility and performance\nrng = default_rng(42)\n# Pre-bind NumPy and RNG functions for faster access\n# Removed convolution & correlation prebind; using set-based computation in compute_ratio\n# Pre\u2010bind RNG functions for clarity\nrand = rng.random\nchoice = rng.choice\nflat_nonzero = np.flatnonzero\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\n# Replace with set-based computation for clarity and speed\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum-to-difference ratio using direct convolution (fast for N~30).\"\"\"\n    # quick infeasible check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # bool\u2192int8 view lets np.convolve work in native C\n    A = A_ind.view(np.int8)\n    B = B_ind.view(np.int8)\n    conv = convolve(A, B, mode='full')\n    corr = convolve(A, B[::-1], mode='full')\n    num_sums = int(np.count_nonzero(conv))\n    num_diffs = int(np.count_nonzero(corr))\n    return -1.0 if num_diffs == 0 else num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a simple single\u2010bit swap to maintain constant cardinality.\"\"\"\n    ones = flat_nonzero(ind)\n    zeros = flat_nonzero(~ind)\n    if ones.size and zeros.size:\n        i_remove = choice(ones)\n        i_add = choice(zeros)\n        ind[i_remove] = False\n        ind[i_add] = True\n    return ind\n\ndef get_local_best(current_A: np.ndarray, current_B: np.ndarray, batch_size: int = 10):\n    \"\"\"Generate batch proposals and return the best local move and its ratio.\"\"\"\n    local_best_ratio = -1.0\n    local_best_A = current_A\n    local_best_B = current_B\n    for _ in range(batch_size):\n        if rand() < 0.5:\n            C_ind = propose_move(current_A.copy())\n            D_ind = current_B.copy()\n        else:\n            C_ind = current_A.copy()\n            D_ind = propose_move(current_B.copy())\n        ratio = compute_ratio(C_ind, D_ind)\n        if ratio > local_best_ratio:\n            local_best_ratio = ratio\n            local_best_A = C_ind\n            local_best_B = D_ind\n    return local_best_ratio, local_best_A, local_best_B\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind = A_ind.copy()\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.99985       # slower cooling for deeper exploration\n\n    batch_size = 50\n    max_iter = 10000\n    # Pre\u2010bind inner\u2010loop functions for speed\n    get_best = get_local_best\n    for _ in range(max_iter):\n        local_best_ratio, local_best_A, local_best_B = get_best(current_A, current_B, batch_size)\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rand() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.5455\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 2 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\nimport functools\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5455)\n\n\n### Program 3 (Score: 1.3611)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n# Prebind convolution for faster inner-loop calls\nconvolve = np.convolve\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.3611)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility and speed\u2010ups\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n### Program D2 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nResearch Abstract\n\nBackground  \nMaximizing the ratio |A+B|\u2009/\u2009|A\u2212B| for two binary indicator vectors of length N remains a highly non\u2010convex, combinatorial challenge. Existing hill\u2010climbing and memetic\u2010evolutionary schemes routinely stall in local optima, incur O(N\u00b2) or worse per\u2010evaluation costs, and rely on static heuristics that limit scaling beyond N\u2009=\u200930 (best known ratio\u2009\u2248\u20091.5455).\n\nProposed Framework  \nWe introduce an adaptive, multi\u2010fidelity, machine\u2010learned memetic framework with five synergistic innovations to overcome these limitations:\n\n1. Hybrid Evaluation Pipeline  \n \u2022 Dynamically select among bitwise\u2010accelerated transforms (AVX\u2010512 popcounts), radix\u20102 FFTs, and CUDA\u2010offloaded convolutions based on N and real\u2010time profiling, reducing average per\u2010evaluation cost from O(N\u2009log\u2009N) to near O(1) for small updates.  \n \u2022 Maintain incremental sum/difference counters so single\u2010bit or block\u2010move updates incur O(1) time and exploit a fixed\u2010size, LRU\u2010cuckoo cache augmented by low\u2010dimensional locality\u2010sensitive hashes to prune redundant evaluations.\n\n2. Hierarchical Bayesian Surrogates with Acquisition\u2010Driven Screening  \n \u2022 Train a multi\u2010fidelity Gaussian\u2010process ensemble to predict both ratio mean and uncertainty for proposed (A,B) pairs.  \n \u2022 Use an expected\u2010improvement acquisition function to pre\u2010filter candidates, cutting exact evaluations by up to 90% while focusing search on high\u2010gain regions.\n\n3. Deep Policy\u2010Guided Move Generation  \n \u2022 Model A,B as nodes in a bipartite graph and learn a Graph Neural Network policy via reinforcement learning on historical search trajectories.  \n \u2022 The policy proposes variable\u2010length multi\u2010bit \u201cblock\u201d or \u201csegment\u201d moves that capture higher\u2010order interactions, enabling systematic escapes from local optima.\n\n4. Distributed Asynchronous Island Search with Adaptive Migration  \n \u2022 Deploy multiple MPI\u2010coordinated islands at varied \u201ctemperatures,\u201d exchanging solutions based on a diversity\u2010weighted migration schedule.  \n \u2022 Dynamically r...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are a few lightweight, targeted patches that eliminate extra attribute lookups, leverage a faster correlation path, and reduce Python\u2010level branching overhead. Together these should shave a few percent off the inner loop:\n\n1) Pre-bind `count_nonzero`  \n2) Pre-bind `correlate`  \n3) Swap out the reverse-&-`convolve` for `np.correlate`  \n4) Use integer draws for the 50/50 branch instead of a float comparison\n\n<<<<<<< SEARCH\n# Pre\u2010bind NumPy and RNG functions for faster access\n# Removed convolution & correlation prebind; using set-based computation in compute_ratio\n# Pre\u2010bind RNG functions for clarity\nrand = rng.random\nchoice = rng.choice\nflat_nonzero = np.flatnonzero\n=======\n# Pre\u2010bind NumPy and RNG functions for faster access\n# Removed convolution & correlation prebind; using set-based computation in compute_ratio\n# Pre\u2010bind RNG functions for clarity\nrand = rng.random\nrand_int = rng.integers            # integer draws for faster 50/50 decisions\nchoice = rng.choice\nflat_nonzero = np.flatnonzero\n# Pre-bind count_nonzero to reduce attribute lookup overhead\ncount_nonzero = np.count_nonzero\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n# Replace FFT path with direct convolution for small N\nconvolve = np.convolve\n=======\n# Replace FFT path with direct convolution for small N\nconvolve = np.convolve\n# Pre-bind fast correlation path\ncorrelate = np.correlate\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    conv = convolve(A, B, mode='full')\n    corr = convolve(A, B[::-1], mode='full')\n    num_sums = int(np.count_nonzero(conv))\n    num_diffs = int(np.count_nonzero(corr))\n=======\n    conv = convolve(A, B, mode='full')\n    corr = correlate(A, B, mode='full')\n    num_sums = int(count_nonzero(conv))\n    num_diffs = int(count_nonzero(corr))\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        for _ in range(batch_size):\n            if rand() < 0.5:\n                C_ind = propose_move(current_A.copy())\n                D_ind = current_B.copy()\n            else:\n                C_ind = current_A.copy()\n                D_ind = propose_move(current_B.copy())\n=======\n        for _ in range(batch_size):\n            if rand_int(2):           # faster integer-based coin flip\n                C_ind = propose_move(current_A.copy())\n                D_ind = current_B.copy()\n            else:\n                C_ind = current_A.copy()\n                D_ind = propose_move(current_B.copy())\n>>>>>>> REPLACE\n\nExplanation of gains:\n\n- Binding `count_nonzero` and `correlate` at module scope removes repeated attribute lookups in the hot inner loop.  \n- Switching from a manually reversed array + `convolve` to `np.correlate` uses a more direct C\u2010level cross\u2010correlation.  \n- Replacing a floating\u2010point comparison (`rand() < 0.5`) with an integer draw (`rand_int(2)`) avoids the FP overhead and branch misprediction penalties.\n\nThese are small mechanical tweaks but in aggregate can yield a noticeable speedup in the O(max_iter\u00b7batch_size) core, enabling more search steps (or a larger batch) within the same wall\u2010clock time."]}}}