{"id": "1f7af08a-5e03-43fe-8a9d-b72493e4c1bd", "code": "import numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# simple cache for compute_ratio to avoid redundant computations\nratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # simple cache lookup to skip redundant computations\n    # Pack boolean arrays into machine\u2010word integers for faster hashing\n    packed_A = int.from_bytes(np.packbits(A_ind).tobytes(), byteorder='little')\n    packed_B = int.from_bytes(np.packbits(B_ind).tobytes(), byteorder='little')\n    key = (packed_A, packed_B)\n    if key in ratio_cache:\n        return ratio_cache[key]\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        # cache infeasible cases to avoid rechecking the same zero\u2010vector pair\n        ratio_cache[key] = -1.0\n        return -1.0\n    # sparse direct\u2010set approach for small supports, else FFT convolution\n    ones_A = np.flatnonzero(A_ind)\n    ones_B = np.flatnonzero(B_ind)\n    if ones_A.size * ones_B.size < 1000:\n        # vectorized Minkowski sum/difference via numpy\n        sums = np.unique(ones_A[:, None] + ones_B[None, :])\n        diffs = np.unique(ones_A[:, None] - ones_B[None, :])\n        ratio = -1.0 if diffs.size == 0 else sums.size / diffs.size\n        ratio_cache[key] = ratio\n        return ratio\n    # FFT\u2010based convolution for larger supports\n    # Use precomputed FFT length to cut down on repeated array allocations\n    fa = np.fft.fft(A_ind.astype(float), FFT_N)\n    fb = np.fft.fft(B_ind.astype(float), FFT_N)\n    n = FFT_N\n    sums_conv = np.rint(np.fft.ifft(fa * fb)).astype(int)\n    num_sums = np.count_nonzero(sums_conv)\n    # use cross\u2010correlation via conjugate to get the difference spectrum\n    diffs_conv = np.rint(np.fft.ifft(fa * np.conj(fb))).astype(int)\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        ratio_cache[key] = -1.0\n        return -1.0\n    ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Return a new vector with one bit added or removed, ensuring at least one '1'.\"\"\"\n    new_ind = ind.copy()\n    ones = np.flatnonzero(new_ind)\n    zeros = np.flatnonzero(~new_ind)\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        new_ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        new_ind[rng.choice(ones)] = False\n    return new_ind\n\n# Configuration constants\nDEFAULT_N = 30\n# Precompute FFT size for constant\u2010length vectors to avoid repeated reallocation\nFFT_N = 2 * DEFAULT_N - 1\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.9995   # slower cooling rate for better exploration\n    T = T0\n    for _ in range(max_iter):\n        # propose move on only one vector to reduce copying overhead\n        if rng.random() < 0.5:\n            # propose_move already copies internally, so pass current_A directly\n            C_ind = propose_move(current_A)\n            D_ind = current_B  # unchanged\n        else:\n            C_ind = current_A  # unchanged\n            # likewise avoid duplicate copy for B\n            D_ind = propose_move(current_B)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current (no need to copy; propose_move already gave us a fresh array)\n            current_A, current_B, current_ratio = C_ind, D_ind, ratio\n            # update global best if improved (we still copy best to freeze it)\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: Hierarchical Trust-Region Bayesian Monte-Carlo Planning with Adaptive Continuous Relaxations and Surrogate Ensembles for Sum-to-Difference Ratio Optimization\n\nAbstract:  \nExisting hill-climbing and simulated-annealing baselines (score = 1.3846) for maximizing the sum-to-difference ratio over 0/1\u2010vectors suffer from two critical drawbacks: (1) myopic single-bit updates that frequently stall in deep local optima, and (2) prohibitively expensive full-FFT evaluations. We propose a unified, implementable framework that overcomes these bottlenecks through four synergistic advances:\n\n1. Multi-Scale Surrogate Ensemble with Trust-Region Control  \n \u2022 A global transformer-based graph surrogate captures long-range bit interactions, while localized Gaussian-process refinements quantify uncertainty at fine scales.  \n \u2022 Ensemble variance defines adaptive trust regions, ensuring surrogate\u2010guided steps remain within reliable prediction bounds and reducing missteps.\n\n2. Continuous Relaxation and Differentiable Rounding  \n \u2022 We relax discrete bit indicators into a continuous domain and perform gradient-based optimization via JAX-powered autodiff of the FFT convolution objective.  \n \u2022 Dynamic rounding projects continuous solutions back to discrete space, seeding high-quality proposals that escape plateaus 8\u00d7 faster than bit-flips alone.\n\n3. Hierarchical Monte-Carlo Tree Search with Adaptive Granularity  \n \u2022 A two-tier MCTS alternates between global \u201cblock\u2010segment\u201d proposals (variable\u2010length merges/splits) and local single-bit refinements.  \n \u2022 A regret-minimizing bandit adapts tree width and search depth online, balancing exploration of novel structures against exploitation of high-reward regions.\n\n4. Asynchronous Multi-Fidelity Evaluation Scheduler  \n \u2022 A contextual bandit scheduler allocates compute among: (a) fast surrogate ensemble predictions, (b) continuous-relaxation gradient steps, and (c) exact AVX-512/CUDA FFT kernels.  \n \u2022 By favoring cheap yet informative evaluations early, we target a \u226598% reduction in full-FFT calls without sacrificing solution quality.\n\nImplementation Plan: Neural surrogates and policy/value networks in PyTorch; continuous relaxations in JAX; high-performance C++/CUDA kernels for FFTs; distributed orchestration via Ray. Benchmarks on N = 50\u20135000 against hill-climbing, tabu search, genetic algorithms, and state-of-the-art surrogates aim for a \u226520% improvement (ratio > 1.66) and \u226598% fewer full evaluations. Beyond sum-to-difference ratio maximization, our framework readily extends to general additive-combinatorics and large-scale sequence-alignment tasks."], "idea_reward": 6.67, "parent_id": "26670884-ea5a-4b65-91e2-bb0891519d93", "generation": 15, "timestamp": 1757790968.0554414, "iteration_found": 712, "metrics": {"score": 1.4230769230769231}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 7 lines with 7 lines\nChange 2: 'max_iter = 20000' to 'max_iter = 50000'\nChange 3: 'alpha = 0.999   # cooling rate' to 'alpha = 0.9995   # slower cooling rate for better exploration'\nChange 4: Replace 6 lines with 6 lines", "parent_metrics": {"score": 1.3846153846153846}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: Multi-Scale Model-Guided Monte-Carlo Planning with Uncertainty-Aware Surrogates and Budget-Aware Fidelity Scheduling for Sum-to-Difference Ratio Maximization\n\nAbstract:\nThe current hill-climbing plus simulated-annealing baseline (score = 1.3846) suffers from two intertwined bottlenecks: (1) myopic single-bit proposals that stagnate in deep local optima and (2) expensive full-FFT evaluations that dominate runtime. To overcome these, we propose a unified, implementable framework with three synergistic components:\n\n1. Uncertainty-Aware Graph Surrogate  \n   \u2022 A graph-neural-process model pre-trained on synthetic bit-flip landscapes to predict sum-to-difference ratios and approximate local gradients.  \n   \u2022 Online active learning via Thompson sampling: high-uncertainty or high-gain moves trigger lightweight AVX-512 convolution checks, reducing full-FFT calls by \u226595%.\n\n2. Hybrid Monte-Carlo Tree Search with Learned Policies  \n   \u2022 Progressive-widening MCTS operating at two granularities\u2014a coarse \u201cblock merge\u201d phase and a fine \u201csingle-flip\u201d phase\u2014each guided by upper-confidence bounds.  \n   \u2022 An actor-critic network, continuously refined through experience replay, biases rollouts to balance exploration and exploitation, escaping plateaus 5\u00d7 faster than memoryless search.\n\n3. Budget-Aware Multi-Fidelity Scheduler  \n   \u2022 A non-stationary, regret-minimizing multi-armed bandit dynamically allocates compute between bit-parallel AVX-512 kernels, JAX/CUDA FFTs, and the surrogate.  \n   \u2022 Resource allocation adapts in real time to observed per-move gains, achieving an order-of-magnitude speedup.\n\nWe will implement neural modules in PyTorch/JAX and high-performance kernels in C++/CUDA, orchestrated via Python. Benchmarked on N = 50\u20135000 against hill-climbing, tabu search, GA, and state-of-the-art surrogate methods, we target a \u226515% improvement (ratio > 1.60) and \u226590% reduction in full-evaluation calls. Our design naturally extends to general additive-combinatorics and large-s...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# simple cache for compute_ratio to avoid redundant computations\nratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # simple cache lookup to skip redundant computations\n    # Pack boolean arrays into machine\u2010word integers for faster hashing\n    packed_A = int.from_bytes(np.packbits(A_ind).tobytes(), byteorder='little')\n    packed_B = int.from_bytes(np.packbits(B_ind).tobytes(), byteorder='little')\n    key = (packed_A, packed_B)\n    if key in ratio_cache:\n        return ratio_cache[key]\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        # cache infeasible cases to avoid rechecking the same zero\u2010vector pair\n        ratio_cache[key] = -1.0\n        return -1.0\n    # sparse direct\u2010set approach for small supports, else FFT convolution\n    ones_A = np.flatnonzero(A_ind)\n    ones_B = np.flatnonzero(B_ind)\n    if ones_A.size * ones_B.size < 1000:\n        # Minkowski sum/difference via Python sets\n        sumset = {i + j for i in ones_A for j in ones_B}\n        difset = {i - j for i in ones_A for j in ones_B}\n        ratio = -1.0 if not difset else len(sumset) / len(difset)\n        ratio_cache[key] = ratio\n        return ratio\n    # FFT\u2010based convolution for larger supports\n    # Use precomputed FFT length to cut down on repeated array allocations\n    fa = np.fft.fft(A_ind.astype(float), FFT_N)\n    fb = np.fft.fft(B_ind.astype(float), FFT_N)\n    n = FFT_N\n    sums_conv = np.rint(np.fft.ifft(fa * fb)).astype(int)\n    num_sums = np.count_nonzero(sums_conv)\n    # use cross\u2010correlation via conjugate to get the difference spectrum\n    diffs_conv = np.rint(np.fft.ifft(fa * np.conj(fb))).astype(int)\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        ratio_cache[key] = -1.0\n        return -1.0\n    ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Return a new vector with one bit added or removed, ensuring at least one '1'.\"\"\"\n    new_ind = ind.copy()\n    ones = np.flatnonzero(new_ind)\n    zeros = np.flatnonzero(~new_ind)\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        new_ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        new_ind[rng.choice(ones)] = False\n    return new_ind\n\n# Configuration constants\nDEFAULT_N = 30\n# Precompute FFT size for constant\u2010length vectors to avoid repeated reallocation\nFFT_N = 2 * DEFAULT_N - 1\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        # propose move on only one vector to reduce copying overhead\n        if rng.random() < 0.5:\n            # propose_move already copies internally, so pass current_A directly\n            C_ind = propose_move(current_A)\n            D_ind = current_B  # unchanged\n        else:\n            C_ind = current_A  # unchanged\n            # likewise avoid duplicate copy for B\n            D_ind = propose_move(current_B)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.3846\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5161\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5312\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5312\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5312)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_arr: np.ndarray, B_arr: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5312)\n\n\n### Program 2 (Score: 1.4000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# removed caching dictionary to reduce overhead; caching rarely hits in random search\n# ratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.4000)\n\n\n### Program 3 (Score: -1.0000)\n```python\nimport numpy as np\n\n# For reproducibility\nrng = np.random.default_rng(42)\n# Global cache for computed ratios to avoid redundant convolution/correlation\n_ratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    \"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: -1.0000)\n```python\nimport numpy as np\n# Use numpy default_rng for reproducibility and better performance\nrng = np.random.default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n### Program D2 (Score: -1.0000)\n```python\nimport numpy as np\nimport random\n\n# For reproducibility\nrandom.seed(42)\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n# Current Proposal\nTitle: Hierarchical Trust-Region Bayesian Monte-Carlo Planning with Adaptive Continuous Relaxations and Surrogate Ensembles for Sum-to-Difference Ratio Optimization\n\nAbstract:  \nExisting hill-climbing and simulated-annealing baselines (score = 1.3846) for maximizing the sum-to-difference ratio over 0/1\u2010vectors suffer from two critical drawbacks: (1) myopic single-bit updates that frequently stall in deep local optima, and (2) prohibitively expensive full-FFT evaluations. We propose a unified, implementable framework that overcomes these bottlenecks through four synergistic advances:\n\n1. Multi-Scale Surrogate Ensemble with Trust-Region Control  \n \u2022 A global transformer-based graph surrogate captures long-range bit interactions, while localized Gaussian-process refinements quantify uncertainty at fine scales.  \n \u2022 Ensemble variance defines adaptive trust regions, ensuring surrogate\u2010guided steps remain within reliable prediction bounds and reducing missteps.\n\n2. Continuous Relaxation and Differentiable Rounding  \n \u2022 We relax discrete bit indicators into a continuous domain and perform gradient-based optimization via JAX-powered autodiff of the FFT convolution objective.  \n \u2022 Dynamic rounding projects continuous solutions back to discrete space, seeding high-quality proposals that escape plateaus 8\u00d7 faster than bit-flips alone.\n\n3. Hierarchical Monte-Carlo Tree Search with Adaptive Granularity  \n \u2022 A two-tier MCTS alternates between global \u201cblock\u2010segment\u201d proposals (variable\u2010length merges/splits) and local single-bit refinements.  \n \u2022 A regret-minimizing bandit adapts tree width and search depth online, balancing exploration of novel structures against exploitation of high-reward regions.\n\n4. Asynchronous Multi-Fidelity Evaluation Scheduler  \n \u2022 A contextual bandit scheduler allocates compute among: (a) fast surrogate ensemble predictions, (b) continuous-relaxation gradient steps, and (c) exact AVX-512/CUDA FFT kernels.  \n \u2022 By favoring cheap yet informative evaluations...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Here are a few targeted changes that should improve both runtime overhead (by removing Python\u2010level loops and excess copies) and solution quality (by more exploration). \n\n1) Replace the Python\u2010set Minkowski computation with a fully vectorized NumPy version.  \n2) Eliminate unnecessary `.copy()` when accepting new \u201ccurrent\u201d solutions (we only really need to freeze the \u201cbest\u201d solutions).  \n3) Increase iteration budget and slow down the cooling slightly to allow deeper exploration.\n\n```diff\n<<<<<<< SEARCH\n    if ones_A.size * ones_B.size < 1000:\n        # Minkowski sum/difference via Python sets\n        sumset = {i + j for i in ones_A for j in ones_B}\n        difset = {i - j for i in ones_A for j in ones_B}\n        ratio = -1.0 if not difset else len(sumset) / len(difset)\n        ratio_cache[key] = ratio\n        return ratio\n=======\n    if ones_A.size * ones_B.size < 1000:\n        # vectorized Minkowski sum/difference via numpy\n        sums = np.unique(ones_A[:, None] + ones_B[None, :])\n        diffs = np.unique(ones_A[:, None] - ones_B[None, :])\n        ratio = -1.0 if diffs.size == 0 else sums.size / diffs.size\n        ratio_cache[key] = ratio\n        return ratio\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    max_iter = 20000\n=======\n    max_iter = 50000\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    alpha = 0.999   # cooling rate\n=======\n    alpha = 0.9995   # slower cooling rate for better exploration\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n=======\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current (no need to copy; propose_move already gave us a fresh array)\n            current_A, current_B, current_ratio = C_ind, D_ind, ratio\n            # update global best if improved (we still copy best to freeze it)\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n>>>>>>> REPLACE\n```\n\nRationale:  \n- The vectorized Minkowski step avoids Python\u2010level loops and set overhead, making small\u2010support ratio calls significantly faster.  \n- Dropping the redundant `.copy()` on \u201ccurrent\u201d cuts memory churn.  \n- Doubling the iteration count and slowing cooling slightly should help climb out of local optima and yield higher final ratios."]}}}