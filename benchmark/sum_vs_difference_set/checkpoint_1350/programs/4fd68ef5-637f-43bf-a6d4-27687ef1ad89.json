{"id": "4fd68ef5-637f-43bf-a6d4-27687ef1ad89", "code": "import numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n# Pre-bind FFT-based convolution and correlation for performance\nfrom scipy.signal import fftconvolve\nconvolve = lambda a, b, mode='full': fftconvolve(a, b, mode=mode)\ncorrelate = lambda a, b, mode='full': fftconvolve(a, b[::-1], mode=mode)\ncount_nonzero = np.count_nonzero\n# Pre-bind math and rng functions to reduce attribute lookups in inner loops\nexp = math.exp\nrng_random = rng.random\nrng_choice = rng.choice\nflatnonzero = np.flatnonzero\n\n# Global cache for computed ratios\n_ratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # caching enabled with simple LRU eviction\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in _ratio_cache:\n        return _ratio_cache[key]\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # use boolean arrays directly in convolution (interpreted as 0/1)\n    A_arr = A_ind\n    B_arr = B_ind\n    # compute sum convolution and cross-correlation for diff\n    sums = convolve(A_arr, B_arr, mode='full')\n    diffs = correlate(A_arr, B_arr, mode='full')\n    num_sums = count_nonzero(sums)\n    num_diffs = count_nonzero(diffs)\n    if num_diffs == 0:\n        return -1.0\n    result = num_sums / num_diffs\n    # cache result with simple size-based eviction\n    if len(_ratio_cache) >= 10000:\n        _ratio_cache.pop(next(iter(_ratio_cache)))\n    _ratio_cache[key] = result\n    return result\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray, max_k: int = 3) -> np.ndarray:\n    \"\"\"Return a new indicator with one or two bits swapped to improve exploration.\"\"\"\n    new_ind = ind.copy()\n    # adaptively choose number of swaps up to max_k\n    k_swaps = 1 if rng_random() < 0.8 else rng.integers(2, max_k+1)\n    ones = flatnonzero(new_ind)\n    zeros = flatnonzero(~new_ind)\n    if ones.size >= k_swaps and zeros.size >= k_swaps:\n        removes = rng_choice(ones, size=k_swaps, replace=False)\n        adds = rng_choice(zeros, size=k_swaps, replace=False)\n        new_ind[removes] = False\n        new_ind[adds] = True\n    return new_ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n# Search parameters\nBATCH_SIZE = 50               # reduced batch size for more focused local search\nSTAGNATION_THRESHOLD = 500    # increased threshold to allow deeper exploration before restart\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind = A_ind.copy()\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 5.0                # higher initial temperature for broader exploration\n    decay = 0.9999        # slower cooling rate to avoid premature convergence\n\n    # Local search: random single\u2010bit flips with simple stagnation escape\n    max_iter = 20000\n    stagnation = 0\n    stagnation_threshold = STAGNATION_THRESHOLD  # restart after threshold steps without improvement\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng_random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store references to reduce memory copies; defer deep copy until acceptance\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng_random() < exp(delta / T):\n            current_ratio = local_best_ratio\n            # Copy accepted proposal to decouple from local buffers\n            current_A, current_B = local_best_A.copy(), local_best_B.copy()\n        # update global best and stagnation handling\n        if current_ratio > best_ratio:\n            best_ratio = current_ratio\n            best_A, best_B = current_A.copy(), current_B.copy()\n            stagnation = 0\n        else:\n            stagnation += 1\n\n        # simple escape: if stuck, do a double\u2010swap perturbation\n        if stagnation >= stagnation_threshold:\n            current_A = propose_move(propose_move(current_A))\n            current_B = propose_move(propose_move(current_B))\n            current_ratio = compute_ratio(current_A, current_B)\n            stagnation = 0\n\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: Hybrid FFT-Accelerated Surrogate-Assisted Variable-Neighborhood Search for Improving Sum-to-Difference Ratios\n\nAbstract  \nWe address two primary weaknesses of the current hill-climbing/annealing approach\u2014its O(n\u00b2) cost per evaluation and its tendency to stagnate under uniform single-swap moves\u2014by introducing a hybrid metaheuristic that leverages (1) fast, incremental convolution via FFTs and caching, (2) adaptive multi-swap neighborhoods, (3) an online surrogate model to guide proposals, and (4) parallel tempering for robust global exploration.  \n\n1. FFT-Accelerated, Incremental Ratio Computation  \n  \u2022 Replace direct np.convolve/np.correlate calls with SciPy\u2019s FFT-based convolution for O(n log n) performance.  \n  \u2022 Maintain and update the convolution result incrementally when bits flip, reducing repeated global recomputation.  \n  \u2022 Implement an LRU cache for recently evaluated (A_ind, B_ind) pairs to avoid redundant evaluations.  \n\n2. Variable-Neighborhood and Tabu-Enhanced Move Proposals  \n  \u2022 Generalize the propose_move operator to k-swaps, with k adaptively increased (1\u21925) upon detected stagnation.  \n  \u2022 Introduce a small tabu list of recent bit changes to prevent immediate reversal and encourage exploration of new regions.  \n\n3. Surrogate-Assisted Candidate Screening  \n  \u2022 Collect lightweight feature vectors (e.g., local contribution sums around flipped indices) during search.  \n  \u2022 Train an online linear regression (or Gaussian process for larger N) to predict \u0394ratio for each proposal.  \n  \u2022 Pre-filter the BATCH_SIZE proposals using surrogate scores, only invoking the full FFT-based ratio for the top candidates.  \n\n4. Adaptive Simulated Annealing and Parallel Tempering  \n  \u2022 Employ a Fermi-Dirac cooling schedule whose temperature update adapts to the recent acceptance rate, preventing premature freezing.  \n  \u2022 Run multiple chains in parallel with periodic state exchanges (parallel tempering) to further escape local optima.  \n\nImplementation is straightforward in Python using NumPy/SciPy (FFT), a small LRU cache, on-the-fly sklearn.linear_model for surrogates, and standard multiprocessing for parallel chains. We anticipate a \u226520 % improvement over the current 1.5588 ratio benchmark on N=30, extended validation on larger N up to 100, and a 30 % reduction in wall-clock time."], "idea_reward": 5.67, "parent_id": "f375038f-3a57-4def-8590-c5ea7200e4e2_migrant_0_migrant_4_migrant_3", "generation": 21, "timestamp": 1757814947.697726, "iteration_found": 1179, "metrics": {"score": 0.9629629629629629}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 3 lines with 4 lines\nChange 2: Replace 4 lines with 4 lines\nChange 3: Replace 2 lines with 6 lines\nChange 4: 'def propose_move(ind: np.ndarray) -> np.ndarray:' to 'def propose_move(ind: np.ndarray, max_k: int = 3) -> np.ndarray:'\nChange 5: Replace new_ind = ind.copy() with 3 lines\nChange 6: Replace 15 lines with 8 lines", "parent_metrics": {"score": 1.5588235294117647}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \n\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n# Pre-bind numpy and rng functions for performance\nconvolve = np.convolve\ncorrelate = np.correlate\ncount_nonzero = np.count_nonzero\n# Pre-bind math and rng functions to reduce attribute lookups in inner loops\nexp = math.exp\nrng_random = rng.random\nrng_choice = rng.choice\nflatnonzero = np.flatnonzero\n\n# Global cache for computed ratios\n_ratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # caching disabled for performance\n    # key = (A_ind.tobytes(), B_ind.tobytes())\n    # if key in _ratio_cache:\n    #     return _ratio_cache[key]\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # use boolean arrays directly in convolution (interpreted as 0/1)\n    A_arr = A_ind\n    B_arr = B_ind\n    # compute sum convolution and cross-correlation for diff\n    sums = convolve(A_arr, B_arr, mode='full')\n    diffs = correlate(A_arr, B_arr, mode='full')\n    num_sums = count_nonzero(sums)\n    num_diffs = count_nonzero(diffs)\n    if num_diffs == 0:\n        return -1.0\n    result = num_sums / num_diffs\n    return result\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Return a new indicator with one or two bits swapped to improve exploration.\"\"\"\n    new_ind = ind.copy()\n    ones = flatnonzero(new_ind)\n    zeros = flatnonzero(~new_ind)\n    if ones.size and zeros.size:\n        # with small probability do a two-bit swap for larger jumps\n        if rng_random() < 0.2 and ones.size > 1 and zeros.size > 1:\n            removes = rng_choice(ones, size=2, replace=False)\n            adds = rng_choice(zeros, size=2, replace=False)\n            new_ind[removes] = False\n            new_ind[adds] = True\n        else:\n            i_remove = rng_choice(ones)\n            i_add = rng_choice(zeros)\n            new_ind[i_remove] = False\n            new_ind[i_add] = True\n    return new_ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n# Search parameters\nBATCH_SIZE = 50               # reduced batch size for more focused local search\nSTAGNATION_THRESHOLD = 500    # increased threshold to allow deeper exploration before restart\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind = A_ind.copy()\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 5.0                # higher initial temperature for broader exploration\n    decay = 0.9999        # slower cooling rate to avoid premature convergence\n\n    # Local search: random single\u2010bit flips with simple stagnation escape\n    max_iter = 20000\n    stagnation = 0\n    stagnation_threshold = STAGNATION_THRESHOLD  # restart after threshold steps without improvement\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng_random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store references to reduce memory copies; defer deep copy until acceptance\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng_random() < exp(delta / T):\n            current_ratio = local_best_ratio\n            # Copy accepted proposal to decouple from local buffers\n            current_A, current_B = local_best_A.copy(), local_best_B.copy()\n        # update global best and stagnation handling\n        if current_ratio > best_ratio:\n            best_ratio = current_ratio\n            best_A, best_B = current_A.copy(), current_B.copy()\n            stagnation = 0\n        else:\n            stagnation += 1\n\n        # simple escape: if stuck, do a double\u2010swap perturbation\n        if stagnation >= stagnation_threshold:\n            current_A = propose_move(propose_move(current_A))\n            current_B = propose_move(propose_move(current_B))\n            current_ratio = compute_ratio(current_A, current_B)\n            stagnation = 0\n\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.5588\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 2 (Score: 1.5161)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5161)\n\n\n### Program 3 (Score: 1.5312)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5312)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: 1.4333)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math  # added for simulated annealing acceptance\n\n# For reproducibility\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nTitle: Hybrid FFT-Accelerated Surrogate-Assisted Variable-Neighborhood Search for Improving Sum-to-Difference Ratios\n\nAbstract  \nWe address two primary weaknesses of the current hill-climbing/annealing approach\u2014its O(n\u00b2) cost per evaluation and its tendency to stagnate under uniform single-swap moves\u2014by introducing a hybrid metaheuristic that leverages (1) fast, incremental convolution via FFTs and caching, (2) adaptive multi-swap neighborhoods, (3) an online surrogate model to guide proposals, and (4) parallel tempering for robust global exploration.  \n\n1. FFT-Accelerated, Incremental Ratio Computation  \n  \u2022 Replace direct np.convolve/np.correlate calls with SciPy\u2019s FFT-based convolution for O(n log n) performance.  \n  \u2022 Maintain and update the convolution result incrementally when bits flip, reducing repeated global recomputation.  \n  \u2022 Implement an LRU cache for recently evaluated (A_ind, B_ind) pairs to avoid redundant evaluations.  \n\n2. Variable-Neighborhood and Tabu-Enhanced Move Proposals  \n  \u2022 Generalize the propose_move operator to k-swaps, with k adaptively increased (1\u21925) upon detected stagnation.  \n  \u2022 Introduce a small tabu list of recent bit changes to prevent immediate reversal and encourage exploration of new regions.  \n\n3. Surrogate-Assisted Candidate Screening  \n  \u2022 Collect lightweight feature vectors (e.g., local contribution sums around flipped indices) during search.  \n  \u2022 Train an online linear regression (or Gaussian process for larger N) to predict \u0394ratio for each proposal.  \n  \u2022 Pre-filter the BATCH_SIZE proposals using surrogate scores, only invoking the full FFT-based ratio for the top candidates.  \n\n4. Adaptive Simulated Annealing and Parallel Tempering  \n  \u2022 Employ a Fermi-Dirac cooling schedule whose temperature update adapts to the recent acceptance rate, preventing premature freezing.  \n  \u2022 Run multiple chains in parallel with periodic state exchanges (parallel tempering) to further escape local optima.  \n\nImplementation is straightfo...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are targeted diffs to introduce FFT-based convolutions for faster O(n log n) evaluation, enable simple LRU caching in `compute_ratio`, and make `propose_move` use an adaptive k-swap neighborhood to escape local minima more effectively.\n\n<<<<<<< SEARCH\n# Pre-bind numpy and rng functions for performance\nconvolve = np.convolve\ncorrelate = np.correlate\n=======\n# Pre-bind FFT-based convolution and correlation for performance\nfrom scipy.signal import fftconvolve\nconvolve = lambda a, b, mode='full': fftconvolve(a, b, mode=mode)\ncorrelate = lambda a, b, mode='full': fftconvolve(a, b[::-1], mode=mode)\n>>>>>>> REPLACE\n\n---\n\n<<<<<<< SEARCH\n    # caching disabled for performance\n    # key = (A_ind.tobytes(), B_ind.tobytes())\n    # if key in _ratio_cache:\n    #     return _ratio_cache[key]\n=======\n    # caching enabled with simple LRU eviction\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in _ratio_cache:\n        return _ratio_cache[key]\n>>>>>>> REPLACE\n\n---\n\n<<<<<<< SEARCH\n    result = num_sums / num_diffs\n    return result\n=======\n    result = num_sums / num_diffs\n    # cache result with simple size-based eviction\n    if len(_ratio_cache) >= 10000:\n        _ratio_cache.pop(next(iter(_ratio_cache)))\n    _ratio_cache[key] = result\n    return result\n>>>>>>> REPLACE\n\n---\n\n<<<<<<< SEARCH\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n=======\ndef propose_move(ind: np.ndarray, max_k: int = 3) -> np.ndarray:\n>>>>>>> REPLACE\n\n---\n\n<<<<<<< SEARCH\n    new_ind = ind.copy()\n=======\n    new_ind = ind.copy()\n    # adaptively choose number of swaps up to max_k\n    k_swaps = 1 if rng_random() < 0.8 else rng.integers(2, max_k+1)\n>>>>>>> REPLACE\n\n---\n\n<<<<<<< SEARCH\n    ones = flatnonzero(new_ind)\n    zeros = flatnonzero(~new_ind)\n    if ones.size and zeros.size:\n        # with small probability do a two-bit swap for larger jumps\n        if rng_random() < 0.2 and ones.size > 1 and zeros.size > 1:\n            removes = rng_choice(ones, size=2, replace=False)\n            adds = rng_choice(zeros, size=2, replace=False)\n            new_ind[removes] = False\n            new_ind[adds] = True\n        else:\n            i_remove = rng_choice(ones)\n            i_add = rng_choice(zeros)\n            new_ind[i_remove] = False\n            new_ind[i_add] = True\n    return new_ind\n=======\n    ones = flatnonzero(new_ind)\n    zeros = flatnonzero(~new_ind)\n    if ones.size >= k_swaps and zeros.size >= k_swaps:\n        removes = rng_choice(ones, size=k_swaps, replace=False)\n        adds = rng_choice(zeros, size=k_swaps, replace=False)\n        new_ind[removes] = False\n        new_ind[adds] = True\n    return new_ind\n>>>>>>> REPLACE\n\nExplanation of changes:\n\n- Switched from `np.convolve`/`np.correlate` to `scipy.signal.fftconvolve` for O(n log n) performance.\n- Turned on caching in `compute_ratio` with a capped LRU cache to avoid redundant expensive evaluations.\n- Expanded `propose_move` to randomly choose between 1 and `max_k` simultaneous bit\u2010swaps (adaptive k-swap) to better escape stagnation."]}}}