{"id": "d24684aa-a8c7-4c31-852e-fdf6a132b413", "code": "import numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nexp = math.exp\ncnz = np.count_nonzero\n\n# Cache for computed ratios to avoid redundant evaluations\nratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # Multi-call caching\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in ratio_cache:\n        return ratio_cache[key]\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        ratio = -1.0\n    else:\n        # arrays are already int8\n        A_arr = A_ind\n        B_arr = B_ind\n        # direct cardinalities: union and symmetric\u2010difference for speed\n        num_sums = int(cnz(A_arr | B_arr))\n        num_diffs = int(cnz(A_arr ^ B_arr))\n        if num_diffs == 0:\n            ratio = -1.0\n        else:\n            ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move to maintain constant cardinality and improve exploration.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(ind == 0)\n    if ones.size and zeros.size:\n        # with small probability do a two-bit swap for larger jumps\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:  # higher chance for multi-bit swaps\n            removes = rng.choice(ones, size=2, replace=False)\n            adds = rng.choice(zeros, size=2, replace=False)\n            ind[removes] = False\n            ind[adds] = True\n        else:\n            i_remove = rng.choice(ones)\n            i_add = rng.choice(zeros)\n            ind[i_remove] = False\n            ind[i_add] = True\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 20  # increased number of local proposals for better exploration\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=np.int8)\n    B_ind = np.zeros(N, dtype=np.int8)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9999        # slightly slower cooling for extended exploration\n    # restart logic to escape stagnation\n    no_improve = 0\n    max_no_improve = 5000\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng_random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng_random() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n            no_improve = 0\n        else:\n            no_improve += 1\n        # cool down\n        T *= decay\n        # restart when stagnated\n        if no_improve >= max_no_improve:\n            current_A, current_B = best_A.copy(), best_B.copy()\n            current_ratio = best_ratio\n            T = 1.0\n            no_improve = 0\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: FlowMetaOpt: Normalizing\u2010Flow\u2010Guided Meta\u2010Optimization with Multi\u2010Fidelity Bayesian Surrogates and Adaptive Annealing for Extremal Sum\u2010to\u2010Difference Ratio\n\nAbstract  \nCurrent hill\u2010climbing and static\u2010schedule annealing methods plateau at a \u223c1.53 sum\u2010to\u2010difference ratio on size\u201030 benchmarks, primarily due to (i) myopic, single\u2010scale bit\u2010swap proposals, (ii) rigid fidelity schedules in surrogate evaluation, (iii) hand\u2010tuned temperature decays that fail to adapt to search dynamics, and (iv) limited global structure awareness. We introduce FlowMetaOpt, a modular, distributed meta\u2010optimization framework that overcomes these bottlenecks through four key innovations:\n\n1. Normalizing\u2010Flow\u2010Guided Global Proposals  \n \u2022 A graph\u2010neural encoder maps indicator\u2010vector states into a continuous latent space.  \n \u2022 A trained normalizing flow generates variable\u2010scale, high\u2010diversity move proposals (from single\u2010bit flips to eight\u2010bit reconfigurations) that are globally informed by prior search experience.\n\n2. Multi\u2010Fidelity Bayesian Surrogate Ensemble  \n \u2022 We replace fixed\u2010schedule ensembles with a Gaussian\u2010process\u2010based Bayesian surrogate hierarchy.  \n \u2022 An Upper\u2010Confidence\u2010Bound acquisition rule dynamically allocates evaluations across cheap linear proxies, mid\u2010cost GNN regressors, and exact computations, ensuring theoretical error bounds on estimated ratios.\n\n3. Meta\u2010Gradient\u2010On\u2010The\u2010Fly Annealing  \n \u2022 We embed search\u2010state statistics (acceptance rate, surrogate uncertainty, proposal entropy) into a small recurrent meta\u2010network.  \n \u2022 Through online meta\u2010gradients, it continuously adjusts inverse\u2010temperature and proposal\u2010mixing weights, automatically balancing intensification and diversification without manual tuning.\n\n4. Differentiable Relaxation Seeding and Batch Bayesian Refinement  \n \u2022 Continuous relaxations of the sum\u2010to\u2010difference objective yield gradient\u2010based seed states that jump\u2010start discrete search.  \n \u2022 A batch\u2010Bayesian\u2010optimization refinement stage evaluates top\u2010k seeds via the multi\u2010fidelity surrogate, guaranteeing precise candidate improvements before expensive exact calls.\n\nImplementation & Impact  \nFlowMetaOpt is implemented in JAX (surrogates, normalizing flows) and PyTorch (meta\u2010network), orchestrated via Ray for asynchronous, population\u2010based distributed search. On N=30\u2013100 benchmarks, we target ratios >1.9 within 5\u00d7 the current compute budget. Its plug\u2010and\u2010play design and rigorous uncertainty controls make it broadly applicable to a wide class of extremal combinatorial optimization tasks."], "idea_reward": 7.0, "parent_id": "9ea44786-cfaf-4b28-85f7-c454fb2f99ed", "generation": 19, "timestamp": 1757825149.9660075, "iteration_found": 1285, "metrics": {"score": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 5 lines with 3 lines", "parent_metrics": {"score": 1.53125}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: HyperOpt\u0394: Hierarchical Transformer-GFlowNet Meta\u2010Optimization with Active Fidelity and Conformal Surrogates for Extremal Sum\u2010to\u2010Difference Ratio\n\nAbstract  \nDespite recent gains, hill\u2010climbing and static\u2010schedule annealing methods stall near a 1.53 sum\u2010to\u2010difference ratio on N=30 benchmarks. We attribute this plateau to (i) myopic bit\u2010swap proposals, (ii) fixed\u2010fidelity surrogate calls, and (iii) hand\u2010tuned temperature schedules. HyperOpt\u0394 overcomes these bottlenecks through four key innovations:\n\n1. Hierarchical Transformer\u2010GFlowNet Policy  \n \u2022 We embed indicator vectors into a multi\u2010scale graph Transformer that captures local and global structure.  \n \u2022 A hierarchical GFlowNet learns to propose variable\u2010length, high\u2010reward move sequences\u2014ranging from single\u2010bit flips to eight\u2010bit reconfigurations\u2014proportional to their Boltzmann posterior.  \n\n2. Active Fidelity Scheduling with Conformal Uncertainty  \n \u2022 We replace fixed multi\u2010fidelity ensembles with an active\u2010learning controller that issues conformal prediction bands around surrogate outputs.  \n \u2022 A Thompson\u2010sampling bandit selects among cheap linear proxies, mid\u2010cost geometric graph nets, and exact evaluations, dynamically balancing exploration and exploitation with rigorous error guarantees.  \n\n3. Meta\u2010Learnt Temperature & Exploration Prior  \n \u2022 A meta\u2010RL agent, trained across N={30,50,80,120}, adjusts both the inverse\u2010temperature and Transformer\u2010attention dropout in real time.  \n \u2022 By observing acceptance rate, surrogate calibration, and GFlowNet sample diversity, it self\u2010regulates intensification and diversification to escape local traps.  \n\n4. Differentiable Relaxation Warm\u2010Start & Beam\u2010Search Refinement  \n \u2022 Continuous spectral relaxations of the sum\u2010to\u2010difference objective provide gradient\u2010based warm starts, seeding the GFlowNet with promising fractional solutions.  \n \u2022 A beam\u2010search refinement stage evaluates top\u2010k proposals via the active\u2010fidelity ensemble, ensuring precise improvements before commi...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nexp = math.exp\ncnz = np.count_nonzero\n\n# Cache for computed ratios to avoid redundant evaluations\nratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # Multi-call caching\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in ratio_cache:\n        return ratio_cache[key]\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        ratio = -1.0\n    else:\n        # arrays are already int8\n        A_arr = A_ind\n        B_arr = B_ind\n        # use prebound functions to reduce attribute lookups\n        sums = convolve_fn(A_arr, B_arr)\n        num_sums = int((sums > 0).sum())\n        diffs = correlate_fn(A_arr, B_arr, mode='full')\n        num_diffs = int((diffs > 0).sum())\n        if num_diffs == 0:\n            ratio = -1.0\n        else:\n            ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move to maintain constant cardinality and improve exploration.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(ind == 0)\n    if ones.size and zeros.size:\n        # with small probability do a two-bit swap for larger jumps\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:  # higher chance for multi-bit swaps\n            removes = rng.choice(ones, size=2, replace=False)\n            adds = rng.choice(zeros, size=2, replace=False)\n            ind[removes] = False\n            ind[adds] = True\n        else:\n            i_remove = rng.choice(ones)\n            i_add = rng.choice(zeros)\n            ind[i_remove] = False\n            ind[i_add] = True\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 20  # increased number of local proposals for better exploration\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=np.int8)\n    B_ind = np.zeros(N, dtype=np.int8)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9999        # slightly slower cooling for extended exploration\n    # restart logic to escape stagnation\n    no_improve = 0\n    max_no_improve = 5000\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng_random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng_random() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n            no_improve = 0\n        else:\n            no_improve += 1\n        # cool down\n        T *= decay\n        # restart when stagnated\n        if no_improve >= max_no_improve:\n            current_A, current_B = best_A.copy(), best_B.copy()\n            current_ratio = best_ratio\n            T = 1.0\n            no_improve = 0\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.5312\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 2 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5455)\n\n\n### Program 3 (Score: 1.4516)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check on non-empty sets\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.4516)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.5161)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: 1.2778)\n```python\nimport numpy as np\nconv = np.convolve\ncount_nonzero = np.count_nonzero\nrfft = np.fft.rfft\nirfft = np.fft.irfft\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nTitle: FlowMetaOpt: Normalizing\u2010Flow\u2010Guided Meta\u2010Optimization with Multi\u2010Fidelity Bayesian Surrogates and Adaptive Annealing for Extremal Sum\u2010to\u2010Difference Ratio\n\nAbstract  \nCurrent hill\u2010climbing and static\u2010schedule annealing methods plateau at a \u223c1.53 sum\u2010to\u2010difference ratio on size\u201030 benchmarks, primarily due to (i) myopic, single\u2010scale bit\u2010swap proposals, (ii) rigid fidelity schedules in surrogate evaluation, (iii) hand\u2010tuned temperature decays that fail to adapt to search dynamics, and (iv) limited global structure awareness. We introduce FlowMetaOpt, a modular, distributed meta\u2010optimization framework that overcomes these bottlenecks through four key innovations:\n\n1. Normalizing\u2010Flow\u2010Guided Global Proposals  \n \u2022 A graph\u2010neural encoder maps indicator\u2010vector states into a continuous latent space.  \n \u2022 A trained normalizing flow generates variable\u2010scale, high\u2010diversity move proposals (from single\u2010bit flips to eight\u2010bit reconfigurations) that are globally informed by prior search experience.\n\n2. Multi\u2010Fidelity Bayesian Surrogate Ensemble  \n \u2022 We replace fixed\u2010schedule ensembles with a Gaussian\u2010process\u2010based Bayesian surrogate hierarchy.  \n \u2022 An Upper\u2010Confidence\u2010Bound acquisition rule dynamically allocates evaluations across cheap linear proxies, mid\u2010cost GNN regressors, and exact computations, ensuring theoretical error bounds on estimated ratios.\n\n3. Meta\u2010Gradient\u2010On\u2010The\u2010Fly Annealing  \n \u2022 We embed search\u2010state statistics (acceptance rate, surrogate uncertainty, proposal entropy) into a small recurrent meta\u2010network.  \n \u2022 Through online meta\u2010gradients, it continuously adjusts inverse\u2010temperature and proposal\u2010mixing weights, automatically balancing intensification and diversification without manual tuning.\n\n4. Differentiable Relaxation Seeding and Batch Bayesian Refinement  \n \u2022 Continuous relaxations of the sum\u2010to\u2010difference objective yield gradient\u2010based seed states that jump\u2010start discrete search.  \n \u2022 A batch\u2010Bayesian\u2010optimization refinement stage evaluates top\u2010k...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below is a targeted change to `compute_ratio` that replaces the expensive convolution/correlation calls with simple bit\u2010wise operations. Since both \u201csum\u201d and \u201cdifference\u201d in our problem reduce to set\u2010union and symmetric\u2010difference cardinalities on two 0/1 vectors, we can compute them directly with `|` and `^` plus a single `count_nonzero` per operation. This should dramatically reduce per\u2010call overhead and speed up the entire search.\n\n<<<<<<< SEARCH\n        # use prebound functions to reduce attribute lookups\n        sums = convolve_fn(A_arr, B_arr)\n        num_sums = int((sums > 0).sum())\n        diffs = correlate_fn(A_arr, B_arr, mode='full')\n        num_diffs = int((diffs > 0).sum())\n=======\n        # direct cardinalities: union and symmetric\u2010difference for speed\n        num_sums = int(cnz(A_arr | B_arr))\n        num_diffs = int(cnz(A_arr ^ B_arr))\n>>>>>>> REPLACE\n\nReasoning:\n- Bitwise OR (`|`) gives the union of ones in `A_arr` and `B_arr`, whose count is exactly `|A+B|`.\n- Bitwise XOR (`^`) gives the symmetric difference, whose count is `|A\u2212B|`.\n- Both operations are single\u2010pass, highly optimized C loops inside NumPy, and eliminate the much heavier convolution/correlation machinery.\n- We still handle the zero\u2010denominator case below, so the rest of the logic remains untouched."]}}}