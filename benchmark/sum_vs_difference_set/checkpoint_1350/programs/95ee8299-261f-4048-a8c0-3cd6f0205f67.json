{"id": "95ee8299-261f-4048-a8c0-3cd6f0205f67", "code": "import numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nexp = math.exp\ncnz = np.count_nonzero\n\n# Cache for computed ratios to avoid redundant evaluations\nratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # caching disabled: bitwise ops are extremely fast and avoid hash overhead\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        ratio = -1.0\n    else:\n        # arrays are already int8\n        A_arr = A_ind\n        B_arr = B_ind\n        # use prebound functions to reduce attribute lookups\n        # Vectorized bitwise operations for faster union/difference counts\n        num_sums = int(cnz(A_arr | B_arr))\n        num_diffs = int(cnz(A_arr != B_arr))\n        if num_diffs == 0:\n            ratio = -1.0\n        else:\n            ratio = num_sums / num_diffs\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move to maintain constant cardinality and improve exploration.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(ind == 0)\n    if ones.size and zeros.size:\n        # with small probability do a two-bit swap for larger jumps\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:  # higher chance for multi-bit swaps\n            removes = rng.choice(ones, size=2, replace=False)\n            adds = rng.choice(zeros, size=2, replace=False)\n            ind[removes] = False\n            ind[adds] = True\n        else:\n            i_remove = rng.choice(ones)\n            i_add = rng.choice(zeros)\n            ind[i_remove] = False\n            ind[i_add] = True\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 20  # increased number of local proposals for better exploration\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=np.int8)\n    B_ind = np.zeros(N, dtype=np.int8)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9999        # slightly slower cooling for extended exploration\n    # restart logic to escape stagnation\n    no_improve = 0\n    max_no_improve = 5000\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng_random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng_random() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n            no_improve = 0\n        else:\n            no_improve += 1\n        # cool down\n        T *= decay\n        # restart when stagnated\n        if no_improve >= max_no_improve:\n            current_A, current_B = best_A.copy(), best_B.copy()\n            current_ratio = best_ratio\n            T = 1.0\n            no_improve = 0\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: GRAFOX: Graph\u2010Adaptive Reinforcement and Fidelity Optimization for Extremal Sum\u2010to\u2010Difference Ratio  \n\nAbstract  \nExisting hill\u2010climbing and annealing\u2010based methods plateau near a 1.53 sum-to-difference ratio on N=30 benchmarks, largely because (i) moves are myopic single- or double-bit swaps, (ii) fidelity of surrogate evaluations is fixed, and (iii) temperature schedules are hand-tuned and static. To overcome these bottlenecks, we propose GRAFOX, a unified meta\u2010optimization framework that combines learned graph representations, adaptive fidelity control, and differentiable warm starts.  \n\nKey innovations:  \n1. Learned Graph\u2010Transformer Proposal Network  \n \u2022 We represent indicator\u2010vector pairs as bipartite graphs and train a small Graph Transformer policy via Reinforcement Learning to propose k\u2010step swap/add/remove sequences.  \n \u2022 The policy is rewarded not only on immediate ratio gains but also on predicted downstream improvement using a value\u2010network critic, enabling non\u2010myopic exploration of high-reward neighborhoods.  \n\n2. Bayesian Multi\u2010Fidelity Surrogate with Embedded Uncertainty  \n \u2022 We replace fixed\u2010cost surrogates with a two\u2010level ensemble\u2014a fast linear proxy and a mid\u2010cost geometric graph net\u2014augmented by a lightweight Gaussian Process uncertainty head.  \n \u2022 A Thompson\u2010sampling controller dynamically allocates evaluations, querying the exact ratio only when the surrogate\u2019s confidence interval straddles a performance threshold.  \n\n3. Differentiable Relaxation Warm\u2010Start & Meta\u2010Learnt Temperature Controller  \n \u2022 Continuous spectral relaxations produce fractional solutions whose gradients initialize the Graph Transformer\u2019s hidden states, biasing proposals toward promising subspaces.  \n \u2022 A small LSTM meta\u2010learner, trained across N\u2208{30,50,80,120}, continuously adjusts inverse\u2010temperature and proposal entropy based on surrogate calibration and acceptance statistics.  \n\nImplementation & Impact  \nWe will implement GRAFOX in JAX for surrogates and PyTorch for RL components, orchestrated via Ray. On N=30\u2013100, we target ratios >1.8 within 5\u00d7 the current compute budget. Our modular design and rigorous uncertainty guarantees make GRAFOX directly applicable to a broad class of extremal combinatorial optimization problems."], "idea_reward": 6.0, "parent_id": "9ea44786-cfaf-4b28-85f7-c454fb2f99ed", "generation": 19, "timestamp": 1757819266.7075436, "iteration_found": 1221, "metrics": {"score": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 8 lines with 7 lines\nChange 2: Replace 4 lines with # caching disabled: bitwise ops are extremely fast and avoid hash overhead\nChange 3: Replace 2 lines with return ratio", "parent_metrics": {"score": 1.53125}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: HyperOpt\u0394: Hierarchical Transformer-GFlowNet Meta\u2010Optimization with Active Fidelity and Conformal Surrogates for Extremal Sum\u2010to\u2010Difference Ratio\n\nAbstract  \nDespite recent gains, hill\u2010climbing and static\u2010schedule annealing methods stall near a 1.53 sum\u2010to\u2010difference ratio on N=30 benchmarks. We attribute this plateau to (i) myopic bit\u2010swap proposals, (ii) fixed\u2010fidelity surrogate calls, and (iii) hand\u2010tuned temperature schedules. HyperOpt\u0394 overcomes these bottlenecks through four key innovations:\n\n1. Hierarchical Transformer\u2010GFlowNet Policy  \n \u2022 We embed indicator vectors into a multi\u2010scale graph Transformer that captures local and global structure.  \n \u2022 A hierarchical GFlowNet learns to propose variable\u2010length, high\u2010reward move sequences\u2014ranging from single\u2010bit flips to eight\u2010bit reconfigurations\u2014proportional to their Boltzmann posterior.  \n\n2. Active Fidelity Scheduling with Conformal Uncertainty  \n \u2022 We replace fixed multi\u2010fidelity ensembles with an active\u2010learning controller that issues conformal prediction bands around surrogate outputs.  \n \u2022 A Thompson\u2010sampling bandit selects among cheap linear proxies, mid\u2010cost geometric graph nets, and exact evaluations, dynamically balancing exploration and exploitation with rigorous error guarantees.  \n\n3. Meta\u2010Learnt Temperature & Exploration Prior  \n \u2022 A meta\u2010RL agent, trained across N={30,50,80,120}, adjusts both the inverse\u2010temperature and Transformer\u2010attention dropout in real time.  \n \u2022 By observing acceptance rate, surrogate calibration, and GFlowNet sample diversity, it self\u2010regulates intensification and diversification to escape local traps.  \n\n4. Differentiable Relaxation Warm\u2010Start & Beam\u2010Search Refinement  \n \u2022 Continuous spectral relaxations of the sum\u2010to\u2010difference objective provide gradient\u2010based warm starts, seeding the GFlowNet with promising fractional solutions.  \n \u2022 A beam\u2010search refinement stage evaluates top\u2010k proposals via the active\u2010fidelity ensemble, ensuring precise improvements before commi...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nexp = math.exp\ncnz = np.count_nonzero\n\n# Cache for computed ratios to avoid redundant evaluations\nratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # Multi-call caching\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in ratio_cache:\n        return ratio_cache[key]\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        ratio = -1.0\n    else:\n        # arrays are already int8\n        A_arr = A_ind\n        B_arr = B_ind\n        # use prebound functions to reduce attribute lookups\n        sums = convolve_fn(A_arr, B_arr)\n        num_sums = int((sums > 0).sum())\n        diffs = correlate_fn(A_arr, B_arr, mode='full')\n        num_diffs = int((diffs > 0).sum())\n        if num_diffs == 0:\n            ratio = -1.0\n        else:\n            ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move to maintain constant cardinality and improve exploration.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(ind == 0)\n    if ones.size and zeros.size:\n        # with small probability do a two-bit swap for larger jumps\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:  # higher chance for multi-bit swaps\n            removes = rng.choice(ones, size=2, replace=False)\n            adds = rng.choice(zeros, size=2, replace=False)\n            ind[removes] = False\n            ind[adds] = True\n        else:\n            i_remove = rng.choice(ones)\n            i_add = rng.choice(zeros)\n            ind[i_remove] = False\n            ind[i_add] = True\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 20  # increased number of local proposals for better exploration\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=np.int8)\n    B_ind = np.zeros(N, dtype=np.int8)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9999        # slightly slower cooling for extended exploration\n    # restart logic to escape stagnation\n    no_improve = 0\n    max_no_improve = 5000\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng_random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng_random() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n            no_improve = 0\n        else:\n            no_improve += 1\n        # cool down\n        T *= decay\n        # restart when stagnated\n        if no_improve >= max_no_improve:\n            current_A, current_B = best_A.copy(), best_B.copy()\n            current_ratio = best_ratio\n            T = 1.0\n            no_improve = 0\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.5312\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 2 (Score: 1.5000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nexp = math.exp\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5000)\n\n\n### Program 3 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\nimport functools\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5455)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.5000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nTitle: GRAFOX: Graph\u2010Adaptive Reinforcement and Fidelity Optimization for Extremal Sum\u2010to\u2010Difference Ratio  \n\nAbstract  \nExisting hill\u2010climbing and annealing\u2010based methods plateau near a 1.53 sum-to-difference ratio on N=30 benchmarks, largely because (i) moves are myopic single- or double-bit swaps, (ii) fidelity of surrogate evaluations is fixed, and (iii) temperature schedules are hand-tuned and static. To overcome these bottlenecks, we propose GRAFOX, a unified meta\u2010optimization framework that combines learned graph representations, adaptive fidelity control, and differentiable warm starts.  \n\nKey innovations:  \n1. Learned Graph\u2010Transformer Proposal Network  \n \u2022 We represent indicator\u2010vector pairs as bipartite graphs and train a small Graph Transformer policy via Reinforcement Learning to propose k\u2010step swap/add/remove sequences.  \n \u2022 The policy is rewarded not only on immediate ratio gains but also on predicted downstream improvement using a value\u2010network critic, enabling non\u2010myopic exploration of high-reward neighborhoods.  \n\n2. Bayesian Multi\u2010Fidelity Surrogate with Embedded Uncertainty  \n \u2022 We replace fixed\u2010cost surrogates with a two\u2010level ensemble\u2014a fast linear proxy and a mid\u2010cost geometric graph net\u2014augmented by a lightweight Gaussian Process uncertainty head.  \n \u2022 A Thompson\u2010sampling controller dynamically allocates evaluations, querying the exact ratio only when the surrogate\u2019s confidence interval straddles a performance threshold.  \n\n3. Differentiable Relaxation Warm\u2010Start & Meta\u2010Learnt Temperature Controller  \n \u2022 Continuous spectral relaxations produce fractional solutions whose gradients initialize the Graph Transformer\u2019s hidden states, biasing proposals toward promising subspaces.  \n \u2022 A small LSTM meta\u2010learner, trained across N\u2208{30,50,80,120}, continuously adjusts inverse\u2010temperature and proposal entropy based on surrogate calibration and acceptance statistics.  \n\nImplementation & Impact  \nWe will implement GRAFOX in JAX for surrogates and PyTorch ...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Here are two targeted diffs to accelerate the critical `compute_ratio` path and remove caching overhead, which can actually dominate runtime now that we use very fast bit\u2010operations. This should both simplify the code and improve overall score.\n\n1) Replace the convolution/correlation logic with pure bitwise OR/XOR counts:\n<<<<<<< SEARCH\n        sums = convolve_fn(A_arr, B_arr)\n        num_sums = int((sums > 0).sum())\n        diffs = correlate_fn(A_arr, B_arr, mode='full')\n        num_diffs = int((diffs > 0).sum())\n        if num_diffs == 0:\n            ratio = -1.0\n        else:\n            ratio = num_sums / num_diffs\n=======\n        # Vectorized bitwise operations for faster union/difference counts\n        num_sums = int(cnz(A_arr | B_arr))\n        num_diffs = int(cnz(A_arr != B_arr))\n        if num_diffs == 0:\n            ratio = -1.0\n        else:\n            ratio = num_sums / num_diffs\n>>>>>>> REPLACE\n\n2) Disable the manual caching in `compute_ratio`\u2014the bitwise path is so cheap that avoiding the tobytes/hash/domain keeps us from thrashing the dict:\n<<<<<<< SEARCH\n    # Multi-call caching\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in ratio_cache:\n        return ratio_cache[key]\n=======\n    # caching disabled: bitwise ops are extremely fast and avoid hash overhead\n>>>>>>> REPLACE\n\nAnd remove the storage at the end of the function:\n<<<<<<< SEARCH\n    ratio_cache[key] = ratio\n    return ratio\n=======\n    return ratio\n>>>>>>> REPLACE"]}}}