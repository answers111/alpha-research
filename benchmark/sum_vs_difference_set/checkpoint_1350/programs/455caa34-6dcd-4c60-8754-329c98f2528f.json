{"id": "455caa34-6dcd-4c60-8754-329c98f2528f", "code": "import numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # convert boolean arrays to int8 once for faster convolution\n    A_int = A_ind.view(np.int8)\n    B_int = B_ind.view(np.int8)\n    # sumset via convolution\n    sums_conv = np.convolve(A_int, B_int)\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via convolution with reversed B\n    diffs_conv = np.convolve(A_int, B_int[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = np.nonzero(ind)[0]\n    zeros = np.nonzero(~ind)[0]\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n        # propose a balanced add/remove move for A or B using helper\n        if rng.random() < 0.5:\n            C_ind = propose_move(C_ind)\n        else:\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: Adaptive Metaheuristic Framework for Maximizing Sum\u2010to\u2010Difference Ratios in Finite Indicator Sets\n\nAbstract:\nWe propose an advanced search methodology to overcome the stagnation and low convergence rate observed in the current hill\u2010climbing approach (score=1.2105) for optimizing the sum\u2010to\u2010difference ratio |A+B|\u2215|A\u2013B| over two binary indicator vectors of length N. Our framework integrates three key innovations: (1) an adaptive simulated\u2010annealing schedule that balances exploration and exploitation through a time\u2010varying temperature, preventing premature convergence; (2) a learned move\u2010proposal network, trained online using multi\u2010armed bandit principles, to prioritize bit\u2010flip candidates that historically yield the greatest ratio improvements; and (3) FFT\u2010accelerated convolution routines that reduce per\u2010evaluation complexity from O(N\u00b2) to O(N log N), enabling scalable searches for N\u226b30. \n\nWe further introduce a restarts\u2010with\u2010elite\u2010seeding mechanism: high\u2010scoring solutions are clustered and used to initialize new runs, thus preserving diversity and concentrating efforts on promising regions of the search space. To validate our approach, we will (i) benchmark against standard hill\u2010climbing, tabu search, and genetic\u2010algorithm baselines on N=30\u2026100; (ii) perform ablation studies isolating the effects of annealing, learned proposals, and FFT acceleration; and (iii) analyze the trade\u2010off between runtime and achieved ratio. We anticipate that our metaheuristic will consistently surpass the current best\u2010known ratio and demonstrate robust scalability, providing a practical and generalizable tool for ratio\u2010maximization problems in additive combinatorics."], "idea_reward": 5.5, "parent_id": "e91bc0e2-58b1-48d3-91aa-9b3333b4455b_migrant_4_migrant_3", "generation": 8, "timestamp": 1757787555.2594588, "iteration_found": 612, "metrics": {"score": 1.2727272727272727}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 2 lines with 5 lines\nChange 2: Replace max_iter = 20000 with 5 lines\nChange 3: Replace 6 lines with 14 lines\nChange 4: Replace 6 lines with 9 lines", "parent_metrics": {"score": 1.2105263157894737}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \n\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # sumset via convolution\n    sums_conv = np.convolve(A_ind.astype(int), B_ind.astype(int))\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via convolution with reversed B\n    diffs_conv = np.convolve(A_ind.astype(int), B_ind[::-1].astype(int))\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = np.nonzero(ind)[0]\n    zeros = np.nonzero(~ind)[0]\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    for _ in range(max_iter):\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n        # propose a balanced add/remove move for A or B using helper\n        if rng.random() < 0.5:\n            C_ind = propose_move(C_ind)\n        else:\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        if ratio > best_ratio:\n            best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.2105\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.4828\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5000\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5000)\n\n\n### Program 2 (Score: 1.2778)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.2778)\n\n\n### Program 3 (Score: -1.0000)\n```python\n# Replace with numpy Generator for reproducible randomness\nimport numpy as np\nrng = np.random.default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    # sum\u2010set via full convolution\n    S = np.convolve(A_ind, B_ind, mode='full')\n    sizeS = np.count_nonzero(S)\n    # difference\u2010set via cross\u2010correlation\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.3448)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n# Initialize random number generator for reproducibility\nrng = default_rng(42)\n\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n# Current Proposal\nTitle: Adaptive Metaheuristic Framework for Maximizing Sum\u2010to\u2010Difference Ratios in Finite Indicator Sets\n\nAbstract:\nWe propose an advanced search methodology to overcome the stagnation and low convergence rate observed in the current hill\u2010climbing approach (score=1.2105) for optimizing the sum\u2010to\u2010difference ratio |A+B|\u2215|A\u2013B| over two binary indicator vectors of length N. Our framework integrates three key innovations: (1) an adaptive simulated\u2010annealing schedule that balances exploration and exploitation through a time\u2010varying temperature, preventing premature convergence; (2) a learned move\u2010proposal network, trained online using multi\u2010armed bandit principles, to prioritize bit\u2010flip candidates that historically yield the greatest ratio improvements; and (3) FFT\u2010accelerated convolution routines that reduce per\u2010evaluation complexity from O(N\u00b2) to O(N log N), enabling scalable searches for N\u226b30. \n\nWe further introduce a restarts\u2010with\u2010elite\u2010seeding mechanism: high\u2010scoring solutions are clustered and used to initialize new runs, thus preserving diversity and concentrating efforts on promising regions of the search space. To validate our approach, we will (i) benchmark against standard hill\u2010climbing, tabu search, and genetic\u2010algorithm baselines on N=30\u2026100; (ii) perform ablation studies isolating the effects of annealing, learned proposals, and FFT acceleration; and (iii) analyze the trade\u2010off between runtime and achieved ratio. We anticipate that our metaheuristic will consistently surpass the current best\u2010known ratio and demonstrate robust scalability, providing a practical and generalizable tool for ratio\u2010maximization problems in additive combinatorics.\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are four targeted diffs that introduce a simple simulated\u2010annealing acceptance to escape local maxima and reduce datatype casting overhead in the convolution. Each diff uses the exact SEARCH/REPLACE format and is confined to the matching lines.\n\n1) Initialize a \u201ccurrent\u201d solution and ratio alongside the best one:\n\n<<<<<<< SEARCH\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n=======\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n>>>>>>> REPLACE\n\n2) Insert annealing parameters right before the main loop:\n\n<<<<<<< SEARCH\n    max_iter = 20000\n=======\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n>>>>>>> REPLACE\n\n3) Replace the hard accept\u2010only improvement logic with an SA acceptance rule:\n\n<<<<<<< SEARCH\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        if ratio > best_ratio:\n            best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n=======\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n>>>>>>> REPLACE\n\n4) Reduce repeated dtype casting in compute_ratio by viewing booleans as int8:\n\n<<<<<<< SEARCH\n    # sumset via convolution\n    sums_conv = np.convolve(A_ind.astype(int), B_ind.astype(int))\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via convolution with reversed B\n    diffs_conv = np.convolve(A_ind.astype(int), B_ind[::-1].astype(int))\n    num_diffs = np.count_nonzero(diffs_conv)\n=======\n    # convert boolean arrays to int8 once for faster convolution\n    A_int = A_ind.view(np.int8)\n    B_int = B_ind.view(np.int8)\n    # sumset via convolution\n    sums_conv = np.convolve(A_int, B_int)\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via convolution with reversed B\n    diffs_conv = np.convolve(A_int, B_int[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n>>>>>>> REPLACE\n\nExplanation of expected benefits:\n\n\u2022 Simulated Annealing (diffs 1\u20133):  \n  \u2013 Allows occasional downhill moves (controlled by temperature), escaping local optima.  \n  \u2013 Temperature T decays slowly so early iterations explore broadly.  \n\n\u2022 Datatype view (diff 4):  \n  \u2013 Avoids repeated full `astype(int)` allocations on each convolution call; instead reinterprets the boolean array as int8.  \n  \u2013 Should marginally reduce per\u2010evaluation overhead, especially when called many times."]}}}