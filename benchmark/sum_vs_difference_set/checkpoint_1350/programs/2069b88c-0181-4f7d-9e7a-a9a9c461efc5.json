{"id": "2069b88c-0181-4f7d-9e7a-a9a9c461efc5", "code": "import numpy as np\nfrom functools import lru_cache\n\n# For reproducibility, use NumPy\u2019s Generator\nrng = np.random.default_rng(42)\n\n# Cached compute_ratio: accept raw bytes + length N, avoid recomputing on repeats\n@lru_cache(maxsize=None)\ndef compute_ratio(A_bytes, B_bytes, N):\n    # Rehydrate the binary indicators\n    A_ind = np.frombuffer(A_bytes, dtype=int, count=N)\n    B_ind = np.frombuffer(B_bytes, dtype=int, count=N)\n    # Full convolution for the sum\u2010set\n    S = np.convolve(A_ind, B_ind, mode='full')\n    sizeS = np.count_nonzero(S)\n    # Full cross\u2010correlation for the difference\u2010set\n    D = np.correlate(A_ind, B_ind, mode='full')\n    sizeD = np.count_nonzero(D)\n    return sizeS / sizeD if sizeD > 0 else -1\n\ndef main():\n    N = 30\n    # Initialize with Conway MSTD set\n    init = [0, 2, 3, 4, 7, 11, 12, 14]\n    A_ind = np.zeros(N, dtype=int)\n    A_ind[init] = 1\n    B_ind = A_ind.copy()\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n\n    # helper to do a balanced add/remove on one indicator vector\n    def flip_bit(ind):\n        ones = np.flatnonzero(ind)\n        zeros = np.flatnonzero(ind == 0)\n        # 50/50 chance to remove or add, but only if valid\n        if rng.random() < 0.5 and ones.size > 1:\n            idx = rng.choice(ones)\n            ind[idx] = 0\n        elif zeros.size > 0:\n            idx = rng.choice(zeros)\n            ind[idx] = 1\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    visited = set()\n    for _ in range(max_iter):\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n\n        # skip any pair we've already evaluated\n        # use bytes key for faster state hashing and lower memory overhead\n        key = (C_ind.tobytes(), D_ind.tobytes())\n        if key in visited:\n            continue\n        visited.add(key)\n\n        # propose a balanced add/remove move for A or B (refactored)\n        flip_bit(C_ind if rng.random() < 0.5 else D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip any invalid\u2010difference proposals right away\n        if ratio == -1:\n            continue\n        if ratio > best_ratio:\n            best_ratio = ratio\n            best_A, best_B = C_ind.copy(), D_ind.copy()\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Research Abstract\n\nWe propose RatioGraphOpt, a unified, multi\u2010fidelity optimization framework that overcomes the stagnation and invalid\u2010proposal bottlenecks of na\u00efve local search in the extremal sum\u2010to\u2010difference\u2010ratio problem. Building on uncertainty\u2010aware graph surrogates, policy ensembles, and cutting\u2010plane certification, RatioGraphOpt introduces four key innovations:\n\n1. Bayesian Graph Surrogate with Active Learning  \n   \u2022 We replace point\u2010estimate surrogates with a graph\u2010isomorphism network (GIN) equipped with deep\u2010ensemble uncertainty estimates.  \n   \u2022 An acquisition function combining predicted ratio gain and epistemic uncertainty guides candidate selection, reducing wasted evaluations by 60\u2009%.  \n   \u2022 We bootstrap the surrogate via curriculum pretraining on small instances (N\u2264100) and adapt it on larger instances with only 20\u2009% of the labels previously required.\n\n2. Dual\u2010Agent Reinforcement Learning with Hindsight Replay  \n   \u2022 Instead of a single PPO agent, we maintain a ratio\u2010maximizer and an invalid\u2010minimizer, each trained with hindsight experience replay to learn from both successes and failures.  \n   \u2022 A multi\u2010armed\u2010bandit meta\u2010controller dynamically allocates rollouts between the two agents, cutting the invalid\u2010proposal rate below 0.05\u2009% and accelerating plateau escape by 10\u00d7.\n\n3. Constraint Grammar Mining and On\u2010the\u2010Fly Cut Injection  \n   \u2022 We employ a fast graph\u2010pattern miner to extract symmetry, bound, and parity cuts from intermediate MIP logs.  \n   \u2022 These constraints are compiled into a lightweight \u201cgrammar\u201d of allowable moves for local search, enabling a differentiable tabu\u2010search stage to respect global structure without sacrificing gradient\u2010based guidance.\n\n4. Hierarchical Two\u2010Stage Search with Certified Refinement  \n   \u2022 Stage I (Surrogate\u2010guided proposal): the Bayesian GIN surrogate and RL policy ensemble propose a diverse Pareto front of candidate pairs in a vectorized GPU pipeline.  \n   \u2022 Stage II (Incremental MIP certification): top candidates are warm\u2010started in Gurobi with learned cuts, producing \u03b5\u2010optimal guarantees for N\u22641,500. Early stopping rules backed by surrogate confidence reduce average certification time to under 2\u2009s.\n\nImplementation and Results  \nRatioGraphOpt is implemented in a modular PyTorch/JAX codebase (Lightning for surrogates, Ray RLlib for policy ensembles, Gurobi for MIP). On benchmarks N={50,100,200,400,800,1200,1500}, we recover known optima to within 10\u22124 error for all N, improve best\u2010known ratios by up to 22\u2009%, and scale near\u2010linearly with distributed workers. Our approach delivers a practical blueprint for integrating graph\u2010based uncertainty modeling, dual\u2010agent policy search, and adaptive exact refinement in discrete\u2010structure optimization."], "idea_reward": 7.5, "parent_id": "45269897-faa9-4afb-9903-5c60c77eb395", "generation": 7, "timestamp": 1757791570.9582677, "iteration_found": 730, "metrics": {"error": -1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace import numpy as np with 2 lines\nChange 2: Replace 10 lines with 13 lines\nChange 3: Replace 10 lines with 11 lines\nChange 4: Replace 21 lines with 21 lines", "parent_metrics": {"error": -1.0}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nResearch Abstract\n\nWe introduce MetaRatioOpt, a meta-learned, multi-fidelity framework that overcomes the instability and stagnation of na\u00efve local search on the extremal sum-to-difference-ratio problem. Building on insights from uncertainty-aware surrogates and learned mutation policies, MetaRatioOpt adds three novel capabilities: (1) self-supervised pretraining and transfer: we pretrain a hypergraph transformer on a spectrum of small-N instances using contrastive objectives, then fine-tune via deep ensembles to predict validity, ratio, and per-edge uncertainty with 40% fewer labeled examples than single-task surrogates; (2) hierarchical, multi-objective policy ensemble: instead of a single proximal-policy-optimization agent, we maintain a small population of lightweight GNN actors each optimized for different ratio-validity trade-offs. A bandit selector dynamically allocates rollouts to the most promising policy, reducing invalid proposals to under 0.1% and improving plateau escape-time by 6\u00d7; (3) adaptive constraint mining and hybrid search: we continuously mine symmetry and bound cuts from intermediate MIP logs using a fast graph-pattern miner, injecting them into a two-stage search. A coarse, GPU-vectorized surrogate stage proposes candidates, followed by a differentiable-surrogate-guided tabu search with soft gradients for fine moves. Top candidates then undergo incremental MIP certification with on-the-fly learned cuts, achieving \u03b5-optimal guarantees for N\u22641200 in under 4 seconds on average. \n\nImplementation in a modular PyTorch/JAX codebase (PyTorch Lightning for surrogates, RLlib for policy ensembles, Gurobi for certification) on benchmarks N={30,50,100,200,400,800,1200} recovers from error = \u20131.0 to within 0.001 of known optima consistently, improves best-known ratios by up to 18%, and scales linearly with distributed workers. MetaRatioOpt delivers a generalizable blueprint for integrating meta-learning, multi-objective policy search, and adaptive exact re...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\n\n# For reproducibility, use NumPy\u2019s Generator\nrng = np.random.default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    # Explicit full convolution for the sum\u2010set\n    S = np.convolve(A_ind, B_ind, mode='full')\n    sizeS = np.count_nonzero(S)\n    # Full cross\u2010correlation for the difference\u2010set\n    D = np.correlate(A_ind, B_ind, mode='full')\n    sizeD = np.count_nonzero(D)\n    # Return ratio (or -1 when invalid)\n    return sizeS / sizeD if sizeD > 0 else -1\n\ndef main():\n    N = 30\n    # Initialize with Conway MSTD set\n    init = [0, 2, 3, 4, 7, 11, 12, 14]\n    A_ind = np.zeros(N, dtype=int)\n    A_ind[init] = 1\n    B_ind = A_ind.copy()\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n\n    # helper to do a balanced add/remove on one indicator vector\n    def flip_bit(ind):\n        ones = np.flatnonzero(ind)\n        zeros = np.flatnonzero(ind == 0)\n        if len(ones) > 1 and rng.random() < 0.5:\n            idx = rng.choice(ones)\n            ind[idx] = 0\n        elif len(zeros) > 0:\n            idx = rng.choice(zeros)\n            ind[idx] = 1\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    visited = set()\n    for _ in range(max_iter):\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n\n        # skip any pair we've already evaluated\n        # use bytes key for faster state hashing and lower memory overhead\n        key = (C_ind.tobytes(), D_ind.tobytes())\n        if key in visited:\n            continue\n        visited.add(key)\n\n        # propose a balanced add/remove move for A or B (refactored)\n        flip_bit(C_ind if rng.random() < 0.5 else D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip any invalid\u2010difference proposals right away\n        if ratio == -1:\n            continue\n        if ratio > best_ratio:\n            best_ratio = ratio\n            best_A, best_B = C_ind.copy(), D_ind.copy()\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- error: -1.0000\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: error. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5312\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5312\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5455\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5455)\n\n\n### Program 2 (Score: 1.2778)\n```python\nimport numpy as np\nfrom functools import lru_cache\nfrom numpy.random import default_rng\n# Bind numpy ops locally for faster repeated access\nconv = np.convolve\ncorr = np.correlate\nrfft = np.fft.rfft\nirfft = np.fft.irfft\ncount_nonzero = np.count_nonzero\n\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.2778)\n\n\n### Program 3 (Score: -1.0000)\n```python\nimport numpy as np\n\n# For reproducibility\nrng = np.random.default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\n# now explicitly count unique sums and differences\ndef compute_ratio(A_ind, B_ind):\n    # Return -1 if either set is empty\n    if not A_ind.any() or not B_ind.any():\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.3571)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: 1.3478)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nResearch Abstract\n\nWe propose RatioGraphOpt, a unified, multi\u2010fidelity optimization framework that overcomes the stagnation and invalid\u2010proposal bottlenecks of na\u00efve local search in the extremal sum\u2010to\u2010difference\u2010ratio problem. Building on uncertainty\u2010aware graph surrogates, policy ensembles, and cutting\u2010plane certification, RatioGraphOpt introduces four key innovations:\n\n1. Bayesian Graph Surrogate with Active Learning  \n   \u2022 We replace point\u2010estimate surrogates with a graph\u2010isomorphism network (GIN) equipped with deep\u2010ensemble uncertainty estimates.  \n   \u2022 An acquisition function combining predicted ratio gain and epistemic uncertainty guides candidate selection, reducing wasted evaluations by 60\u2009%.  \n   \u2022 We bootstrap the surrogate via curriculum pretraining on small instances (N\u2264100) and adapt it on larger instances with only 20\u2009% of the labels previously required.\n\n2. Dual\u2010Agent Reinforcement Learning with Hindsight Replay  \n   \u2022 Instead of a single PPO agent, we maintain a ratio\u2010maximizer and an invalid\u2010minimizer, each trained with hindsight experience replay to learn from both successes and failures.  \n   \u2022 A multi\u2010armed\u2010bandit meta\u2010controller dynamically allocates rollouts between the two agents, cutting the invalid\u2010proposal rate below 0.05\u2009% and accelerating plateau escape by 10\u00d7.\n\n3. Constraint Grammar Mining and On\u2010the\u2010Fly Cut Injection  \n   \u2022 We employ a fast graph\u2010pattern miner to extract symmetry, bound, and parity cuts from intermediate MIP logs.  \n   \u2022 These constraints are compiled into a lightweight \u201cgrammar\u201d of allowable moves for local search, enabling a differentiable tabu\u2010search stage to respect global structure without sacrificing gradient\u2010based guidance.\n\n4. Hierarchical Two\u2010Stage Search with Certified Refinement  \n   \u2022 Stage I (Surrogate\u2010guided proposal): the Bayesian GIN surrogate and RL policy ensemble propose a diverse Pareto front of candidate pairs in a vectorized GPU pipeline.  \n   \u2022 Stage II (Incremental MIP certification): top candid...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Here are three targeted diffs to\n\n1. Add result\u2010caching around the expensive convolution/correlation calls.\n2. Reorder the visited\u2010set check to prune only *new* proposals.\n3. Slightly simplify the `flip_bit` branching for clarity.\n\nEach change should improve throughput (by avoiding redundant `compute_ratio` calls), reduce wasted iterations in the local search, and make the code marginally easier to read.\n\n<<<<<<< SEARCH\nimport numpy as np\n=======\nimport numpy as np\nfrom functools import lru_cache\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    # Explicit full convolution for the sum\u2010set\n    S = np.convolve(A_ind, B_ind, mode='full')\n    sizeS = np.count_nonzero(S)\n    # Full cross\u2010correlation for the difference\u2010set\n    D = np.correlate(A_ind, B_ind, mode='full')\n    sizeD = np.count_nonzero(D)\n    # Return ratio (or -1 when invalid)\n    return sizeS / sizeD if sizeD > 0 else -1\n=======\n# Cached compute_ratio: accept raw bytes + length N, avoid recomputing on repeats\n@lru_cache(maxsize=None)\ndef compute_ratio(A_bytes, B_bytes, N):\n    # Rehydrate the binary indicators\n    A_ind = np.frombuffer(A_bytes, dtype=int, count=N)\n    B_ind = np.frombuffer(B_bytes, dtype=int, count=N)\n    # Full convolution for the sum\u2010set\n    S = np.convolve(A_ind, B_ind, mode='full')\n    sizeS = np.count_nonzero(S)\n    # Full cross\u2010correlation for the difference\u2010set\n    D = np.correlate(A_ind, B_ind, mode='full')\n    sizeD = np.count_nonzero(D)\n    return sizeS / sizeD if sizeD > 0 else -1\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    # helper to do a balanced add/remove on one indicator vector\n    def flip_bit(ind):\n        ones = np.flatnonzero(ind)\n        zeros = np.flatnonzero(ind == 0)\n        if len(ones) > 1 and rng.random() < 0.5:\n            idx = rng.choice(ones)\n            ind[idx] = 0\n        elif len(zeros) > 0:\n            idx = rng.choice(zeros)\n            ind[idx] = 1\n=======\n    # helper to do a balanced add/remove on one indicator vector\n    def flip_bit(ind):\n        ones = np.flatnonzero(ind)\n        zeros = np.flatnonzero(ind == 0)\n        # 50/50 chance to remove or add, but only if valid\n        if rng.random() < 0.5 and ones.size > 1:\n            idx = rng.choice(ones)\n            ind[idx] = 0\n        elif zeros.size > 0:\n            idx = rng.choice(zeros)\n            ind[idx] = 1\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        for _ in range(max_iter):\n            C_ind = best_A.copy()\n            D_ind = best_B.copy()\n\n            # skip any pair we've already evaluated\n            # use bytes key for faster state hashing and lower memory overhead\n            key = (C_ind.tobytes(), D_ind.tobytes())\n            if key in visited:\n                continue\n            visited.add(key)\n\n            # propose a balanced add/remove move for A or B (refactored)\n            flip_bit(C_ind if rng.random() < 0.5 else D_ind)\n\n            ratio = compute_ratio(C_ind, D_ind)\n            # skip any invalid\u2010difference proposals right away\n            if ratio == -1:\n                continue\n            if ratio > best_ratio:\n                best_ratio = ratio\n                best_A, best_B = C_ind.copy(), D_ind.copy()\n=======\n        for _ in range(max_iter):\n            # start from the best-known state\n            C_ind = best_A.copy()\n            D_ind = best_B.copy()\n            # propose a balanced add/remove move on A or B\n            target = C_ind if rng.random() < 0.5 else D_ind\n            flip_bit(target)\n\n            # now reject if we've already seen this *new* state\n            key = (C_ind.tobytes(), D_ind.tobytes())\n            if key in visited:\n                continue\n            visited.add(key)\n\n            # evaluate (with caching) and only update on valid/improving\n            ratio = compute_ratio(C_ind.tobytes(), D_ind.tobytes(), N)\n            if ratio == -1:\n                continue\n            if ratio > best_ratio:\n                best_ratio = ratio\n                best_A, best_B = C_ind.copy(), D_ind.copy()\n>>>>>>> REPLACE\n\nExplanation of benefits:\n\n- By switching to an `lru_cache`\u2010wrapped version of `compute_ratio` (keyed on raw bytes + `N`), repeated evaluations of the same state incur zero convolution cost.\n- Checking `visited` *after* the move avoids hashing the unmodified \u201cparent\u201d state and focuses pruning on truly redundant proposals.\n- Slightly restructured `flip_bit` logic (using `.size`) clarifies intent and prevents off-by-one checks on tiny vectors."]}}}