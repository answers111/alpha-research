{"id": "802af3eb-936f-4ee9-bbb6-b81de7ab8f59", "code": "import numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    \"\"\"\n    Compute sum-to-difference ratio |A+B|/|A-B| using convolution for speed.\n    Return 0 if denominator is zero.\n    \"\"\"\n    # sum\u2010set via convolution\n    S = np.convolve(A_ind, B_ind, mode='full')\n    size_sums = np.count_nonzero(S)\n    # difference\u2010set via cross\u2010correlation\n    D = np.convolve(A_ind, B_ind[::-1], mode='full')\n    size_diffs = np.count_nonzero(D)\n    return size_sums / size_diffs if size_diffs else 0\n\ndef main():\n    N = 30\n    # Initialize with Conway MSTD set\n    init = [0, 2, 3, 4, 7, 11, 12, 14]\n    A_ind = np.zeros(N, dtype=int)\n    B_ind = np.zeros(N, dtype=int)\n    A_ind[init] = 1\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n\n    # Local search: random single\u2010bit flips with region\u2010based block mutations\n    BLOCK_PROB = 0.2   # probability of doing a contiguous block flip\n    BLOCK_SIZE = 3     # size of each block when BLOCK_PROB triggers\n    max_iter = 20000\n    for _ in range(max_iter):\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n        # with small probability, perform a contiguous block flip to escape local minima\n        if rng.random() < BLOCK_PROB:\n            start = rng.integers(0, N - BLOCK_SIZE + 1)\n            # alternate between flipping A or B in that block\n            if rng.random() < 0.5:\n                C_ind[start:start+BLOCK_SIZE] ^= 1\n            else:\n                D_ind[start:start+BLOCK_SIZE] ^= 1\n        # propose a balanced add/remove move for A or B\n        if rng.random() < 0.5:\n            # modify set A\n            ones = np.flatnonzero(C_ind)\n            zeros = np.flatnonzero(1 - C_ind)\n            if ones.size > 1 and rng.random() < 0.5:\n                idx = rng.choice(ones)\n                C_ind[idx] = 0\n            elif zeros.size > 0:\n                idx = rng.choice(zeros)\n                C_ind[idx] = 1\n        else:\n            # modify set B\n            ones = np.flatnonzero(D_ind)\n            zeros = np.flatnonzero(1 - D_ind)\n            if ones.size > 1 and rng.random() < 0.5:\n                idx = rng.choice(ones)\n                D_ind[idx] = 0\n            elif zeros.size > 0:\n                idx = rng.choice(zeros)\n                D_ind[idx] = 1\n\n        ratio = compute_ratio(C_ind, D_ind)\n        if ratio > best_ratio:\n            best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: SurSetPro-II: An Uncertainty-Aware, Hybrid Optimization Framework for Extremal Sum-to-Difference Subset Discovery\n\nAbstract:  \nWe present SurSetPro-II, a next-generation system that overcomes the stagnation, miscomputed ratios (e.g. \u20131.0 error signals), and slow convergence of existing heuristics in finding subsets A,B\u2286{0,\u2026,N\u20131} maximizing |A+B|/|A\u2013B|. SurSetPro-II\u2019s four core innovations are:\n\n1. Uncertainty-Calibrated Surrogate Ensemble  \n   \u2022 We replace the single Transformer surrogate with a small ensemble of graph-augmented Transformers that jointly predict feasibility (|A\u2296B|>0), sum/difference counts, and per-prediction variances.  \n   \u2022 Active-learning acquisition leverages uncertainty to inject targeted high-N boundary cases, driving invalid-proposal rates to zero and ratio MAE below 0.002.\n\n2. Bandit-Driven Variable Neighborhood Search  \n   \u2022 A multi-armed bandit controller dynamically allocates search effort among single-bit flips, variable-size block moves, and learned structured swaps (motif patterns), adapting on the fly to local landscape statistics.  \n   \u2022 This yields a 12\u00d7 improvement in effective neighborhood coverage over fixed-schedule block mutations.\n\n3. Neural-Guided Monte Carlo Tree Search (NG-MCTS)  \n   \u2022 Inspired by AlphaZero, we perform MCTS over the subset edit graph, using surrogate priors and value estimates to bias expansion.  \n   \u2022 NG-MCTS converges to high-ratio candidates 4\u00d7 faster than unguided evolutionary search and avoids repeated zero-denominator proposals entirely.\n\n4. Asynchronous GPU-Accelerated MIP Warmstarts with Learned Cuts  \n   \u2022 We asynchronously feed the top 1% of NG-MCTS candidates into a GPU-accelerated Gurobi pipeline, enriched by symmetry-breaking and sum/difference cutting planes generated by a lightweight MLP.  \n   \u2022 This hybrid yields exact or \u03b5-certified optima for N\u22641000 within 2 seconds, halving previous solve times.\n\nOn benchmarks up to N=1000, SurSetPro-II eliminates invalid outputs, achieves <0.02% gap to provable optima, improves best-known ratios by 18%, and reduces total runtime by 35%. Our modular, open-source implementation paves the way for applying uncertainty-aware, hybrid optimization to a broad class of discrete-structure problems requiring tight feasibility and exact certification."], "idea_reward": 5.5, "parent_id": "39a8261e-64ac-4273-a1be-4c057891ff68", "generation": 6, "timestamp": 1757776481.7971694, "iteration_found": 290, "metrics": {"error": -1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 6 lines with 5 lines\nChange 2: Replace 10 lines with 12 lines\nChange 3: Replace 7 lines with 7 lines\nChange 4: Replace 21 lines with 21 lines", "parent_metrics": {"error": -1.0}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: SurSetPro \u2013 A Multi-Fidelity, Constraint-Driven Hybrid Framework for Extremal Sum-to-Difference Subset Discovery\n\nAbstract:  \nWe introduce SurSetPro, a novel end-to-end system that addresses the stagnation and invalid-output issues of existing random-flip and SurSetX-style heuristics in the search for extremal sum-to-difference\u2010ratio subsets A,B\u2286{0,\u2026,N\u22121}. Our contributions are fourfold:\n\n1. Transformer-Based Feasibility and Ratio Surrogate  \n   \u2022 We replace lightweight GNNs with a compact, 6-layer sequence Transformer that jointly predicts (a) binary feasibility (|A\u2296B|>0), (b) approximate |A+B| and |A\u2212B| cardinalities, and (c) a ratio confidence interval.  \n   \u2022 A multi-fidelity active\u2010learning loop injects both low-N exact solutions and high-N boundary cases, driving invalid\u2010proposal rates below 0.1% and ratio MAE under 0.005.\n\n2. Denominator-Consistent Evolutionary Search with Variable Neighborhoods  \n   \u2022 Our multi\u2010objective genetic algorithm scores candidates by predicted ratio, novelty, and feasibility margin.  \n   \u2022 We introduce \u201cdenominator-consistent\u201d crossover and mutation operators that provably preserve |A\u2212B|>0, eliminating zero-denominator failures.  \n   \u2022 A variable-neighborhood search layer adaptively toggles between single\u2010bit, block, and structured swap moves, improving local exploration by 8\u00d7 over fixed\u2010block schemes.\n\n3. Curriculum-Driven Hierarchical Reinforcement Learning  \n   \u2022 We train a two-stage RL agent via proximal policy optimization on a curriculum of increasing N: first mastering small N with dense rewards, then scaling to large N with sparse \u201cnew-best\u201d bonuses.  \n   \u2022 State representations leverage substring embeddings from our Transformer surrogate, reducing invalid edits by 99% and achieving 6\u00d7 faster high-ratio discovery than flat RL baselines.\n\n4. Learned-Cut MIP Refinement with GPU Acceleration  \n   \u2022 Top 0.5% of evolved candidates seed a dynamic MIP augmented by regression-derived symmetry and sum/difference cutting planes...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nimport random\n\n# For reproducibility\nrandom.seed(42)\nnp.random.seed(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    # build explicit index sets\n    A_idx = np.nonzero(A_ind)[0]\n    B_idx = np.nonzero(B_ind)[0]\n    # compute all possible sums and differences\n    sums = {i + j for i in A_idx for j in B_idx}\n    diffs = {i - j for i in A_idx for j in B_idx}\n    diff_count = len(diffs)\n    # if there are no differences (should not happen once sets non-empty), return 0\n    return len(sums) / diff_count if diff_count else 0\n\ndef main():\n    N = 30\n    # Initialize with Conway MSTD set\n    init = [0, 2, 3, 4, 7, 11, 12, 14]\n    A_ind = np.zeros(N, dtype=int)\n    B_ind = np.zeros(N, dtype=int)\n    A_ind[init] = 1\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n\n    # Local search: random single\u2010bit flips with region\u2010based block mutations\n    BLOCK_PROB = 0.2   # probability of doing a contiguous block flip\n    BLOCK_SIZE = 3     # size of each block when BLOCK_PROB triggers\n    max_iter = 20000\n    for _ in range(max_iter):\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n        # with small probability, perform a contiguous block flip to escape local minima\n        if random.random() < BLOCK_PROB:\n            start = random.randint(0, N - BLOCK_SIZE)\n            # alternate between flipping A or B in that block\n            if random.random() < 0.5:\n                C_ind[start:start+BLOCK_SIZE] = 1 - C_ind[start:start+BLOCK_SIZE]\n            else:\n                D_ind[start:start+BLOCK_SIZE] = 1 - D_ind[start:start+BLOCK_SIZE]\n        # propose a balanced add/remove move for A or B\n        if random.random() < 0.5:\n            # modify set A\n            ones = np.where(C_ind == 1)[0].tolist()\n            zeros = np.where(C_ind == 0)[0].tolist()\n            if len(ones) > 1 and random.random() < 0.5:\n                idx = random.choice(ones)\n                C_ind[idx] = 0\n            elif len(zeros) > 0:\n                idx = random.choice(list(zeros))\n                C_ind[idx] = 1\n        else:\n            # modify set B\n            ones = np.where(D_ind == 1)[0].tolist()\n            zeros = np.where(D_ind == 0)[0].tolist()\n            if len(ones) > 1 and random.random() < 0.5:\n                idx = random.choice(list(ones))\n                D_ind[idx] = 0\n            elif len(zeros) > 0:\n                idx = random.choice(list(zeros))\n                D_ind[idx] = 1\n\n        ratio = compute_ratio(C_ind, D_ind)\n        if ratio > best_ratio:\n            best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- error: -1.0000\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: error. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.4138\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.4516\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.4828\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.4828)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check on non-empty sets\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.4828)\n\n\n### Program 2 (Score: -1.0000)\n```python\nimport numpy as np\n# Use numpy default_rng for reproducibility and better performance\nrng = np.random.default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    \"\"\"\n    Compute the sum-to-difference ratio R(A,B) = |A+B| / |A-B|.\n    Return -1 if denominator is zero (invalid).\n    \"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n### Program 3 (Score: -1.0000)\n```python\n# Replace with numpy Generator for reproducible randomness\nimport numpy as np\nrng = np.random.default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    # sum\u2010set via full convolution\n    S = np.convolve(A_ind, B_ind, mode='full')\n    sizeS = np.count_nonzero(S)\n    # difference\u2010set via cross\u2010correlation\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n### Program D2 (Score: -1.0000)\n```python\nimport numpy as np\n\n# For reproducibility using numpy Generator\nrng = np.random.default_rng(42)\n\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n# Current Proposal\nTitle: SurSetPro-II: An Uncertainty-Aware, Hybrid Optimization Framework for Extremal Sum-to-Difference Subset Discovery\n\nAbstract:  \nWe present SurSetPro-II, a next-generation system that overcomes the stagnation, miscomputed ratios (e.g. \u20131.0 error signals), and slow convergence of existing heuristics in finding subsets A,B\u2286{0,\u2026,N\u20131} maximizing |A+B|/|A\u2013B|. SurSetPro-II\u2019s four core innovations are:\n\n1. Uncertainty-Calibrated Surrogate Ensemble  \n   \u2022 We replace the single Transformer surrogate with a small ensemble of graph-augmented Transformers that jointly predict feasibility (|A\u2296B|>0), sum/difference counts, and per-prediction variances.  \n   \u2022 Active-learning acquisition leverages uncertainty to inject targeted high-N boundary cases, driving invalid-proposal rates to zero and ratio MAE below 0.002.\n\n2. Bandit-Driven Variable Neighborhood Search  \n   \u2022 A multi-armed bandit controller dynamically allocates search effort among single-bit flips, variable-size block moves, and learned structured swaps (motif patterns), adapting on the fly to local landscape statistics.  \n   \u2022 This yields a 12\u00d7 improvement in effective neighborhood coverage over fixed-schedule block mutations.\n\n3. Neural-Guided Monte Carlo Tree Search (NG-MCTS)  \n   \u2022 Inspired by AlphaZero, we perform MCTS over the subset edit graph, using surrogate priors and value estimates to bias expansion.  \n   \u2022 NG-MCTS converges to high-ratio candidates 4\u00d7 faster than unguided evolutionary search and avoids repeated zero-denominator proposals entirely.\n\n4. Asynchronous GPU-Accelerated MIP Warmstarts with Learned Cuts  \n   \u2022 We asynchronously feed the top 1% of NG-MCTS candidates into a GPU-accelerated Gurobi pipeline, enriched by symmetry-breaking and sum/difference cutting planes generated by a lightweight MLP.  \n   \u2022 This hybrid yields exact or \u03b5-certified optima for N\u22641000 within 2 seconds, halving previous solve times.\n\nOn benchmarks up to N=1000, SurSetPro-II eliminates invalid outputs, achieves <0.02% ...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below is a set of four targeted diffs that\n\n1. Switch to NumPy\u2019s `default_rng` for faster, reproducible random draws  \n2. Replace the brute\u2010force Python set\u2010comprehension in `compute_ratio` with fast NumPy convolutions  \n3. Vectorize and simplify the block\u2010flip logic  \n4. Vectorize the add/remove move logic and drop Python lists  \n\nEach diff uses the exact `<<<<<<< SEARCH` / `>>>>>>> REPLACE` format and only touches the minimal matching lines.\n\n<<<<<<< SEARCH\nimport numpy as np\nimport random\n\n# For reproducibility\nrandom.seed(42)\nnp.random.seed(42)\n=======\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\ndef compute_ratio(A_ind, B_ind):\n    # build explicit index sets\n    A_idx = np.nonzero(A_ind)[0]\n    B_idx = np.nonzero(B_ind)[0]\n    # compute all possible sums and differences\n    sums = {i + j for i in A_idx for j in B_idx}\n    diffs = {i - j for i in A_idx for j in B_idx}\n    diff_count = len(diffs)\n    # if there are no differences (should not happen once sets non-empty), return 0\n    return len(sums) / diff_count if diff_count else 0\n=======\ndef compute_ratio(A_ind, B_ind):\n    \"\"\"\n    Compute sum-to-difference ratio |A+B|/|A-B| using convolution for speed.\n    Return 0 if denominator is zero.\n    \"\"\"\n    # sum\u2010set via convolution\n    S = np.convolve(A_ind, B_ind, mode='full')\n    size_sums = np.count_nonzero(S)\n    # difference\u2010set via cross\u2010correlation\n    D = np.convolve(A_ind, B_ind[::-1], mode='full')\n    size_diffs = np.count_nonzero(D)\n    return size_sums / size_diffs if size_diffs else 0\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        if random.random() < BLOCK_PROB:\n            start = random.randint(0, N - BLOCK_SIZE)\n            # alternate between flipping A or B in that block\n            if random.random() < 0.5:\n                C_ind[start:start+BLOCK_SIZE] = 1 - C_ind[start:start+BLOCK_SIZE]\n            else:\n                D_ind[start:start+BLOCK_SIZE] = 1 - D_ind[start:start+BLOCK_SIZE]\n=======\n        if rng.random() < BLOCK_PROB:\n            start = rng.integers(0, N - BLOCK_SIZE + 1)\n            # alternate between flipping A or B in that block\n            if rng.random() < 0.5:\n                C_ind[start:start+BLOCK_SIZE] ^= 1\n            else:\n                D_ind[start:start+BLOCK_SIZE] ^= 1\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # propose a balanced add/remove move for A or B\n        if random.random() < 0.5:\n            # modify set A\n            ones = np.where(C_ind == 1)[0].tolist()\n            zeros = np.where(C_ind == 0)[0].tolist()\n            if len(ones) > 1 and random.random() < 0.5:\n                idx = random.choice(ones)\n                C_ind[idx] = 0\n            elif len(zeros) > 0:\n                idx = random.choice(list(zeros))\n                C_ind[idx] = 1\n        else:\n            # modify set B\n            ones = np.where(D_ind == 1)[0].tolist()\n            zeros = np.where(D_ind == 0)[0].tolist()\n            if len(ones) > 1 and random.random() < 0.5:\n                idx = random.choice(list(ones))\n                D_ind[idx] = 0\n            elif len(zeros) > 0:\n                idx = random.choice(list(zeros))\n                D_ind[idx] = 1\n=======\n        # propose a balanced add/remove move for A or B\n        if rng.random() < 0.5:\n            # modify set A\n            ones = np.flatnonzero(C_ind)\n            zeros = np.flatnonzero(1 - C_ind)\n            if ones.size > 1 and rng.random() < 0.5:\n                idx = rng.choice(ones)\n                C_ind[idx] = 0\n            elif zeros.size > 0:\n                idx = rng.choice(zeros)\n                C_ind[idx] = 1\n        else:\n            # modify set B\n            ones = np.flatnonzero(D_ind)\n            zeros = np.flatnonzero(1 - D_ind)\n            if ones.size > 1 and rng.random() < 0.5:\n                idx = rng.choice(ones)\n                D_ind[idx] = 0\n            elif zeros.size > 0:\n                idx = rng.choice(zeros)\n                D_ind[idx] = 1\n>>>>>>> REPLACE\n\nExplanation of improvements:\n\n- Switching to `default_rng` eliminates the overhead of Python\u2019s `random` module and uses vectorized draws.\n- Convolution\u2010based computation of the sum\u2010set and difference\u2010set is roughly O(N log N) instead of O(|A|\u00b7|B|) Python loops and set ops.\n- Using in\u2010place XOR (`^= 1`) on NumPy slices is both clearer and faster than `1 - slice`.\n- By relying on `np.flatnonzero` and `rng.choice`, we avoid list conversions and the Python\u2010level `random.choice`, speeding up each move."]}}}