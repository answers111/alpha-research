{"id": "7aed7078-38e0-4cd5-9b0b-3ae85dd2707e", "code": "import numpy as np\nfrom numpy.random import default_rng\n# Prebind numpy functions for performance\n_unique = np.unique\n_count_nonzero = np.count_nonzero\n_rint = np.rint\n\n# For reproducibility\nrng = default_rng(42)\n# simple cache for compute_ratio to avoid redundant computations\nratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # simple cache lookup to skip redundant computations\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in ratio_cache:\n        return ratio_cache[key]\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        # cache infeasible cases to avoid rechecking the same zero\u2010vector pair\n        ratio_cache[key] = -1.0\n        return -1.0\n    # sparse direct\u2010set approach for small supports, else FFT convolution\n    ones_A = np.flatnonzero(A_ind)\n    ones_B = np.flatnonzero(B_ind)\n    if ones_A.size * ones_B.size < 1000:\n        # Vectorized Minkowski sum/difference using numpy\n        sum_vals = ones_A[:, None] + ones_B[None, :]\n        dif_vals = ones_A[:, None] - ones_B[None, :]\n        num_sums = _unique(sum_vals).size\n        num_diffs = _unique(dif_vals).size\n        if num_diffs == 0:\n            ratio_cache[key] = -1.0\n            return -1.0\n        ratio = num_sums / num_diffs\n        ratio_cache[key] = ratio\n        return ratio\n    # FFT\u2010based convolution for larger supports (use real FFT for binary signals)\n    n = A_ind.size + B_ind.size - 1\n    fa = np.fft.rfft(A_ind.astype(float), n)\n    fb = np.fft.rfft(B_ind.astype(float), n)\n    sums_conv = _rint(np.fft.irfft(fa * fb, n)).astype(int)\n    num_sums = _count_nonzero(sums_conv)\n    # cross-correlation via conjugate multiplication\n    diffs_conv = _rint(np.fft.irfft(fa * np.conj(fb), n)).astype(int)\n    num_diffs = _count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        ratio_cache[key] = -1.0\n        return -1.0\n    ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Return a new vector with one bit added or removed, ensuring at least one '1'.\"\"\"\n    new_ind = ind.copy()\n    ones = np.flatnonzero(new_ind)\n    zeros = np.flatnonzero(~new_ind)\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        new_ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        new_ind[rng.choice(ones)] = False\n    return new_ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        # propose move on only one vector to reduce copying overhead\n        if rng.random() < 0.5:\n            # propose_move already copies internally, so pass current_A directly\n            C_ind = propose_move(current_A)\n            D_ind = current_B  # unchanged\n        else:\n            C_ind = current_A  # unchanged\n            # likewise avoid duplicate copy for B\n            D_ind = propose_move(current_B)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: Multi-Agent Surrogate-Guided Hierarchical Planning with Continuous Relaxation and Low-Fidelity Ensembles for Efficient Sum-to-Difference Ratio Maximization\n\nAbstract:  \nWe observe that current hill-climbing and single-agent metaheuristics for maximizing the sum-to-difference ratio on large indicator vectors suffer from (i) short-sighted, memoryless bit-flip moves that become trapped in deep local optima and (ii) excessive full-scale FFT evaluations, limiting scalability. Building on graph-based surrogates, multi-fidelity bandits, and hierarchical search, we propose an integrated framework with four key innovations:\n\n1. Continuous-Relaxation Move Generator with Gumbel-Softmax  \n   \u2022 We embed binary indicator vectors into a continuous latent space via a lightweight encoder and use Gumbel-Softmax parameterizations to propose multi\u2013bit flips in a single step.  \n   \u2022 A policy network (actor) trained with a diversity-seeking objective (GFlowNet loss) learns a probabilistic distribution over high-impact move patterns, significantly improving exploration of distant optima.\n\n2. Ensemble Graph-Neural-Network Surrogates with Uncertainty Quantification  \n   \u2022 Instead of a single transformer surrogate, we maintain an ensemble of shallow bipartite graph neural networks (GNNs) that predict ratio changes and provide Bayesian-style uncertainty estimates via Monte-Carlo dropout.  \n   \u2022 Uncertain or high-potential proposals trigger on-the-fly calibration using fast incremental convolution, reducing unnecessary full FFT calls by up to 95%.\n\n3. Hierarchical Multi-Agent Planning with Contextual Bandit Meta-Controller  \n   \u2022 We orchestrate two cooperating agents: a \u201cBlock-Merge\u201d planner for coarse, multi-bit jump proposals, and a \u201cLocal-Refine\u201d planner for fine bit flips, each guided by surrogate gradients and value estimates.  \n   \u2022 A contextual bandit controller dynamically allocates CPU/GPU resources and selects between agents, fidelity levels, and sampling strategies based on live performance gains and regret minimization.\n\n4. Adaptive Multi-Fidelity Evaluation Pipeline  \n   \u2022 We design a three-tier evaluation cascade: (i) bit-parallel incremental convolution (AVX512) for micro-moves, (ii) cached low-resolution FFT subsampling for moderate changes, and (iii) full JAX/CUDA FFT with tensor tiling only for final verification.  \n   \u2022 A budget-aware scheduler enforces overall runtime and resource constraints, ensuring at least a 90% reduction in expensive full\u2010FFT calls.\n\nImplementation and Benchmarks:  \nWe will implement the policy and surrogate ensembles in PyTorch, the continuous relaxations in JAX, and high-performance kernels in C++/AVX512. Our contextual bandit and scheduler will be developed in Python. We will benchmark on N up to 10,000 against hill-climbing, tabu search, genetic algorithms, and prior surrogate-assisted methods, reporting final ratio, runtime, and FFT usage. Our goal is to surpass the current best ratio of 1.3846\u2014targeting \u22651.52\u2014and to reduce full FFT calls by \u226595%, while maintaining extensibility to related additive combinatorics and large-scale sequence alignment tasks."], "idea_reward": 5.75, "parent_id": "59f8d8cc-af1a-4afd-af38-eb55c27e9f09", "generation": 14, "timestamp": 1757789388.2046962, "iteration_found": 667, "metrics": {"score": 1.3846153846153846}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 2 lines with 6 lines\nChange 2: Replace 7 lines with 12 lines\nChange 3: Replace 15 lines with 15 lines", "parent_metrics": {"score": 1.3846153846153846}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: Hierarchically Guided Monte-Carlo Planning with Differentiable Graph Surrogates and Adaptive Fidelity for High-Performance Sum-to-Difference Ratio Optimization\n\nAbstract:\nWe address two critical limitations in existing metaheuristics for sum-to-difference ratio maximization: (1) myopic, memoryless move proposals that struggle to escape deep local optima and (2) prohibitive cost of repeated full FFT evaluations at scale. Building on successful graph-attention surrogates and actor-critic move generators, we propose a unified framework with three key innovations:\n\n1. Differentiable Graph Transformer Surrogate  \n   \u2022 A bipartite graph transformer learns both ratio predictions and smooth gradient approximations over bit-flip neighborhoods, enabling fast, informed estimates of multi-bit move impact.  \n   \u2022 We employ a two-tier active-learning loop in which uncertain or high-gain proposals trigger a lightweight incremental convolution check, refining the surrogate on the fly and cutting full-FFT calls by >90%.\n\n2. Hierarchical Monte-Carlo Tree Search Planner  \n   \u2022 A progressive-widening MCTS uses surrogate gradients to bias rollout policies at two scales: a coarse \u201cblock merge\u201d level for large jumps and a fine \u201clocal flip\u201d level for precise adjustments.  \n   \u2022 Rollouts are guided by an entropy-regularized value network that balances exploration/exploitation, accelerating escape from plateaus 5\u00d7 faster than single-scale RL alone.\n\n3. Adaptive Multi-Fidelity Evaluation Pipeline  \n   \u2022 We integrate bit-parallel AVX512 incremental convolution for small neighborhoods and a JAX/CUDA FFT kernel with dynamic tensor tiling for large moves, orchestrated by a budget-aware scheduler.  \n   \u2022 A non-stationary, regret-minimizing bandit dynamically allocates CPU/GPU resources and fidelity levels based on live performance gains, ensuring an order-of-magnitude reduction in per-move cost.\n\nWe will implement the surrogate and policy networks in PyTorch, high-performance kernels in C++...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# simple cache for compute_ratio to avoid redundant computations\nratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # simple cache lookup to skip redundant computations\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in ratio_cache:\n        return ratio_cache[key]\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        # cache infeasible cases to avoid rechecking the same zero\u2010vector pair\n        ratio_cache[key] = -1.0\n        return -1.0\n    # sparse direct\u2010set approach for small supports, else FFT convolution\n    ones_A = np.flatnonzero(A_ind)\n    ones_B = np.flatnonzero(B_ind)\n    if ones_A.size * ones_B.size < 1000:\n        # Minkowski sum/difference via Python sets\n        sumset = {i + j for i in ones_A for j in ones_B}\n        difset = {i - j for i in ones_A for j in ones_B}\n        ratio = -1.0 if not difset else len(sumset) / len(difset)\n        ratio_cache[key] = ratio\n        return ratio\n    # FFT\u2010based convolution for larger supports\n    n = len(A_ind) + len(B_ind) - 1\n    fa = np.fft.fft(A_ind.astype(float), n)\n    fb = np.fft.fft(B_ind.astype(float), n)\n    sums_conv = np.rint(np.fft.ifft(fa * fb)).astype(int)\n    num_sums = np.count_nonzero(sums_conv)\n    # use cross\u2010correlation via conjugate to get the difference spectrum\n    diffs_conv = np.rint(np.fft.ifft(fa * np.conj(fb))).astype(int)\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        ratio_cache[key] = -1.0\n        return -1.0\n    ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Return a new vector with one bit added or removed, ensuring at least one '1'.\"\"\"\n    new_ind = ind.copy()\n    ones = np.flatnonzero(new_ind)\n    zeros = np.flatnonzero(~new_ind)\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        new_ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        new_ind[rng.choice(ones)] = False\n    return new_ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        # propose move on only one vector to reduce copying overhead\n        if rng.random() < 0.5:\n            # propose_move already copies internally, so pass current_A directly\n            C_ind = propose_move(current_A)\n            D_ind = current_B  # unchanged\n        else:\n            C_ind = current_A  # unchanged\n            # likewise avoid duplicate copy for B\n            D_ind = propose_move(current_B)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.3846\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5161\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5161)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5161)\n\n\n### Program 2 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n# Initialize random number generator for reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_sum_diff_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    S = np.convolve(A_ind, B_ind)\n    sizeS = np.count_nonzero(S)\n    D = np.convolve(A_ind, B_ind[::-1])\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n### Program 3 (Score: -1.0000)\n```python\nimport numpy as np\n# For reproducible randomness, use numpy's Generator\nrng = np.random.default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    S = np.convolve(A_ind, B_ind)\n    sizeS = np.count_nonzero(S)\n    D = np.convolve(A_ind, B_ind[::-1])\n    sizeD = np.count_nonzero(D)\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.2105)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n# Prebind convolution for faster inner-loop calls\nconvolve = np.convolve\n\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: -1.0000)\n```python\nimport numpy as np\n\n# For reproducibility\nrng = np.random.default_rng(42)\n\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n# Current Proposal\nTitle: Multi-Agent Surrogate-Guided Hierarchical Planning with Continuous Relaxation and Low-Fidelity Ensembles for Efficient Sum-to-Difference Ratio Maximization\n\nAbstract:  \nWe observe that current hill-climbing and single-agent metaheuristics for maximizing the sum-to-difference ratio on large indicator vectors suffer from (i) short-sighted, memoryless bit-flip moves that become trapped in deep local optima and (ii) excessive full-scale FFT evaluations, limiting scalability. Building on graph-based surrogates, multi-fidelity bandits, and hierarchical search, we propose an integrated framework with four key innovations:\n\n1. Continuous-Relaxation Move Generator with Gumbel-Softmax  \n   \u2022 We embed binary indicator vectors into a continuous latent space via a lightweight encoder and use Gumbel-Softmax parameterizations to propose multi\u2013bit flips in a single step.  \n   \u2022 A policy network (actor) trained with a diversity-seeking objective (GFlowNet loss) learns a probabilistic distribution over high-impact move patterns, significantly improving exploration of distant optima.\n\n2. Ensemble Graph-Neural-Network Surrogates with Uncertainty Quantification  \n   \u2022 Instead of a single transformer surrogate, we maintain an ensemble of shallow bipartite graph neural networks (GNNs) that predict ratio changes and provide Bayesian-style uncertainty estimates via Monte-Carlo dropout.  \n   \u2022 Uncertain or high-potential proposals trigger on-the-fly calibration using fast incremental convolution, reducing unnecessary full FFT calls by up to 95%.\n\n3. Hierarchical Multi-Agent Planning with Contextual Bandit Meta-Controller  \n   \u2022 We orchestrate two cooperating agents: a \u201cBlock-Merge\u201d planner for coarse, multi-bit jump proposals, and a \u201cLocal-Refine\u201d planner for fine bit flips, each guided by surrogate gradients and value estimates.  \n   \u2022 A contextual bandit controller dynamically allocates CPU/GPU resources and selects between agents, fidelity levels, and sampling strategies based on l...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Here are three targeted improvements focused on reducing Python\u2010level loops, cutting down FFT cost, and prebinding hot functions for better throughput:\n\n1) Vectorize the small-support \u201cMinkowski\u201d case to eliminate the Python set comprehensions.  \n2) Switch from complex FFT to real\u2010FFT (rfft/irfft) for the large\u2010support convolution, roughly halving the work.  \n3) Prebind key numpy routines once at import time to cut attribute lookups in the hot loop.\n\nAll changes are in SEARCH/REPLACE diff form:\n\n```diff\n<<<<<<< SEARCH\nimport numpy as np\nfrom numpy.random import default_rng\n=======\nimport numpy as np\nfrom numpy.random import default_rng\n# Prebind numpy functions for performance\n_unique = np.unique\n_count_nonzero = np.count_nonzero\n_rint = np.rint\n>>>>>>> REPLACE\n```\n\n```diff\n<<<<<<< SEARCH\n    if ones_A.size * ones_B.size < 1000:\n        # Minkowski sum/difference via Python sets\n        sumset = {i + j for i in ones_A for j in ones_B}\n        difset = {i - j for i in ones_A for j in ones_B}\n        ratio = -1.0 if not difset else len(sumset) / len(difset)\n        ratio_cache[key] = ratio\n        return ratio\n=======\n    if ones_A.size * ones_B.size < 1000:\n        # Vectorized Minkowski sum/difference using numpy\n        sum_vals = ones_A[:, None] + ones_B[None, :]\n        dif_vals = ones_A[:, None] - ones_B[None, :]\n        num_sums = _unique(sum_vals).size\n        num_diffs = _unique(dif_vals).size\n        if num_diffs == 0:\n            ratio_cache[key] = -1.0\n            return -1.0\n        ratio = num_sums / num_diffs\n        ratio_cache[key] = ratio\n        return ratio\n>>>>>>> REPLACE\n```\n\n```diff\n<<<<<<< SEARCH\n    # FFT\u2010based convolution for larger supports\n    n = len(A_ind) + len(B_ind) - 1\n    fa = np.fft.fft(A_ind.astype(float), n)\n    fb = np.fft.fft(B_ind.astype(float), n)\n    sums_conv = np.rint(np.fft.ifft(fa * fb)).astype(int)\n    num_sums = np.count_nonzero(sums_conv)\n    # use cross\u2010correlation via conjugate to get the difference spectrum\n    diffs_conv = np.rint(np.fft.ifft(fa * np.conj(fb))).astype(int)\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        ratio_cache[key] = -1.0\n        return -1.0\n    ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n=======\n    # FFT\u2010based convolution for larger supports (use real FFT for binary signals)\n    n = A_ind.size + B_ind.size - 1\n    fa = np.fft.rfft(A_ind.astype(float), n)\n    fb = np.fft.rfft(B_ind.astype(float), n)\n    sums_conv = _rint(np.fft.irfft(fa * fb, n)).astype(int)\n    num_sums = _count_nonzero(sums_conv)\n    # cross-correlation via conjugate multiplication\n    diffs_conv = _rint(np.fft.irfft(fa * np.conj(fb), n)).astype(int)\n    num_diffs = _count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        ratio_cache[key] = -1.0\n        return -1.0\n    ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n>>>>>>> REPLACE\n```\n\nRationale:\n- The numpy\u2010vectorized Minkowski (outer sum/diff + unique) avoids Python loops for small supports.  \n- Real\u2010FFT (rfft/irfft) halves the transform size and memory traffic on real-valued binary arrays.  \n- Prebinding `_unique, _count_nonzero, _rint` cuts down attribute lookups in the inner loop.  \n\nThese targeted tweaks should boost both raw throughput and maintainability without a wholesale rewrite."]}}}