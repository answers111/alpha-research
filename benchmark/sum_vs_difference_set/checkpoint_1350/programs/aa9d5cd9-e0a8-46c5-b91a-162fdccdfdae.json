{"id": "aa9d5cd9-e0a8-46c5-b91a-162fdccdfdae", "code": "import numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nchoice_fn = rng.choice\nexp = math.exp\ncnz = np.count_nonzero\n\n# Cache for computed ratios to avoid redundant evaluations\nratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # Multi-call caching\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in ratio_cache:\n        return ratio_cache[key]\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        ratio = -1.0\n    else:\n        # arrays are already int8\n        A_arr = A_ind\n        B_arr = B_ind\n        # use prebound functions to reduce attribute lookups\n        sums = convolve_fn(A_arr, B_arr)\n        num_sums = int((sums > 0).sum())\n        diffs = correlate_fn(A_arr, B_arr, mode='full')\n        num_diffs = int((diffs > 0).sum())\n        if num_diffs == 0:\n            ratio = -1.0\n        else:\n            ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move to maintain constant cardinality and improve exploration.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(ind == 0)\n    if ones.size and zeros.size:\n        # with small probability do a two-bit swap for larger jumps\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:  # higher chance for multi-bit swaps\n            removes = rng.choice(ones, size=2, replace=False)\n            adds = rng.choice(zeros, size=2, replace=False)\n            ind[removes] = False\n            ind[adds] = True\n        else:\n            i_remove = rng.choice(ones)\n            i_add = rng.choice(zeros)\n            ind[i_remove] = False\n            ind[i_add] = True\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 20  # increased number of local proposals for better exploration\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=np.int8)\n    B_ind = np.zeros(N, dtype=np.int8)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9999        # slightly slower cooling for extended exploration\n    # restart logic to escape stagnation\n    no_improve = 0\n    max_no_improve = 5000\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng_random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng_random() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n            no_improve = 0\n        else:\n            no_improve += 1\n        # cool down\n        T *= decay\n        # restart when stagnated\n        if no_improve >= max_no_improve:\n            current_A, current_B = best_A.copy(), best_B.copy()\n            current_ratio = best_ratio\n            T = 1.0\n            no_improve = 0\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: EquiRatioX: Equivariant Multi\u2010Agent GFlowNet with Curriculum Fidelity and Trust\u2010Region Meta\u2010Control for Extremal Sum\u2010to\u2010Difference Optimization\n\nAbstract  \nDespite advances in hierarchical GFlowNets and adaptive\u2010fidelity surrogates, current approaches plateau near a 1.53 sum\u2010to\u2010difference ratio on benchmark N=30 problems. We trace this stagnation to (i) myopic bit\u2010swap moves lacking symmetry\u2010aware generalization, (ii) static batch\u2010size and fidelity schedules that under\u2010utilize surrogate capacity, and (iii) hand\u2010tuned annealing controllers sensitive to problem scale. EquiRatioX bridges these gaps through four synergistic innovations:\n\n1. Equivariant Graph Surrogate Ensemble  \n \u2022 We represent indicator\u2010vector pairs (A,B) as union\u2010difference graphs and train a permutation\u2010equivariant ensemble of surrogate models (linear, spectral\u2010regularized GNN, and convolutional flow) with shared contrastive embeddings.  \n \u2022 A Gaussian\u2010process uncertainty head atop the ensemble yields tight conformal bands, enabling principled fidelity switching.\n\n2. Curriculum\u2010Driven Fidelity & Batch\u2010Size Scheduling  \n \u2022 Instead of fixed or bandit\u2010driven fidelity, we introduce a curriculum scheduler that allocates evaluations across surrogates and exact calls based on rolling regret estimates and surrogate calibration drift.  \n \u2022 Batch\u2010size adapts online to state\u2010space entropy: high\u2010entropy phases spawn larger proposal batches to maximize exploratory coverage, while low\u2010entropy phases shrink batches for focused exploitation.\n\n3. Multi\u2010Agent Hierarchical GFlowNet with Motif Proposals  \n \u2022 We decompose the space of moves into \u201cmotifs\u201d\u2014learned subgraph rewiring primitives of variable size (2\u201316 bits)\u2014and train a cooperative multi\u2010agent GFlowNet where each agent specializes in a subset of motif moves.  \n \u2022 A master controller samples among agents proportional to their learned Boltzmann rewards, enabling both fine\u2010grained flips and global reconfigurations in a single unified policy.\n\n4. Trust\u2010Region Meta\u2010Controller & Differentiable Warm\u2010Starts  \n \u2022 A meta\u2010RL controller, optimized via proximal policy optimization (PPO), jointly adjusts inverse\u2010temperature, agent\u2010selection priors, and dropout in real time, enforcing trust\u2010region constraints to stabilize learning across N={30,50,80,120}.  \n \u2022 We embed a differentiable continuous relaxation of the sum\u2010to\u2010difference ratio\u2014backpropagated through the equivariant surrogates\u2014to generate pseudo\u2010gradient warm\u2010starts, refined via a limited\u2010width beam search under the current surrogate ensemble.\n\nImplementation & Impact  \nEquiRatioX is implemented in JAX (surrogates, GFlowNets) and PyTorch (meta\u2010RL, controller), orchestrated with Dask for distributed parallelism. We target surpassing a 1.9 ratio on N=30\u2013100 within 5\u00d7 the compute budget of prior work. The modular, equivariant design and curriculum fidelity framework generalize directly to other extremal combinatorial objectives, promising broad applicability and reproducible performance gains."], "idea_reward": 6.67, "parent_id": "9ea44786-cfaf-4b28-85f7-c454fb2f99ed", "generation": 19, "timestamp": 1757820311.2978046, "iteration_found": 1233, "metrics": {"score": 1.53125}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace rng_random = rng.random with 2 lines\nChange 2: 'removes = rng.choice(ones, size=2, replace=False)' to 'removes = choice_fn(ones, size=2, replace=False)'\nChange 3: 'adds = rng.choice(zeros, size=2, replace=False)' to 'adds = choice_fn(zeros, size=2, replace=False)'\nChange 4: 'i_remove = rng.choice(ones)' to 'i_remove = choice_fn(ones)'\nChange 5: 'i_add = rng.choice(zeros)' to 'i_add = choice_fn(zeros)'\nChange 6: Replace for _ in range(BATCH_SIZE):  # batch size configurable with 2 lines\nChange 7: Replace 2 lines with 2 lines", "parent_metrics": {"score": 1.53125}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: HyperOpt\u0394: Hierarchical Transformer-GFlowNet Meta\u2010Optimization with Active Fidelity and Conformal Surrogates for Extremal Sum\u2010to\u2010Difference Ratio\n\nAbstract  \nDespite recent gains, hill\u2010climbing and static\u2010schedule annealing methods stall near a 1.53 sum\u2010to\u2010difference ratio on N=30 benchmarks. We attribute this plateau to (i) myopic bit\u2010swap proposals, (ii) fixed\u2010fidelity surrogate calls, and (iii) hand\u2010tuned temperature schedules. HyperOpt\u0394 overcomes these bottlenecks through four key innovations:\n\n1. Hierarchical Transformer\u2010GFlowNet Policy  \n \u2022 We embed indicator vectors into a multi\u2010scale graph Transformer that captures local and global structure.  \n \u2022 A hierarchical GFlowNet learns to propose variable\u2010length, high\u2010reward move sequences\u2014ranging from single\u2010bit flips to eight\u2010bit reconfigurations\u2014proportional to their Boltzmann posterior.  \n\n2. Active Fidelity Scheduling with Conformal Uncertainty  \n \u2022 We replace fixed multi\u2010fidelity ensembles with an active\u2010learning controller that issues conformal prediction bands around surrogate outputs.  \n \u2022 A Thompson\u2010sampling bandit selects among cheap linear proxies, mid\u2010cost geometric graph nets, and exact evaluations, dynamically balancing exploration and exploitation with rigorous error guarantees.  \n\n3. Meta\u2010Learnt Temperature & Exploration Prior  \n \u2022 A meta\u2010RL agent, trained across N={30,50,80,120}, adjusts both the inverse\u2010temperature and Transformer\u2010attention dropout in real time.  \n \u2022 By observing acceptance rate, surrogate calibration, and GFlowNet sample diversity, it self\u2010regulates intensification and diversification to escape local traps.  \n\n4. Differentiable Relaxation Warm\u2010Start & Beam\u2010Search Refinement  \n \u2022 Continuous spectral relaxations of the sum\u2010to\u2010difference objective provide gradient\u2010based warm starts, seeding the GFlowNet with promising fractional solutions.  \n \u2022 A beam\u2010search refinement stage evaluates top\u2010k proposals via the active\u2010fidelity ensemble, ensuring precise improvements before commi...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nexp = math.exp\ncnz = np.count_nonzero\n\n# Cache for computed ratios to avoid redundant evaluations\nratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # Multi-call caching\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in ratio_cache:\n        return ratio_cache[key]\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        ratio = -1.0\n    else:\n        # arrays are already int8\n        A_arr = A_ind\n        B_arr = B_ind\n        # use prebound functions to reduce attribute lookups\n        sums = convolve_fn(A_arr, B_arr)\n        num_sums = int((sums > 0).sum())\n        diffs = correlate_fn(A_arr, B_arr, mode='full')\n        num_diffs = int((diffs > 0).sum())\n        if num_diffs == 0:\n            ratio = -1.0\n        else:\n            ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move to maintain constant cardinality and improve exploration.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(ind == 0)\n    if ones.size and zeros.size:\n        # with small probability do a two-bit swap for larger jumps\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:  # higher chance for multi-bit swaps\n            removes = rng.choice(ones, size=2, replace=False)\n            adds = rng.choice(zeros, size=2, replace=False)\n            ind[removes] = False\n            ind[adds] = True\n        else:\n            i_remove = rng.choice(ones)\n            i_add = rng.choice(zeros)\n            ind[i_remove] = False\n            ind[i_add] = True\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 20  # increased number of local proposals for better exploration\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=np.int8)\n    B_ind = np.zeros(N, dtype=np.int8)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9999        # slightly slower cooling for extended exploration\n    # restart logic to escape stagnation\n    no_improve = 0\n    max_no_improve = 5000\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng_random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng_random() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n            no_improve = 0\n        else:\n            no_improve += 1\n        # cool down\n        T *= decay\n        # restart when stagnated\n        if no_improve >= max_no_improve:\n            current_A, current_B = best_A.copy(), best_B.copy()\n            current_ratio = best_ratio\n            T = 1.0\n            no_improve = 0\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.5312\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 2 (Score: 1.2941)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# Cache to avoid redundant ratio computations\n_ratio_cache = {}\n# Prebind convolution for faster inner-loop calls\nconvolve = np.convolve\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.2941)\n\n\n### Program 3 (Score: 1.5312)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5312)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.5312)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: 1.2750)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nTitle: EquiRatioX: Equivariant Multi\u2010Agent GFlowNet with Curriculum Fidelity and Trust\u2010Region Meta\u2010Control for Extremal Sum\u2010to\u2010Difference Optimization\n\nAbstract  \nDespite advances in hierarchical GFlowNets and adaptive\u2010fidelity surrogates, current approaches plateau near a 1.53 sum\u2010to\u2010difference ratio on benchmark N=30 problems. We trace this stagnation to (i) myopic bit\u2010swap moves lacking symmetry\u2010aware generalization, (ii) static batch\u2010size and fidelity schedules that under\u2010utilize surrogate capacity, and (iii) hand\u2010tuned annealing controllers sensitive to problem scale. EquiRatioX bridges these gaps through four synergistic innovations:\n\n1. Equivariant Graph Surrogate Ensemble  \n \u2022 We represent indicator\u2010vector pairs (A,B) as union\u2010difference graphs and train a permutation\u2010equivariant ensemble of surrogate models (linear, spectral\u2010regularized GNN, and convolutional flow) with shared contrastive embeddings.  \n \u2022 A Gaussian\u2010process uncertainty head atop the ensemble yields tight conformal bands, enabling principled fidelity switching.\n\n2. Curriculum\u2010Driven Fidelity & Batch\u2010Size Scheduling  \n \u2022 Instead of fixed or bandit\u2010driven fidelity, we introduce a curriculum scheduler that allocates evaluations across surrogates and exact calls based on rolling regret estimates and surrogate calibration drift.  \n \u2022 Batch\u2010size adapts online to state\u2010space entropy: high\u2010entropy phases spawn larger proposal batches to maximize exploratory coverage, while low\u2010entropy phases shrink batches for focused exploitation.\n\n3. Multi\u2010Agent Hierarchical GFlowNet with Motif Proposals  \n \u2022 We decompose the space of moves into \u201cmotifs\u201d\u2014learned subgraph rewiring primitives of variable size (2\u201316 bits)\u2014and train a cooperative multi\u2010agent GFlowNet where each agent specializes in a subset of motif moves.  \n \u2022 A master controller samples among agents proportional to their learned Boltzmann rewards, enabling both fine\u2010grained flips and global reconfigurations in a single unified policy.\n\n4. Trust\u2010Regi...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are a handful of targeted diffs to reduce attribute lookups, avoid unnecessary copies, and start to adapt the batch\u2010size to your current temperature (exploration/exploitation) to push your score upward while also cleaning up a bit of the code. Each patch uses the exact SEARCH/REPLACE format requested.\n\nReasoning summary:  \n- Bind `rng.choice` once to `choice_fn` and switch all calls to it (cuts down on repeated attribute lookups).  \n- Only do a deep copy when we actually pick a new best candidate (instead of holding references to possibly mutated arrays).  \n- Dynamically scale the inner\u2010loop `batch_size` with the current temperature `T` to get larger exploration early and focused search later.  \n\nPlease apply each diff in turn.\n\n<<<<<<< SEARCH\nrng_random = rng.random\n=======\nrng_random = rng.random\nchoice_fn = rng.choice\n>>>>>>> REPLACE\n\n---\n\n<<<<<<< SEARCH\nremoves = rng.choice(ones, size=2, replace=False)\n=======\nremoves = choice_fn(ones, size=2, replace=False)\n>>>>>>> REPLACE\n\n---\n\n<<<<<<< SEARCH\nadds = rng.choice(zeros, size=2, replace=False)\n=======\nadds = choice_fn(zeros, size=2, replace=False)\n>>>>>>> REPLACE\n\n---\n\n<<<<<<< SEARCH\ni_remove = rng.choice(ones)\n=======\ni_remove = choice_fn(ones)\n>>>>>>> REPLACE\n\n---\n\n<<<<<<< SEARCH\ni_add = rng.choice(zeros)\n=======\ni_add = choice_fn(zeros)\n>>>>>>> REPLACE\n\n---\n\n<<<<<<< SEARCH\n            for _ in range(BATCH_SIZE):  # batch size configurable\n=======\n            batch_size = max(1, int(BATCH_SIZE * T))\n            for _ in range(batch_size):\n>>>>>>> REPLACE\n\n---\n\n<<<<<<< SEARCH\n                    # Store best candidate references directly (no extra copies)\n                    local_best_A, local_best_B = C_ind, D_ind\n=======\n                    # Store deep copies for safe isolation of best candidate\n                    local_best_A, local_best_B = C_ind.copy(), D_ind.copy()\n>>>>>>> REPLACE\n\n---\n\nWith these small but impactful changes you\u2019ll reduce overhead in the inner loop, keep your \u201cbest\u201d candidate insulated from in\u2010place mutations, and vary batch size automatically to better balance exploration vs. exploitation as your temperature cools. Let me know how the new score trends!"]}}}