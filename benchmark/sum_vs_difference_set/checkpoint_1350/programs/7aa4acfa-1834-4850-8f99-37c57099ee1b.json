{"id": "7aa4acfa-1834-4850-8f99-37c57099ee1b", "code": "import numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Removed caching to reduce per\u2010call overhead\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nrng_choice = rng.choice\nexp = math.exp\ncnz = np.count_nonzero\nany_fn = np.any\nflatnonzero_fn = np.flatnonzero\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check for empty sets\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    num_sums = cnz(convolve_fn(A_ind, B_ind))\n    num_diffs = cnz(correlate_fn(A_ind, B_ind, mode='full'))\n    return -1.0 if num_diffs == 0 else num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move to maintain constant cardinality and improve exploration.\"\"\"\n    ones = flatnonzero_fn(ind)\n    zeros = flatnonzero_fn(1 - ind)\n    if ones.size and zeros.size:\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:\n            removes = rng_choice(ones, size=2, replace=False)\n            adds = rng_choice(zeros, size=2, replace=False)\n            ind[removes] = 0\n            ind[adds] = 1\n        else:\n            i_remove = rng_choice(ones)\n            i_add = rng_choice(zeros)\n            ind[i_remove] = 0\n            ind[i_add] = 1\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 20  # increased number of local proposals for better exploration\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    # use boolean arrays for lighter\u2010weight, faster bit\u2010ops\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = 1\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.99995       # slower cooling for extended exploration\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    # Pre-bind heavy functions once per outer loop\n    _compute_ratio = compute_ratio\n    _rand        = rng_random\n    _exp         = exp\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            r = _rand()\n            # With some probability, mutate both for deeper exploration\n            if r < 0.2:\n                C_ind = propose_move(C_ind)\n                D_ind = propose_move(D_ind)\n            elif r < 0.6:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = _compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or _rand() < _exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: An Adaptive Multi\u2010Island Quality\u2010Diversity Framework with Hierarchical Surrogates for Enhanced Sum\u2010to\u2010Difference Optimization\n\nAbstract:  \nThe current hill\u2010climbing and batchedannealing hybrid achieves a modest sum\u2010to\u2010difference ratio (\u22481.5455 on N=30) but suffers from stagnation in local optima, uneven exploration\u2013exploitation balance, and high convolutional overhead. We propose a novel, implementable framework that addresses these issues through four key innovations:\n\n1. Dynamic Multi\u2010Island Quality\u2010Diversity (QD) Search  \n   \u2022 Maintain an asynchronous pool of \u201cislands,\u201d each running distinct evolutionary operators (1\u2010swap, 2\u2010swap, block exchange, differential crossover) under differing temperature schedules.  \n   \u2022 Employ MAP\u2010Elites to archive elites by (ratio, Hamming\u2010distance diversity) bins, ensuring continual discovery of both high\u2010ratio and structurally novel solutions.  \n   \u2022 Periodically migrate top performers between islands based on an adaptive migration policy derived from multi\u2010armed\u2010bandit credit assignment, preventing premature convergence.\n\n2. Hierarchical Multi\u2010Fidelity Surrogate Modeling  \n   \u2022 Introduce a two\u2010level surrogate: a linear\u2010time \u201csketch\u201d model using random projections to approximate conv/corr counts, followed by a lightweight graph\u2010neural surrogate trained on summary statistics and low\u2010rank convolution features.  \n   \u2022 Pre\u2010screen move proposals through the sketch model, then refine candidate ranking with the neural surrogate, resorting to exact GPU\u2010accelerated cuFFT conv/corr only for the top\u2010K moves per island per generation.\n\n3. Reinforcement\u2010Augmented Operator Selection  \n   \u2022 Frame mutation and crossover choice as a contextual bandit, where each operator\u2019s historical improvement, runtime cost, and diversity contribution inform its selection probability.  \n   \u2022 Continuously update operator utilities during search, enabling the system to focus compute on the most promising transformations.\n\n4. Incremental Ratio Computation and GPU Optimization  \n   \u2022 Leverage bit\u2010parallel rolling\u2010update formulas to incrementally adjust convolution and correlation counts after local moves, reducing full\u2010scale FFT calls by \u226570%.  \n   \u2022 Implement all heavy kernels in JAX/PyTorch with tensor\u2010core acceleration, achieving an estimated 10\u00d7 speed\u2010up over vanilla NumPy.\n\nExpected Outcomes:  \nOn N=30\u2013100 problems, we anticipate reaching target ratios \u22651.8 in 30\u201350% fewer evaluations, while the QD archive maintains a diverse portfolio of high\u2010quality solutions. The hierarchical surrogates and operator learning together promise robust escape from local traps and orders\u2010of\u2010magnitude runtime gains.  \n\nImplementation Plan:  \nPhase I: Develop and benchmark the sketch and neural surrogates on random (A,B) data.  \nPhase II: Integrate surrogates into isolated QD islands and validate incremental update logic.  \nPhase III: Build the reinforcement\u2010based operator scheduler and GPU kernels, followed by ablation studies.  \nPhase IV: Deploy on full\u2010scale instances (N=100), compare against current baselines, and refine migration and restart heuristics.  \n\nThis adaptive, multi\u2010island, quality\u2010diversity approach combines proven evolutionary strategies, lightweight surrogate screening, and reinforcement\u2010guided exploration to surmount existing bottlenecks in sum\u2010to\u2010difference maximization."], "idea_reward": 6.0, "parent_id": "d77dd970-31da-4084-bf82-e39b0985e10f", "generation": 20, "timestamp": 1757821878.4850936, "iteration_found": 1248, "metrics": {"score": 1.53125}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 5 lines with 7 lines\nChange 2: 'decay = 0.9999        # even slower cooling to escape local optima' to 'decay = 0.99995       # slower cooling for extended exploration'\nChange 3: Replace 2 lines with 2 lines", "parent_metrics": {"score": 1.5454545454545454}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: A Hybrid Evolutionary-Surrogate Framework for Maximizing Sum\u2010to\u2010Difference Ratios in Binary Indicator Sets\n\nAbstract:\nWe address key limitations of existing hill\u2010climbing and simulated\u2010annealing methods for maximizing the sum-to-difference ratio |A+B|/|A\u2212B| over 0/1 indicator vectors: slow convergence, vulnerability to local optima, and high per\u2010evaluation cost. Building on successful elements of batch proposal schemes and adaptive cooling, we propose a novel, implementable framework that combines evolutionary search, learned surrogate models, and GPU\u2010accelerated ratio estimation.\n\n1. Identified Weaknesses  \n\u2022 Local\u2010optima stagnation under simple bit\u2010swap proposals and single\u2010chain annealing schedules.  \n\u2022 High computational overhead from repeated full convolutions and correlations in ratio evaluations.  \n\u2022 Limited diversity in proposals due to fixed\u2010size batch sampling from the current state.\n\n2. Technical Innovations  \n\u2022 Surrogate\u2010Accelerated Evaluation: Train a lightweight neural or Gaussian\u2010process surrogate to predict the sum\u2010to\u2010difference ratio from low\u2010dimensional summary statistics of (A,B). Use the surrogate to pre\u2010screen candidate moves, invoking the exact conv/corr only on top\u2010K proposals per batch.  \n\u2022 Multi\u2010Population Evolutionary Scheme: Maintain multiple subpopulations of (A,B) vectors evolving in parallel, each using specialized mutation operators (1-swap, 2-swap, block exchange). Periodically exchange elite individuals across populations to escape local traps.  \n\u2022 Adaptive Proposal Distribution: Instead of uniform random swaps, learn a per\u2010bit importance weight via reinforcement\u2010learning\u2013style credit assignment, biasing moves towards positions historically contributing most to ratio improvements.  \n\u2022 GPU\u2010Accelerated Convolutions: Implement convolution/correlation ratio computation using cuFFT or tensor\u2010core\u2013optimized dot\u2010products, reducing per\u2010evaluation latency by up to 10\u00d7 for large N.  \n\u2022 Dynamic Cooling and Restart Strategies: Replace fix...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Removed caching to reduce per\u2010call overhead\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nrng_choice = rng.choice\nexp = math.exp\ncnz = np.count_nonzero\nany_fn = np.any\nflatnonzero_fn = np.flatnonzero\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check for empty sets\n    if not any_fn(A_ind) or not any_fn(B_ind):\n        return -1.0\n    num_sums = cnz(convolve_fn(A_ind, B_ind))\n    num_diffs = cnz(correlate_fn(A_ind, B_ind, mode='full'))\n    return -1.0 if num_diffs == 0 else num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move to maintain constant cardinality and improve exploration.\"\"\"\n    ones = flatnonzero_fn(ind)\n    zeros = flatnonzero_fn(1 - ind)\n    if ones.size and zeros.size:\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:\n            removes = rng_choice(ones, size=2, replace=False)\n            adds = rng_choice(zeros, size=2, replace=False)\n            ind[removes] = 0\n            ind[adds] = 1\n        else:\n            i_remove = rng_choice(ones)\n            i_add = rng_choice(zeros)\n            ind[i_remove] = 0\n            ind[i_add] = 1\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 20  # increased number of local proposals for better exploration\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    # use boolean arrays for lighter\u2010weight, faster bit\u2010ops\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = 1\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9999        # even slower cooling to escape local optima\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    # Pre-bind heavy functions once per outer loop\n    _compute_ratio = compute_ratio\n    _rand        = rng_random\n    _exp         = exp\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            r = _rand()\n            # With some probability, mutate both for deeper exploration\n            if r < 0.2:\n                C_ind = propose_move(C_ind)\n                D_ind = propose_move(D_ind)\n            elif r < 0.6:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = _compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or _rand() < _exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.5455\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 2 (Score: 1.3478)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# bind once to avoid repeated attribute lookups in compute_ratio\n_conv = np.convolve\n_corr = np.correlate\n_cnt_nonzero = np.count_nonzero\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.3478)\n\n\n### Program 3 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.5000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility and performance\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nTitle: An Adaptive Multi\u2010Island Quality\u2010Diversity Framework with Hierarchical Surrogates for Enhanced Sum\u2010to\u2010Difference Optimization\n\nAbstract:  \nThe current hill\u2010climbing and batchedannealing hybrid achieves a modest sum\u2010to\u2010difference ratio (\u22481.5455 on N=30) but suffers from stagnation in local optima, uneven exploration\u2013exploitation balance, and high convolutional overhead. We propose a novel, implementable framework that addresses these issues through four key innovations:\n\n1. Dynamic Multi\u2010Island Quality\u2010Diversity (QD) Search  \n   \u2022 Maintain an asynchronous pool of \u201cislands,\u201d each running distinct evolutionary operators (1\u2010swap, 2\u2010swap, block exchange, differential crossover) under differing temperature schedules.  \n   \u2022 Employ MAP\u2010Elites to archive elites by (ratio, Hamming\u2010distance diversity) bins, ensuring continual discovery of both high\u2010ratio and structurally novel solutions.  \n   \u2022 Periodically migrate top performers between islands based on an adaptive migration policy derived from multi\u2010armed\u2010bandit credit assignment, preventing premature convergence.\n\n2. Hierarchical Multi\u2010Fidelity Surrogate Modeling  \n   \u2022 Introduce a two\u2010level surrogate: a linear\u2010time \u201csketch\u201d model using random projections to approximate conv/corr counts, followed by a lightweight graph\u2010neural surrogate trained on summary statistics and low\u2010rank convolution features.  \n   \u2022 Pre\u2010screen move proposals through the sketch model, then refine candidate ranking with the neural surrogate, resorting to exact GPU\u2010accelerated cuFFT conv/corr only for the top\u2010K moves per island per generation.\n\n3. Reinforcement\u2010Augmented Operator Selection  \n   \u2022 Frame mutation and crossover choice as a contextual bandit, where each operator\u2019s historical improvement, runtime cost, and diversity contribution inform its selection probability.  \n   \u2022 Continuously update operator utilities during search, enabling the system to focus compute on the most promising transformations.\n\n4. Incremental Ratio Computation and...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are a few targeted changes that (1) increase the chance and size of \u201cmulti\u2010swap\u201d mutations to escape local optima, (2) slow down cooling for longer exploration, and (3) simplify the empty\u2010set check in `compute_ratio`. Each diff strictly follows the SEARCH/REPLACE format.\n\n1) Increase multi\u2010swap probability from 0.3\u21920.5 and allow up to 3\u2010bit swaps instead of always 2:\n\n<<<<<<< SEARCH\n    if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:\n        removes = rng_choice(ones, size=2, replace=False)\n        adds = rng_choice(zeros, size=2, replace=False)\n        ind[removes] = 0\n        ind[adds] = 1\n=======\n    if rng_random() < 0.5 and ones.size > 1 and zeros.size > 1:\n        # allow up to 3\u2010bit block swaps for deeper jumps\n        l = min(3, ones.size, zeros.size)\n        removes = rng_choice(ones, size=l, replace=False)\n        adds    = rng_choice(zeros, size=l, replace=False)\n        ind[removes] = 0\n        ind[adds]    = 1\n>>>>>>> REPLACE\n\n2) Slow down the temperature decay (0.9999\u21920.99995) so the annealer explores longer before freezing:\n\n<<<<<<< SEARCH\n    decay = 0.9999        # even slower cooling to escape local optima\n=======\n    decay = 0.99995       # slower cooling for extended exploration\n>>>>>>> REPLACE\n\n3) Replace the generic `any_fn` alias with the array\u2010method `.any()`, removing one level of indirection and making the intent clearer:\n\n<<<<<<< SEARCH\n    if not any_fn(A_ind) or not any_fn(B_ind):\n        return -1.0\n=======\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n>>>>>>> REPLACE\n\nExplanation of benefits:\n\n- A larger and more frequent multi\u2010swap raises the chance of jumping out of poor local optima, which should help improve the final ratio.\n- A slightly reduced decay rate keeps the system \u201chot\u201d longer, allowing uphill moves later in the run.\n- Using `A_ind.any()` is marginally faster and more readable than a separate alias."]}}}