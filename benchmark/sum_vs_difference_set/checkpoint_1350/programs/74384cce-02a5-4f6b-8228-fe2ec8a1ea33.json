{"id": "74384cce-02a5-4f6b-8228-fe2ec8a1ea33", "code": "import numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# simple cache for compute_ratio to avoid redundant computations\nratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # simple cache lookup to skip redundant computations\n    # Pack boolean arrays into bytes for hashing\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in ratio_cache:\n        return ratio_cache[key]\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        # cache infeasible cases to avoid rechecking the same zero\u2010vector pair\n        ratio_cache[key] = -1.0\n        return -1.0\n    # sparse direct\u2010set approach for small supports, else FFT convolution\n    ones_A = np.flatnonzero(A_ind)\n    ones_B = np.flatnonzero(B_ind)\n    if ones_A.size * ones_B.size < 1000:\n        # Vectorized Minkowski sum/difference using numpy broadcasting\n        sum_idxs = np.add.outer(ones_A, ones_B).ravel()\n        dif_idxs = np.subtract.outer(ones_A, ones_B).ravel()\n        num_sums = np.unique(sum_idxs).size\n        unique_diffs = np.unique(dif_idxs)\n        if unique_diffs.size == 0:\n            ratio = -1.0\n        else:\n            ratio = num_sums / unique_diffs.size\n        ratio_cache[key] = ratio\n        return ratio\n    # FFT\u2010based convolution for larger supports using real FFT for efficiency\n    fa = np.fft.rfft(A_ind.astype(float), FFT_N)\n    fb = np.fft.rfft(B_ind.astype(float), FFT_N)\n    # compute Minkowski sum via convolution\n    sums_conv = np.rint(np.fft.irfft(fa * fb, FFT_N)).astype(int)\n    num_sums = np.count_nonzero(sums_conv)\n    # compute Minkowski difference via cross-correlation\n    diffs_conv = np.rint(np.fft.irfft(fa * np.conj(fb), FFT_N)).astype(int)\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        ratio_cache[key] = -1.0\n        return -1.0\n    ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Return a new vector with one bit added or removed, ensuring at least one '1'.\"\"\"\n    new_ind = ind.copy()\n    ones = np.flatnonzero(new_ind)\n    zeros = np.flatnonzero(~new_ind)\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        new_ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        new_ind[rng.choice(ones)] = False\n    return new_ind\n\n# Configuration constants\nDEFAULT_N = 30\n# Precompute FFT size for constant\u2010length vectors to avoid repeated reallocation\nFFT_N = 2 * DEFAULT_N - 1\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        # propose move on only one vector to reduce copying overhead\n        if rng.random() < 0.5:\n            # propose_move already copies internally, so pass current_A directly\n            C_ind = propose_move(current_A)\n            D_ind = current_B  # unchanged\n        else:\n            C_ind = current_A  # unchanged\n            # likewise avoid duplicate copy for B\n            D_ind = propose_move(current_B)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: Hybrid Latent-Space Bayesian Optimization with Meta-Surrogates and Contextual Fidelity Scheduling for Sum-to-Difference Ratio Maximization\n\nAbstract:\nWe propose a unified, implementable framework that addresses two core shortcomings of existing hill-climbing and surrogate-MCTS baselines\u2014namely, myopic local moves in a high-dimensional discrete space and the prohibitive cost of full-FFT evaluations. Our four key contributions are:\n\n1. Latent-Space Meta-Surrogate Ensemble  \n   \u2022 We train a variational autoencoder (VAE) to embed pairs of 0/1 indicator vectors into a low-dimensional continuous latent.  \n   \u2022 On top of the latent codes, we fit an ensemble of deep-kernel Gaussian processes (DKGPs) calibrated via deep ensembles to provide well-quantified uncertainty and approximate gradients for the sum-to-difference ratio.\n\n2. Global Bayesian Optimization + Discrete Refinement  \n   \u2022 We perform global search in latent space using Thompson-sampling-driven expected-improvement acquisition, yielding diverse candidate decodings.  \n   \u2022 Each decoded bitstring is locally refined by an uncertainty-aware actor-critic policy network\u2014trained via reinforced Monte-Carlo Tree Search with progressive widening\u2014to escape deep local optima 10\u00d7 faster than memoryless flips.\n\n3. Contextual Multi-Fidelity Scheduler  \n   \u2022 A contextual bandit model dynamically allocates compute among three fidelities: lightweight bit-parallel bitboard approximations, mixed-precision JAX/CUDA FFT kernels, and exact full-FFT.  \n   \u2022 Allocation decisions are driven by predicted information gain per second, achieving \u226595% reduction in exact-evaluation calls without degrading final ratio quality.\n\n4. Meta-Learning Across Problem Scales  \n   \u2022 We employ Model-Agnostic Meta-Learning (MAML) to precondition both the VAE and policy networks across a range of N, enabling rapid adaptation to new vector lengths with minimal additional samples.\n\nWe will implement neural components in PyTorch/JAX and high-performance kernels in C++/CUDA, orchestrated via Python. Benchmarks on N=50\u201310 000 against hill-climbing, tabu search, GA, and state-of-the-art surrogate methods will target sum-to-difference ratios > 1.75 and full-FFT calls reduced by \u2265 95%. Our design generalizes readily to other additive-combinatorics and large-scale sequence-alignment problems."], "idea_reward": 6.0, "parent_id": "26670884-ea5a-4b65-91e2-bb0891519d93", "generation": 15, "timestamp": 1757793633.6571949, "iteration_found": 789, "metrics": {"score": 1.3846153846153846}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 4 lines with 2 lines\nChange 2: Replace 7 lines with 12 lines\nChange 3: Replace 16 lines with 15 lines", "parent_metrics": {"score": 1.3846153846153846}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: Multi-Scale Model-Guided Monte-Carlo Planning with Uncertainty-Aware Surrogates and Budget-Aware Fidelity Scheduling for Sum-to-Difference Ratio Maximization\n\nAbstract:\nThe current hill-climbing plus simulated-annealing baseline (score = 1.3846) suffers from two intertwined bottlenecks: (1) myopic single-bit proposals that stagnate in deep local optima and (2) expensive full-FFT evaluations that dominate runtime. To overcome these, we propose a unified, implementable framework with three synergistic components:\n\n1. Uncertainty-Aware Graph Surrogate  \n   \u2022 A graph-neural-process model pre-trained on synthetic bit-flip landscapes to predict sum-to-difference ratios and approximate local gradients.  \n   \u2022 Online active learning via Thompson sampling: high-uncertainty or high-gain moves trigger lightweight AVX-512 convolution checks, reducing full-FFT calls by \u226595%.\n\n2. Hybrid Monte-Carlo Tree Search with Learned Policies  \n   \u2022 Progressive-widening MCTS operating at two granularities\u2014a coarse \u201cblock merge\u201d phase and a fine \u201csingle-flip\u201d phase\u2014each guided by upper-confidence bounds.  \n   \u2022 An actor-critic network, continuously refined through experience replay, biases rollouts to balance exploration and exploitation, escaping plateaus 5\u00d7 faster than memoryless search.\n\n3. Budget-Aware Multi-Fidelity Scheduler  \n   \u2022 A non-stationary, regret-minimizing multi-armed bandit dynamically allocates compute between bit-parallel AVX-512 kernels, JAX/CUDA FFTs, and the surrogate.  \n   \u2022 Resource allocation adapts in real time to observed per-move gains, achieving an order-of-magnitude speedup.\n\nWe will implement neural modules in PyTorch/JAX and high-performance kernels in C++/CUDA, orchestrated via Python. Benchmarked on N = 50\u20135000 against hill-climbing, tabu search, GA, and state-of-the-art surrogate methods, we target a \u226515% improvement (ratio > 1.60) and \u226590% reduction in full-evaluation calls. Our design naturally extends to general additive-combinatorics and large-s...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# simple cache for compute_ratio to avoid redundant computations\nratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # simple cache lookup to skip redundant computations\n    # Pack boolean arrays into machine\u2010word integers for faster hashing\n    packed_A = int.from_bytes(np.packbits(A_ind).tobytes(), byteorder='little')\n    packed_B = int.from_bytes(np.packbits(B_ind).tobytes(), byteorder='little')\n    key = (packed_A, packed_B)\n    if key in ratio_cache:\n        return ratio_cache[key]\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        # cache infeasible cases to avoid rechecking the same zero\u2010vector pair\n        ratio_cache[key] = -1.0\n        return -1.0\n    # sparse direct\u2010set approach for small supports, else FFT convolution\n    ones_A = np.flatnonzero(A_ind)\n    ones_B = np.flatnonzero(B_ind)\n    if ones_A.size * ones_B.size < 1000:\n        # Minkowski sum/difference via Python sets\n        sumset = {i + j for i in ones_A for j in ones_B}\n        difset = {i - j for i in ones_A for j in ones_B}\n        ratio = -1.0 if not difset else len(sumset) / len(difset)\n        ratio_cache[key] = ratio\n        return ratio\n    # FFT\u2010based convolution for larger supports\n    # Use precomputed FFT length to cut down on repeated array allocations\n    fa = np.fft.fft(A_ind.astype(float), FFT_N)\n    fb = np.fft.fft(B_ind.astype(float), FFT_N)\n    n = FFT_N\n    sums_conv = np.rint(np.fft.ifft(fa * fb)).astype(int)\n    num_sums = np.count_nonzero(sums_conv)\n    # use cross\u2010correlation via conjugate to get the difference spectrum\n    diffs_conv = np.rint(np.fft.ifft(fa * np.conj(fb))).astype(int)\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        ratio_cache[key] = -1.0\n        return -1.0\n    ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Return a new vector with one bit added or removed, ensuring at least one '1'.\"\"\"\n    new_ind = ind.copy()\n    ones = np.flatnonzero(new_ind)\n    zeros = np.flatnonzero(~new_ind)\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        new_ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        new_ind[rng.choice(ones)] = False\n    return new_ind\n\n# Configuration constants\nDEFAULT_N = 30\n# Precompute FFT size for constant\u2010length vectors to avoid repeated reallocation\nFFT_N = 2 * DEFAULT_N - 1\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        # propose move on only one vector to reduce copying overhead\n        if rng.random() < 0.5:\n            # propose_move already copies internally, so pass current_A directly\n            C_ind = propose_move(current_A)\n            D_ind = current_B  # unchanged\n        else:\n            C_ind = current_A  # unchanged\n            # likewise avoid duplicate copy for B\n            D_ind = propose_move(current_B)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.3846\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5312\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5455\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5455\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5455)\n\n\n### Program 2 (Score: 0.9737)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check on non-empty sets\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (0.9737)\n\n\n### Program 3 (Score: 1.0294)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# Hyperparameters for advanced local search\nMULTI_FLIP_PROB = 0.05      # chance to try a 2\u2010bit block swap instead of a single flip\nWORSE_ACCEPT_PROB = 0.01    # small probability to accept a non\u2010improving move\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.0294)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.4138)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: -1.0000)\n```python\nimport numpy as np\nimport random\n\n# For reproducibility\nrandom.seed(42)\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n# Current Proposal\nTitle: Hybrid Latent-Space Bayesian Optimization with Meta-Surrogates and Contextual Fidelity Scheduling for Sum-to-Difference Ratio Maximization\n\nAbstract:\nWe propose a unified, implementable framework that addresses two core shortcomings of existing hill-climbing and surrogate-MCTS baselines\u2014namely, myopic local moves in a high-dimensional discrete space and the prohibitive cost of full-FFT evaluations. Our four key contributions are:\n\n1. Latent-Space Meta-Surrogate Ensemble  \n   \u2022 We train a variational autoencoder (VAE) to embed pairs of 0/1 indicator vectors into a low-dimensional continuous latent.  \n   \u2022 On top of the latent codes, we fit an ensemble of deep-kernel Gaussian processes (DKGPs) calibrated via deep ensembles to provide well-quantified uncertainty and approximate gradients for the sum-to-difference ratio.\n\n2. Global Bayesian Optimization + Discrete Refinement  \n   \u2022 We perform global search in latent space using Thompson-sampling-driven expected-improvement acquisition, yielding diverse candidate decodings.  \n   \u2022 Each decoded bitstring is locally refined by an uncertainty-aware actor-critic policy network\u2014trained via reinforced Monte-Carlo Tree Search with progressive widening\u2014to escape deep local optima 10\u00d7 faster than memoryless flips.\n\n3. Contextual Multi-Fidelity Scheduler  \n   \u2022 A contextual bandit model dynamically allocates compute among three fidelities: lightweight bit-parallel bitboard approximations, mixed-precision JAX/CUDA FFT kernels, and exact full-FFT.  \n   \u2022 Allocation decisions are driven by predicted information gain per second, achieving \u226595% reduction in exact-evaluation calls without degrading final ratio quality.\n\n4. Meta-Learning Across Problem Scales  \n   \u2022 We employ Model-Agnostic Meta-Learning (MAML) to precondition both the VAE and policy networks across a range of N, enabling rapid adaptation to new vector lengths with minimal additional samples.\n\nWe will implement neural components in PyTorch/JAX and high-performance...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are three targeted diffs to simplify hashing, speed\u2010up the small\u2010support branch via vectorized NumPy, and cut FFT cost in half by using real\u2010valued transforms.\n\n<<<<<<< SEARCH\n    # Pack boolean arrays into machine\u2010word integers for faster hashing\n    packed_A = int.from_bytes(np.packbits(A_ind).tobytes(), byteorder='little')\n    packed_B = int.from_bytes(np.packbits(B_ind).tobytes(), byteorder='little')\n    key = (packed_A, packed_B)\n=======\n    # Pack boolean arrays into bytes for hashing\n    key = (A_ind.tobytes(), B_ind.tobytes())\n>>>>>>> REPLACE\n\nReasoning: dropping `np.packbits`+`int.from_bytes` in favor of raw `.tobytes()` yields a simpler key, removes an expensive pack/unpack step, and keeps all bits.\n\n---\n\n<<<<<<< SEARCH\n    if ones_A.size * ones_B.size < 1000:\n        # Minkowski sum/difference via Python sets\n        sumset = {i + j for i in ones_A for j in ones_B}\n        difset = {i - j for i in ones_A for j in ones_B}\n        ratio = -1.0 if not difset else len(sumset) / len(difset)\n        ratio_cache[key] = ratio\n        return ratio\n=======\n    if ones_A.size * ones_B.size < 1000:\n        # Vectorized Minkowski sum/difference using numpy broadcasting\n        sum_idxs = np.add.outer(ones_A, ones_B).ravel()\n        dif_idxs = np.subtract.outer(ones_A, ones_B).ravel()\n        num_sums = np.unique(sum_idxs).size\n        unique_diffs = np.unique(dif_idxs)\n        if unique_diffs.size == 0:\n            ratio = -1.0\n        else:\n            ratio = num_sums / unique_diffs.size\n        ratio_cache[key] = ratio\n        return ratio\n>>>>>>> REPLACE\n\nReasoning: replaces Python\u2010level loops and set constructions with pure NumPy C\u2010loops, drastically reducing Python overhead for moderate pair sizes.\n\n---\n\n<<<<<<< SEARCH\n    # FFT\u2010based convolution for larger supports\n    # Use precomputed FFT length to cut down on repeated array allocations\n    fa = np.fft.fft(A_ind.astype(float), FFT_N)\n    fb = np.fft.fft(B_ind.astype(float), FFT_N)\n    n = FFT_N\n    sums_conv = np.rint(np.fft.ifft(fa * fb)).astype(int)\n    num_sums = np.count_nonzero(sums_conv)\n    # use cross\u2010correlation via conjugate to get the difference spectrum\n    diffs_conv = np.rint(np.fft.ifft(fa * np.conj(fb))).astype(int)\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        ratio_cache[key] = -1.0\n        return -1.0\n    ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n=======\n    # FFT\u2010based convolution for larger supports using real FFT for efficiency\n    fa = np.fft.rfft(A_ind.astype(float), FFT_N)\n    fb = np.fft.rfft(B_ind.astype(float), FFT_N)\n    # compute Minkowski sum via convolution\n    sums_conv = np.rint(np.fft.irfft(fa * fb, FFT_N)).astype(int)\n    num_sums = np.count_nonzero(sums_conv)\n    # compute Minkowski difference via cross-correlation\n    diffs_conv = np.rint(np.fft.irfft(fa * np.conj(fb), FFT_N)).astype(int)\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        ratio_cache[key] = -1.0\n        return -1.0\n    ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n>>>>>>> REPLACE\n\nReasoning: switching to `rfft`/`irfft` halves both memory and compute for real\u2010valued inputs, yet yields identical integer counts when rounded."]}}}