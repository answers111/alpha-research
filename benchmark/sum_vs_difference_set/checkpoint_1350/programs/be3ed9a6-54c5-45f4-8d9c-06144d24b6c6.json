{"id": "be3ed9a6-54c5-45f4-8d9c-06144d24b6c6", "code": "import numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # convert boolean arrays to int8 once for faster convolution\n    A_int = A_ind.astype(np.int8, copy=False)\n    B_int = B_ind.astype(np.int8, copy=False)\n    # sumset via convolution\n    sums_conv = np.convolve(A_int, B_int)\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via convolution with reversed B\n    diffs_conv = np.convolve(A_int, B_int[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = np.nonzero(ind)[0]\n    zeros = np.nonzero(~ind)[0]\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        C_ind = current_A.copy()\n        D_ind = current_B.copy()\n        # propose a balanced add/remove move for A or B using helper\n        if rng.random() < 0.5:\n            C_ind = propose_move(C_ind)\n        else:\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: Hybrid Evolutionary\u2013Graph\u2010guided Metaheuristic for Efficient Sum-to-Difference Ratio Maximization\n\nAbstract:\nThe current simulated\u2010annealing\u2010driven hill\u2010climbing approach plateaus at a ratio of \u22481.27 for two N-length indicator vectors, largely due to (i) uniform, memoryless bit\u2010flip proposals that struggle to escape deep local optima, (ii) costly full\u2010length convolutions (O(N\u00b2)) at each move, and (iii) limited population diversity. We propose a novel, implementable framework that overcomes these limitations by integrating three core innovations:\n\n1. Graph Neural Move Proposer:\n   \u2022 Represent the pair (A, B) as a bipartite interaction graph whose nodes correspond to bit positions and edges encode current contributions to sums and differences.\n   \u2022 Train a lightweight graph\u2010convolutional network (GCN) offline on small synthetic instances to predict high\u2010impact bit flips, then fine\u2010tune online via a contextual multi\u2010armed bandit that balances exploration of uncertain positions and exploitation of high\u2010reward moves.\n   \u2022 This yields a nonuniform, history\u2010aware proposal distribution that rapidly navigates challenging basins.\n\n2. Incremental Convolutional Evaluation:\n   \u2022 Replace full re\u2010convolution with incremental update rules: maintain sparse representations of sumset and diffset counts and update them in O(k log N) per flip (k = number of affected auto\u2010correlations), reducing per\u2010move cost by orders of magnitude for moderate sparsity.\n   \u2022 Leverage FFT\u2010based batch updates on GPU (via JAX) when population\u2010level evaluations exceed a threshold, ensuring scalability to N\u226b100.\n\n3. Evolutionary Hybridization with Niching:\n   \u2022 Maintain a small population of candidate pairs, periodically applying multi\u2010parent crossover that preserves high\u2010order indicator patterns identified by the GCN.\n   \u2022 Introduce adaptive niching: cluster current elites in Hamming space and allocate search budgets to underexplored niches, thereby maintaining diversity and preventing premature convergence.\n\nTo validate the approach, we will:\n(i) implement the framework in PyTorch and JAX, interfacing for incremental updates and GPU FFT;\n(ii) benchmark against standard hill\u2010climbing, tabu search, genetic algorithms, and the existing SA baseline on N=30, 50, 100, 200;\n(iii) perform ablations isolating the GCN proposer, incremental evaluation, and niching mechanisms; and\n(iv) analyze trade\u2010offs between runtime, memory footprint, and achieved ratios.\n\nWe anticipate that our hybrid metaheuristic will consistently exceed the 1.27 threshold, scale efficiently to large N, and generalize to a broad class of additive\u2010combinatorics ratio\u2010maximization problems."], "idea_reward": 6.25, "parent_id": "455caa34-6dcd-4c60-8754-329c98f2528f", "generation": 9, "timestamp": 1757787679.558886, "iteration_found": 615, "metrics": {"score": 1.3846153846153846}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 2 lines with 2 lines\nChange 2: Replace 3 lines with 3 lines", "parent_metrics": {"score": 1.2727272727272727}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: Adaptive Metaheuristic Framework for Maximizing Sum\u2010to\u2010Difference Ratios in Finite Indicator Sets\n\nAbstract:\nWe propose an advanced search methodology to overcome the stagnation and low convergence rate observed in the current hill\u2010climbing approach (score=1.2105) for optimizing the sum\u2010to\u2010difference ratio |A+B|\u2215|A\u2013B| over two binary indicator vectors of length N. Our framework integrates three key innovations: (1) an adaptive simulated\u2010annealing schedule that balances exploration and exploitation through a time\u2010varying temperature, preventing premature convergence; (2) a learned move\u2010proposal network, trained online using multi\u2010armed bandit principles, to prioritize bit\u2010flip candidates that historically yield the greatest ratio improvements; and (3) FFT\u2010accelerated convolution routines that reduce per\u2010evaluation complexity from O(N\u00b2) to O(N log N), enabling scalable searches for N\u226b30. \n\nWe further introduce a restarts\u2010with\u2010elite\u2010seeding mechanism: high\u2010scoring solutions are clustered and used to initialize new runs, thus preserving diversity and concentrating efforts on promising regions of the search space. To validate our approach, we will (i) benchmark against standard hill\u2010climbing, tabu search, and genetic\u2010algorithm baselines on N=30\u2026100; (ii) perform ablation studies isolating the effects of annealing, learned proposals, and FFT acceleration; and (iii) analyze the trade\u2010off between runtime and achieved ratio. We anticipate that our metaheuristic will consistently surpass the current best\u2010known ratio and demonstrate robust scalability, providing a practical and generalizable tool for ratio\u2010maximization problems in additive combinatorics.\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # convert boolean arrays to int8 once for faster convolution\n    A_int = A_ind.view(np.int8)\n    B_int = B_ind.view(np.int8)\n    # sumset via convolution\n    sums_conv = np.convolve(A_int, B_int)\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via convolution with reversed B\n    diffs_conv = np.convolve(A_int, B_int[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = np.nonzero(ind)[0]\n    zeros = np.nonzero(~ind)[0]\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n        # propose a balanced add/remove move for A or B using helper\n        if rng.random() < 0.5:\n            C_ind = propose_move(C_ind)\n        else:\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.2727\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.4828\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5000\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5000)\n\n\n### Program 2 (Score: 1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check on non-empty sets\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.0000)\n\n\n### Program 3 (Score: 0.9697)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (0.9697)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: -1.0000)\n```python\nimport numpy as np\n# For reproducible randomness, use numpy's Generator\nrng = np.random.default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n### Program D2 (Score: 1.3214)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nTitle: Hybrid Evolutionary\u2013Graph\u2010guided Metaheuristic for Efficient Sum-to-Difference Ratio Maximization\n\nAbstract:\nThe current simulated\u2010annealing\u2010driven hill\u2010climbing approach plateaus at a ratio of \u22481.27 for two N-length indicator vectors, largely due to (i) uniform, memoryless bit\u2010flip proposals that struggle to escape deep local optima, (ii) costly full\u2010length convolutions (O(N\u00b2)) at each move, and (iii) limited population diversity. We propose a novel, implementable framework that overcomes these limitations by integrating three core innovations:\n\n1. Graph Neural Move Proposer:\n   \u2022 Represent the pair (A, B) as a bipartite interaction graph whose nodes correspond to bit positions and edges encode current contributions to sums and differences.\n   \u2022 Train a lightweight graph\u2010convolutional network (GCN) offline on small synthetic instances to predict high\u2010impact bit flips, then fine\u2010tune online via a contextual multi\u2010armed bandit that balances exploration of uncertain positions and exploitation of high\u2010reward moves.\n   \u2022 This yields a nonuniform, history\u2010aware proposal distribution that rapidly navigates challenging basins.\n\n2. Incremental Convolutional Evaluation:\n   \u2022 Replace full re\u2010convolution with incremental update rules: maintain sparse representations of sumset and diffset counts and update them in O(k log N) per flip (k = number of affected auto\u2010correlations), reducing per\u2010move cost by orders of magnitude for moderate sparsity.\n   \u2022 Leverage FFT\u2010based batch updates on GPU (via JAX) when population\u2010level evaluations exceed a threshold, ensuring scalability to N\u226b100.\n\n3. Evolutionary Hybridization with Niching:\n   \u2022 Maintain a small population of candidate pairs, periodically applying multi\u2010parent crossover that preserves high\u2010order indicator patterns identified by the GCN.\n   \u2022 Introduce adaptive niching: cluster current elites in Hamming space and allocate search budgets to underexplored niches, thereby maintaining diversity and preventing premature conv...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["I focused on two targeted fixes: correcting the SA\u2010proposal bug (you were always proposing from the global best instead of the current state), and replacing the potentially unsafe/view\u2010based cast with `astype(..., copy=False)` for slightly faster, clearer integer conversion in the tight convolution path.\n\n<<<<<<< SEARCH\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n=======\n        C_ind = current_A.copy()\n        D_ind = current_B.copy()\n>>>>>>> REPLACE\n\nReasoning: You want your SA proposals to be based on the *current* solution, not the best\u2010so\u2010far, otherwise you never actually explore down the current trajectory.\n\n---\n\n<<<<<<< SEARCH\n    # convert boolean arrays to int8 once for faster convolution\n    A_int = A_ind.view(np.int8)\n    B_int = B_ind.view(np.int8)\n=======\n    # convert boolean arrays to int8 once for faster convolution\n    A_int = A_ind.astype(np.int8, copy=False)\n    B_int = B_ind.astype(np.int8, copy=False)\n>>>>>>> REPLACE\n\nReasoning: Using `astype(..., copy=False)` is clearer and avoids any unexpected aliasing issues that `.view()` can introduce, while ensuring you still get int8 data without an unnecessary copy when possible."]}}}