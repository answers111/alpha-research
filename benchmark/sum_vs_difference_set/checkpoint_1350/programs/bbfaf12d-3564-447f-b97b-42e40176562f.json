{"id": "bbfaf12d-3564-447f-b97b-42e40176562f", "code": "import numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Cache to avoid redundant ratio computations\n_ratio_cache = {}\n# Prebind convolution for faster repeated calls\nconv = np.convolve\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # Check cache first\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in _ratio_cache:\n        return _ratio_cache[key]\n\n    # quick feasibility check\n    if not A_ind.any() or not B_ind.any():\n        _ratio_cache[key] = -1.0\n        return -1.0\n\n    # convert boolean arrays to int8 once for faster convolution\n    A_int = A_ind.astype(np.int8, copy=False)\n    B_int = B_ind.astype(np.int8, copy=False)\n\n    # sumset via convolution\n    sums_conv = conv(A_int, B_int)\n    num_sums = np.count_nonzero(sums_conv)\n\n    # diffset via convolution with reversed B\n    diffs_conv = conv(A_int, B_int[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n\n    if num_diffs == 0:\n        _ratio_cache[key] = -1.0\n        return -1.0\n\n    ratio = num_sums / num_diffs\n    _ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = np.nonzero(ind)[0]\n    zeros = np.nonzero(~ind)[0]\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        # propose a balanced add/remove move for A or B, minimize full\u2010array copies\n        if rng.random() < 0.5:\n            C_ind = current_A.copy()\n            C_ind = propose_move(C_ind)\n            D_ind = current_B\n        else:\n            C_ind = current_A\n            D_ind = current_B.copy()\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: Surrogate-Assisted Graph-Guided Reinforcement Metaheuristic for High-Ratio Sum-to-Difference Optimization\n\nAbstract:\nCurrent hill-climbing and simulated\u2010annealing heuristics for maximizing the sum-to-difference ratio of two N-length indicator vectors have recently reached a plateau near 1.38, constrained by uniform bit flips, costly full convolutions, and limited exploration of correlated move patterns. We propose a novel, fully implementable framework that overcomes these bottlenecks through three synergistic advances:\n\n1. Graph-Attention Move Policy with Meta-Reinforcement Learning  \n   \u2022 Model each candidate pair (A, B) as a complete bipartite graph enriched with node\u2010level auto-correlation and cross-correlation scores.  \n   \u2022 Pretrain a lightweight Graph Attention Network (GAT) on small synthetic instances to predict promising multi-bit flip regions, then fine-tune online via a meta-reinforcement learner that updates its policy using variance-reduced policy gradients.  \n   \u2022 This yields a non\u2010uniform, context\u2010aware proposal distribution capable of escaping deep local optima through coordinated k-bit moves.\n\n2. Surrogate-Assisted Incremental Evaluation  \n   \u2022 Maintain a sparse, dynamically updated hash table of recent (move, ratio) pairs to fit a fast neural surrogate (e.g., a small MLP).  \n   \u2022 Use the surrogate to pre-screen candidate moves in O(1) time, invoking exact incremental convolution updates\u2014O(k\u2009log\u2009N) per flip\u2014for only the top-K proposals.  \n   \u2022 Batch remaining evaluations via FFT on GPU using JAX when the cache warms up, reducing overall convolution costs by up to 90%.\n\n3. Hierarchical Coevolution with Adaptive Resource Allocation  \n   \u2022 Organize populations into two tiers: a \u201cmacro\u201d level exploring global pattern recombination via multi-parent crossover, and a \u201cmicro\u201d level refining local improvements with GAT-guided flips.  \n   \u2022 Dynamically reallocate computational budget between tiers based on online diversity and convergence metrics, preventing premature fixation.\n\nWe will implement the framework in PyTorch and JAX, benchmark against standard hill-climbing, tabu search, genetic algorithms, and the existing SA baseline for N up to 400, and perform systematic ablations. We anticipate consistent ratio gains beyond 1.5 for small N and scalable improvements (>1.42) for large N, coupled with substantial runtime reductions."], "idea_reward": 6.0, "parent_id": "be3ed9a6-54c5-45f4-8d9c-06144d24b6c6", "generation": 10, "timestamp": 1757789335.156274, "iteration_found": 665, "metrics": {"score": 1.3846153846153846}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 2 lines with 7 lines\nChange 2: Replace 17 lines with 31 lines\nChange 3: Replace 7 lines with 9 lines", "parent_metrics": {"score": 1.3846153846153846}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: Hybrid Evolutionary\u2013Graph\u2010guided Metaheuristic for Efficient Sum-to-Difference Ratio Maximization\n\nAbstract:\nThe current simulated\u2010annealing\u2010driven hill\u2010climbing approach plateaus at a ratio of \u22481.27 for two N-length indicator vectors, largely due to (i) uniform, memoryless bit\u2010flip proposals that struggle to escape deep local optima, (ii) costly full\u2010length convolutions (O(N\u00b2)) at each move, and (iii) limited population diversity. We propose a novel, implementable framework that overcomes these limitations by integrating three core innovations:\n\n1. Graph Neural Move Proposer:\n   \u2022 Represent the pair (A, B) as a bipartite interaction graph whose nodes correspond to bit positions and edges encode current contributions to sums and differences.\n   \u2022 Train a lightweight graph\u2010convolutional network (GCN) offline on small synthetic instances to predict high\u2010impact bit flips, then fine\u2010tune online via a contextual multi\u2010armed bandit that balances exploration of uncertain positions and exploitation of high\u2010reward moves.\n   \u2022 This yields a nonuniform, history\u2010aware proposal distribution that rapidly navigates challenging basins.\n\n2. Incremental Convolutional Evaluation:\n   \u2022 Replace full re\u2010convolution with incremental update rules: maintain sparse representations of sumset and diffset counts and update them in O(k log N) per flip (k = number of affected auto\u2010correlations), reducing per\u2010move cost by orders of magnitude for moderate sparsity.\n   \u2022 Leverage FFT\u2010based batch updates on GPU (via JAX) when population\u2010level evaluations exceed a threshold, ensuring scalability to N\u226b100.\n\n3. Evolutionary Hybridization with Niching:\n   \u2022 Maintain a small population of candidate pairs, periodically applying multi\u2010parent crossover that preserves high\u2010order indicator patterns identified by the GCN.\n   \u2022 Introduce adaptive niching: cluster current elites in Hamming space and allocate search budgets to underexplored niches, thereby maintaining diversity and preventing premature conv...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # convert boolean arrays to int8 once for faster convolution\n    A_int = A_ind.astype(np.int8, copy=False)\n    B_int = B_ind.astype(np.int8, copy=False)\n    # sumset via convolution\n    sums_conv = np.convolve(A_int, B_int)\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via convolution with reversed B\n    diffs_conv = np.convolve(A_int, B_int[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = np.nonzero(ind)[0]\n    zeros = np.nonzero(~ind)[0]\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        C_ind = current_A.copy()\n        D_ind = current_B.copy()\n        # propose a balanced add/remove move for A or B using helper\n        if rng.random() < 0.5:\n            C_ind = propose_move(C_ind)\n        else:\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.3846\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5161\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5161)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5161)\n\n\n### Program 2 (Score: 1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check on non-empty sets\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.0000)\n\n\n### Program 3 (Score: 1.4500)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n# Cache to avoid redundant ratio computations\n_ratio_cache = {}\n# Prebind convolution removed; replaced by set\u2010based operations for efficiency\n# convolve = np.convolve\n\n# For reproducibility\nrng = default_rng(42)\n\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.4500)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.2692)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nfrom scipy.signal import fftconvolve\n\n# For reproducibility\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: -1.0000)\n```python\nimport numpy as np\n# Use numpy default_rng for reproducibility and better performance\nrng = np.random.default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n# Current Proposal\nTitle: Surrogate-Assisted Graph-Guided Reinforcement Metaheuristic for High-Ratio Sum-to-Difference Optimization\n\nAbstract:\nCurrent hill-climbing and simulated\u2010annealing heuristics for maximizing the sum-to-difference ratio of two N-length indicator vectors have recently reached a plateau near 1.38, constrained by uniform bit flips, costly full convolutions, and limited exploration of correlated move patterns. We propose a novel, fully implementable framework that overcomes these bottlenecks through three synergistic advances:\n\n1. Graph-Attention Move Policy with Meta-Reinforcement Learning  \n   \u2022 Model each candidate pair (A, B) as a complete bipartite graph enriched with node\u2010level auto-correlation and cross-correlation scores.  \n   \u2022 Pretrain a lightweight Graph Attention Network (GAT) on small synthetic instances to predict promising multi-bit flip regions, then fine-tune online via a meta-reinforcement learner that updates its policy using variance-reduced policy gradients.  \n   \u2022 This yields a non\u2010uniform, context\u2010aware proposal distribution capable of escaping deep local optima through coordinated k-bit moves.\n\n2. Surrogate-Assisted Incremental Evaluation  \n   \u2022 Maintain a sparse, dynamically updated hash table of recent (move, ratio) pairs to fit a fast neural surrogate (e.g., a small MLP).  \n   \u2022 Use the surrogate to pre-screen candidate moves in O(1) time, invoking exact incremental convolution updates\u2014O(k\u2009log\u2009N) per flip\u2014for only the top-K proposals.  \n   \u2022 Batch remaining evaluations via FFT on GPU using JAX when the cache warms up, reducing overall convolution costs by up to 90%.\n\n3. Hierarchical Coevolution with Adaptive Resource Allocation  \n   \u2022 Organize populations into two tiers: a \u201cmacro\u201d level exploring global pattern recombination via multi-parent crossover, and a \u201cmicro\u201d level refining local improvements with GAT-guided flips.  \n   \u2022 Dynamically reallocate computational budget between tiers based on online diversity and convergence metrics, pre...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are a few targeted edits to (1) add a simple on\u2010the\u2010fly cache for `compute_ratio` to avoid recomputing the same convolution over and over, (2) prebind `np.convolve` to a local name, and (3) reduce unnecessary full\u2010array copies in the inner loop. Together these should reduce per\u2010iteration overhead without changing the overall logic.\n\n1) After the RNG definition, add your cache and a prebound conv alias:\n\n<<<<<<< SEARCH\n# For reproducibility\nrng = default_rng(42)\n=======\n# For reproducibility\nrng = default_rng(42)\n\n# Cache to avoid redundant ratio computations\n_ratio_cache = {}\n# Prebind convolution for faster repeated calls\nconv = np.convolve\n>>>>>>> REPLACE\n\n2) Inside `compute_ratio`, bake in a byte\u2010level cache lookup and store:\n\n<<<<<<< SEARCH\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # convert boolean arrays to int8 once for faster convolution\n    A_int = A_ind.astype(np.int8, copy=False)\n    B_int = B_ind.astype(np.int8, copy=False)\n    # sumset via convolution\n    sums_conv = np.convolve(A_int, B_int)\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via convolution with reversed B\n    diffs_conv = np.convolve(A_int, B_int[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n=======\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # Check cache first\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in _ratio_cache:\n        return _ratio_cache[key]\n\n    # quick feasibility check\n    if not A_ind.any() or not B_ind.any():\n        _ratio_cache[key] = -1.0\n        return -1.0\n\n    # convert boolean arrays to int8 once for faster convolution\n    A_int = A_ind.astype(np.int8, copy=False)\n    B_int = B_ind.astype(np.int8, copy=False)\n\n    # sumset via convolution\n    sums_conv = conv(A_int, B_int)\n    num_sums = np.count_nonzero(sums_conv)\n\n    # diffset via convolution with reversed B\n    diffs_conv = conv(A_int, B_int[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n\n    if num_diffs == 0:\n        _ratio_cache[key] = -1.0\n        return -1.0\n\n    ratio = num_sums / num_diffs\n    _ratio_cache[key] = ratio\n    return ratio\n>>>>>>> REPLACE\n\n3) In the main simulated\u2010annealing loop, only copy the array you actually mutate:\n\n<<<<<<< SEARCH\n        C_ind = current_A.copy()\n        D_ind = current_B.copy()\n        # propose a balanced add/remove move for A or B using helper\n        if rng.random() < 0.5:\n            C_ind = propose_move(C_ind)\n        else:\n            D_ind = propose_move(D_ind)\n=======\n        # propose a balanced add/remove move for A or B, minimize full\u2010array copies\n        if rng.random() < 0.5:\n            C_ind = current_A.copy()\n            C_ind = propose_move(C_ind)\n            D_ind = current_B\n        else:\n            C_ind = current_A\n            D_ind = current_B.copy()\n            D_ind = propose_move(D_ind)\n>>>>>>> REPLACE\n\nReasoning:\n- A tiny cache prevents repeating the same convolution/`tobytes` work when a previous (A,B) reoccurs.\n- Prebinding `conv` saves a couple attribute lookups per call.\n- Only copying the array you actually change halves the number of temporary arrays created each iteration.\n\nTogether these should drop a few microseconds per move in the inner loop and help push your metrics up."]}}}