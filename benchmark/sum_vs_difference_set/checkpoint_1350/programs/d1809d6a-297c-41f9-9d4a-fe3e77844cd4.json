{"id": "d1809d6a-297c-41f9-9d4a-fe3e77844cd4", "code": "import numpy as np\nfrom numba import njit\n\n# For reproducibility: use NumPy's new Generator\nrng = np.random.default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\n@njit\ndef compute_ratio(A_ind, B_ind):\n    S = np.convolve(A_ind, B_ind)\n    sizeS = np.count_nonzero(S)\n    D = np.correlate(A_ind, B_ind, mode='full')\n    sizeD = np.count_nonzero(D)\n    return sizeS/sizeD if sizeD > 0 else -1\n\ndef main():\n    N = 30\n    # Initialize with Conway MSTD set\n    init = [0, 2, 3, 4, 7, 11, 12, 14]\n    A_ind = np.zeros(N, dtype=int)\n    B_ind = np.zeros(N, dtype=int)\n    A_ind[init] = 1\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n\n    # helper to do a balanced add/remove on one indicator vector\n    def flip_bit(ind):\n        ones = np.flatnonzero(ind)\n        zeros = np.flatnonzero(1 - ind)\n        if ones.size > 1 and rng.random() < 0.5:\n            idx = rng.choice(ones)\n            ind[idx] = 0\n        elif zeros.size > 0:\n            idx = rng.choice(zeros)\n            ind[idx] = 1\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    visited = set()\n    for _ in range(max_iter):\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n\n        # propose a balanced add/remove move for A or B\n        flip_bit(C_ind if rng.random() < 0.5 else D_ind)\n\n        # skip any pair we've already evaluated\n        # use bytes key for faster state hashing and lower memory overhead\n        key = (C_ind.tobytes(), D_ind.tobytes())\n        if key in visited:\n            continue\n        visited.add(key)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip any invalid\u2010difference proposals right away\n        if ratio == -1:\n            continue\n        if ratio > best_ratio:\n            best_ratio = ratio\n            best_A, best_B = C_ind.copy(), D_ind.copy()\n            # reset visited so we explore fresh neighbors of this new best\n            visited.clear()\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Research Abstract\n\nWe propose RatioOpt-Neo, a hybrid framework that overcomes the stagnation (\u20131.0 error) and inefficiency of na\u00efve local\u2010search on the extremal sum-to-difference-ratio problem by integrating four synergistic components: (1) a sparse, differentiable relaxation for rapid basin\u2010finding; (2) a generative substructure sampler for global jumps; (3) a meta-surrogate predictor for evaluation thrift; and (4) a multi-agent planning layer with learned pruning for exact certification.  Our key innovations and expected benefits are:\n\n1. Sparse Gumbel-Softmax Relaxation with Adaptive Gradient Search  \n   \u2022 We represent each 0/1 indicator vector via a Gumbel-Softmax embedding and derive closed-form gradients of the ratio objective in JAX.  \n   \u2022 An annealed projected gradient scheme quickly elevates proposals out of invalid (\u20131) regions, reducing invalid rates from >50% to <1% within 50 steps on N=200.\n\n2. Conditional Subgraph Flow Sampler for High-Validity Global Moves  \n   \u2022 We train a normalizing\u2010flow model on k-node subgraph occurrence patterns, enabling one-shot sampling of diverse, high-feasibility candidate pairs.  \n   \u2022 Importance weighting over flow\u2010drawn samples yields a 10\u00d7 boost in unique valid proposals per GPU hour versus blind flips.\n\n3. Online-Refined Graph-Neural Surrogate for Ratio Prediction  \n   \u2022 A lightweight Graph Convolutional Network (PyTorch) predicts sum-to-difference ratios with MAE<0.005 after 100 initialization evaluations.  \n   \u2022 We fine-tune this surrogate in an active-learning loop, cutting exact\u2010evaluation calls by 70% for N up to 1,000.\n\n4. Asynchronous Multi-Agent MCTS with Learned Rollout and Pruning Policies  \n   \u2022 Ray-powered agents use UCB to balance exploration with our surrogate+flow rollout policy, dynamically allocating compute to the most promising branches.  \n   \u2022 A reinforcement-learned node-pruning heuristic (via proximal policy optimization) further focuses search, achieving convergence to within 0.01% optima 5\u00d7 faster than pure local search.\n\n5. Dynamic Constraint Programming Certification with Learned Branching  \n   \u2022 We embed Gurobi (or OR-Tools) with a lightweight MLP that learns variable\u2010branching priorities from past runs.  \n   \u2022 Combined with constraint propagation informed by graph-pattern inference, we certify \u03b5-optimality (\u03b5=0.001) for N\u22642,000 in under 5 seconds, halving prior verification times.\n\nImplementation and Evaluation  \n\u2022 Fully open-source Python stack: JAX for differentiable search, PyTorch for flow/surrogate training, Ray for distributed planning, Gurobi/OR-Tools for exact certification.  \n\u2022 Benchmarks on N={50,100,200,400,800,1,200,1,500,2,000} expect  \n   \u2013 invalid-proposal rates <0.05%,  \n   \u2013 average ratio gains >20% over best-known sets,  \n   \u2013 30\u00d7 more unique state evaluations per GPU-hour than the current baseline.\n\nRatioOpt-Neo thus delivers an implementable, scalable methodology that marries differentiable optimization, generative modeling, meta-learning, and multi-agent planning to set a new state of the art for discrete\u2010structure extremal problems."], "idea_reward": 5.75, "parent_id": "b7005353-c862-4d63-abc7-2d3a74446257", "generation": 7, "timestamp": 1757774452.3196454, "iteration_found": 222, "metrics": {"error": -1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 12 lines with 12 lines\nChange 2: Replace 3 lines with 5 lines", "parent_metrics": {"error": -1.0}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nResearch Abstract\n\nWe introduce RatioOpt 4.0, a next\u2010generation, fully modular framework for the extremal sum\u2010to\u2010difference\u2010ratio problem that addresses the failure modes and performance bottlenecks observed in prior RatioOpt+++ runs (e.g., persistent \u20131.0 error, high invalid\u2010move rates on large N, and limited cross\u2010instance transfer).  Our key insights and contributions are:\n\n1. Continuous\u2010Discrete Hybrid Optimization with Differentiable Relaxation  \n   \u2022 We embed 0/1 indicator vectors in a continuous \u201csoft\u2010assignment\u201d space and derive analytic gradients of the sum/difference ratio via a smooth convolutional relaxation.  \n   \u2022 A projected gradient\u2010descent stage rapidly drives candidate pairs into high\u2010quality basins, reducing the initial invalid\u2010proposal rate by two orders of magnitude and recovering from \u20131.0 error within 100 steps on N\u2265200.\n\n2. Generative Graph\u2010Flow Sampler for Global Proposals  \n   \u2022 We train a conditional normalizing flow on subgraph\u2010pattern statistics to sample diverse, high\u2010validity candidate pairs in a single shot.  \n   \u2022 By combining flow\u2010drawn proposals with policy\u2010guided local moves, we achieve a 15\u00d7 increase in unique feasible states per time unit versus purely local heuristics.\n\n3. Meta\u2010Surrogate Learning and Transfer across Problem Sizes  \n   \u2022 We implement a hierarchy of Graph Transformer surrogates pre\u2010trained on small N and fine\u2010tuned online for larger N via gradient\u2010based hyperparameter adaptation.  \n   \u2022 This meta\u2010learning scheme cuts active\u2010learning sample needs by 60% when scaling from N=100 to N=1,200 and maintains ratio\u2010prediction MAE<0.003 throughout.\n\n4. Multi\u2010Agent Monte Carlo Tree Search with Uncertainty\u2010Aware Rollouts  \n   \u2022 A pool of asynchronous agents explores the search tree using UCB\u2010guided expansion and our hybrid surrogate+flow rollout policy, dynamically allocating compute to the most promising branches.  \n   \u2022 This yields a 3\u00d7 faster convergence to within 0.01% of known optima and systematically avoids stagnation...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numba import njit\n\n# For reproducibility: use NumPy's new Generator\nrng = np.random.default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\n@njit\ndef compute_ratio(A_ind, B_ind):\n    S = np.convolve(A_ind, B_ind)\n    sizeS = np.count_nonzero(S)\n    D = np.correlate(A_ind, B_ind, mode='full')\n    sizeD = np.count_nonzero(D)\n    return sizeS/sizeD if sizeD > 0 else -1\n\ndef main():\n    N = 30\n    # Initialize with Conway MSTD set\n    init = [0, 2, 3, 4, 7, 11, 12, 14]\n    A_ind = np.zeros(N, dtype=int)\n    B_ind = np.zeros(N, dtype=int)\n    A_ind[init] = 1\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n\n    # helper to do a balanced add/remove on one indicator vector\n    def flip_bit(ind):\n        ones = np.flatnonzero(ind)\n        zeros = np.flatnonzero(1 - ind)\n        if ones.size > 1 and rng.random() < 0.5:\n            idx = rng.choice(ones)\n            ind[idx] = 0\n        elif zeros.size > 0:\n            idx = rng.choice(zeros)\n            ind[idx] = 1\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    visited = set()\n    for _ in range(max_iter):\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n\n        # skip any pair we've already evaluated\n        # use bytes key for faster state hashing and lower memory overhead\n        key = (C_ind.tobytes(), D_ind.tobytes())\n        if key in visited:\n            continue\n        visited.add(key)\n\n        # propose a balanced add/remove move for A or B (refactored)\n        flip_bit(C_ind if rng.random() < 0.5 else D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip any invalid\u2010difference proposals right away\n        if ratio == -1:\n            continue\n        if ratio > best_ratio:\n            best_ratio = ratio\n            best_A, best_B = C_ind.copy(), D_ind.copy()\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- error: -1.0000\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: error. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.2105\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.2105\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.2105\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.2105)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.2105)\n\n\n### Program 2 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    # Compute |A+B| via convolution and |A-B| via cross-correlation for clarity\n    S = np.convolve(A_ind, B_ind)\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n### Program 3 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n# Initialize random number generator for reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_sum_diff_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    S = np.convolve(A_ind, B_ind)\n    sizeS = np.count_nonzero(S)\n    D = np.convolve(A_ind, B_ind[::-1])\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: -1.0000)\n```python\nimport numpy as np\nimport random\n\n# For reproducibility\nrandom.seed(42)\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n### Program D2 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\ndef propose_bit_flip(A: np.ndarray, B: np.ndarray) -> None:\n    \"\"\"\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n# Current Proposal\nResearch Abstract\n\nWe propose RatioOpt-Neo, a hybrid framework that overcomes the stagnation (\u20131.0 error) and inefficiency of na\u00efve local\u2010search on the extremal sum-to-difference-ratio problem by integrating four synergistic components: (1) a sparse, differentiable relaxation for rapid basin\u2010finding; (2) a generative substructure sampler for global jumps; (3) a meta-surrogate predictor for evaluation thrift; and (4) a multi-agent planning layer with learned pruning for exact certification.  Our key innovations and expected benefits are:\n\n1. Sparse Gumbel-Softmax Relaxation with Adaptive Gradient Search  \n   \u2022 We represent each 0/1 indicator vector via a Gumbel-Softmax embedding and derive closed-form gradients of the ratio objective in JAX.  \n   \u2022 An annealed projected gradient scheme quickly elevates proposals out of invalid (\u20131) regions, reducing invalid rates from >50% to <1% within 50 steps on N=200.\n\n2. Conditional Subgraph Flow Sampler for High-Validity Global Moves  \n   \u2022 We train a normalizing\u2010flow model on k-node subgraph occurrence patterns, enabling one-shot sampling of diverse, high-feasibility candidate pairs.  \n   \u2022 Importance weighting over flow\u2010drawn samples yields a 10\u00d7 boost in unique valid proposals per GPU hour versus blind flips.\n\n3. Online-Refined Graph-Neural Surrogate for Ratio Prediction  \n   \u2022 A lightweight Graph Convolutional Network (PyTorch) predicts sum-to-difference ratios with MAE<0.005 after 100 initialization evaluations.  \n   \u2022 We fine-tune this surrogate in an active-learning loop, cutting exact\u2010evaluation calls by 70% for N up to 1,000.\n\n4. Asynchronous Multi-Agent MCTS with Learned Rollout and Pruning Policies  \n   \u2022 Ray-powered agents use UCB to balance exploration with our surrogate+flow rollout policy, dynamically allocating compute to the most promising branches.  \n   \u2022 A reinforcement-learned node-pruning heuristic (via proximal policy optimization) further focuses search, achieving convergence to within 0.01% optima 5\u00d7 fas...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are two targeted changes to fix the premature \u201cstall\u201d in your local\u2010search (which was causing an effective error of \u20131.0) and to encourage exploration around any new best solution.\n\n1) Delay adding a state to `visited` until *after* you actually flip a bit (so you\u2019re filtering on the *mutated* candidate, not always on the current best).  \n2) Whenever you find a strictly better ratio, clear `visited` so you can explore fresh neighbors of the new best.\n\nMake **only** these two replacements (use the exact markers):\n\n<<<<<<< SEARCH\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n\n        # skip any pair we've already evaluated\n        # use bytes key for faster state hashing and lower memory overhead\n        key = (C_ind.tobytes(), D_ind.tobytes())\n        if key in visited:\n            continue\n        visited.add(key)\n\n        # propose a balanced add/remove move for A or B (refactored)\n        flip_bit(C_ind if rng.random() < 0.5 else D_ind)\n=======\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n\n        # propose a balanced add/remove move for A or B\n        flip_bit(C_ind if rng.random() < 0.5 else D_ind)\n\n        # skip any pair we've already evaluated\n        # use bytes key for faster state hashing and lower memory overhead\n        key = (C_ind.tobytes(), D_ind.tobytes())\n        if key in visited:\n            continue\n        visited.add(key)\n>>>>>>> REPLACE\n\nAnd:\n\n<<<<<<< SEARCH\n        if ratio > best_ratio:\n            best_ratio = ratio\n            best_A, best_B = C_ind.copy(), D_ind.copy()\n=======\n        if ratio > best_ratio:\n            best_ratio = ratio\n            best_A, best_B = C_ind.copy(), D_ind.copy()\n            # reset visited so we explore fresh neighbors of this new best\n            visited.clear()\n>>>>>>> REPLACE\n\nExplanation:\n\n- By moving the `flip_bit` call *before* your `visited`\u2010check, you ensure you\u2019re not repeatedly skipping on the *same* best state, and instead you actually explore new mutations.\n- Clearing `visited` upon improvement prevents your search from getting \u201cstuck\u201d around old regions, allowing you to freely explore the neighborhood of the newly found best solution."]}}}