{"id": "55647cad-c827-4633-9a93-a53817dce102", "code": "import numpy as np\nfrom numpy.random import default_rng\nimport math\nimport numpy.fft as fft_module\n# Pre\u2010bind exponential and initialize a cache for compute_ratio\nexp = math.exp\nratio_cache = {}\n# Pre-bind convolution, correlation, and array operations for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\ncount_nonzero = np.count_nonzero\nround_fn = np.round\n\n# For reproducibility\nrng = default_rng(42)\n# Pre\u2010bind RNG methods to avoid attribute lookups in tight loops\nrng_random = rng.random\nrng_choice = rng.choice\nflatnonzero = np.flatnonzero\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    key = (tuple(flatnonzero(A_ind)), tuple(flatnonzero(B_ind)))\n    if key in ratio_cache:\n        return ratio_cache[key]\n    if not A_ind.any() or not B_ind.any():\n        ratio_cache[key] = -1.0\n        return -1.0\n    A_arr = A_ind.view(np.int8)\n    B_arr = B_ind.view(np.int8)\n    n = A_arr.size\n    if n > 512:\n        # FFT\u2010based convolution/correlation (pre\u2010imported)\n        m = 2*n - 1\n        fsize = fft_module.next_fast_len(m)\n        fa = fft_module.fft(A_arr, fsize)\n        fb = fft_module.fft(B_arr, fsize)\n        conv = fft_module.ifft(fa * fb).real[:m]\n        corr = fft_module.ifft(fa * np.conj(fb)).real[:m]\n        num_sums = count_nonzero(round_fn(conv).astype(int))\n        num_diffs = count_nonzero(round_fn(corr).astype(int))\n    else:\n        # Direct methods for small arrays\n        num_sums = count_nonzero(convolve_fn(A_arr, B_arr))\n        num_diffs = count_nonzero(correlate_fn(A_arr, B_arr, mode='full'))\n    if num_diffs == 0:\n        ratio_cache[key] = -1.0\n        return -1.0\n    ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray, multi_swap_prob: float = 0.1) -> np.ndarray:\n    \"\"\"Perform one or two swap moves to maintain constant cardinality.\"\"\"\n    new_ind = ind.copy()\n    # with small probability, do two swaps instead of one\n    swaps = 2 if rng_random() < multi_swap_prob else 1\n    for _ in range(swaps):\n        ones_idx = rng_choice(flatnonzero(new_ind))\n        zeros_idx = rng_choice(flatnonzero(~new_ind))\n        new_ind[ones_idx] = False\n        new_ind[zeros_idx] = True\n    return new_ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9997        # slower cooling for wider exploration\n\n    # Local search: random single\u2010bit flips\n    max_iter = 40000\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(50):  # batch size = 50\n            # Generate independent proposals for both state vectors\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng.random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate copies to avoid aliasing\n                local_best_A, local_best_B = C_ind.copy(), D_ind.copy()\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng.random() < math.exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: Adaptive Co\u2010evolutionary Surrogate\u2010Assisted Framework for Sum-to-Difference Ratio Maximization in Binary Indicator Pairs\n\nAbstract:  \nWe present ACSA, a novel, implementable algorithm designed to overcome the 1.5455 performance ceiling of current simulated\u2010annealing and hill\u2010climbing methods for maximizing the ratio |A+B|/|A\u2013B| of binary indicator vector pairs. Our analysis of existing approaches uncovers three core limitations:  \n1. Limited structural exploration due to single\u2010swap or local bit moves,  \n2. Rigid, hand-tuned batch sizes and cooling schedules that fail to adapt to search dynamics,  \n3. High computational overhead from repeated full convolution and correlation evaluations.  \n\nTo address these, ACSA integrates four key innovations:  \n1. Multi-resolution Co-evolution: We maintain two interacting populations\u2014coarse\u2010grain individuals employing block\u2010level flips and fine\u2010grain individuals using bit\u2010level swaps. Promising structures migrate between scales, enabling both global jumps and local refinements.  \n2. Adaptive Differential Evolution Operators: Building on DE/rand/1, we generate candidate pairs via vectorized arithmetic recombination and enforce constant cardinality through projection onto the hyperplane of fixed 1\u2010counts, driving long\u2010range exploration without violating feasibility.  \n3. Deep\u2010Kernel Surrogate with Active Learning: We replace Gaussian processes with a lightweight convolutional neural network that predicts ratios from partial convolution features. Uncertainty\u2010guided sampling ensures only the most informative offspring are evaluated exactly, cutting true evaluations by \u224880%.  \n4. Reinforcement\u2010Learning\u2013Driven Resource Allocation: We frame evaluation budgeting and operator selection as a Markov Decision Process and pre\u2010train a policy on small N instances using policy gradients. The learned policy dynamically balances exploration, surrogate retraining, and local intensification.  \n\nTo accelerate exact evaluations, we implement incremental, bit\u2010parallel convolution on GPU, updating only affected lags in O(s) time per s\u2010sized move. ACSA will be coded in PyTorch and Numba, benchmarked up to N=2000. We will conduct ablation studies to isolate the impact of each component. We anticipate ACSA to exceed a ratio of 1.6 under 10^6 true evaluations, establishing a new state\u2010of\u2010the\u2010art for combinatorial ratio optimization in indicator spaces."], "idea_reward": 6.0, "parent_id": "c1f27b76-0c25-4c14-ab5d-31f4d8f2db46", "generation": 18, "timestamp": 1757826642.700955, "iteration_found": 1308, "metrics": {"score": 1.5454545454545454}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 3 lines with 8 lines\nChange 2: Replace rng_choice = rng.choice with 2 lines\nChange 3: 'key = (A_ind.tobytes(), B_ind.tobytes())' to 'key = (tuple(flatnonzero(A_ind)), tuple(flatnonzero(B_ind)))'\nChange 4: Replace 2 lines with 2 lines\nChange 5: Replace 2 lines with 2 lines\nChange 6: 'swaps = 2 if rng.random() < multi_swap_prob else 1' to 'swaps = 2 if rng_random() < multi_swap_prob else 1'\nChange 7: Replace 2 lines with 2 lines", "parent_metrics": {"score": 1.5454545454545454}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: Hybrid Evolutionary\u2013Surrogate Framework for Maximizing Sum\u2010to\u2010Difference Ratios in Indicator Vector Pairs\n\nAbstract:  \nWe propose a novel, implementable framework to overcome the limitations of the current simulated\u2010annealing\u2013hill\u2010climbing approach for optimizing the sum\u2010to\u2010difference ratio |A+B|/|A\u2013B| of 0/1 indicator vector pairs. Our analysis of the existing method (score=1.5455) reveals three primary weaknesses: (1) reliance on single\u2010bit swaps limits exploration of distant basins; (2) static batch sizes and cooling schedules hinder adaptive search; (3) repeated full convolution/correlation evaluations incur high computational cost. To address these, we integrate insights from successful evolutionary strategies and surrogate\u2010accelerated metaheuristics:\n\n1. Problem\u2010Specific Evolutionary Operators  \n   \u2022 Population Initialization: Generate a diverse pool of indicator pairs via randomized cardinality\u2010preserving perturbations of known high\u2010quality seeds (e.g., Conway\u2019s MSTD).  \n   \u2022 Structured Crossover: Exchange contiguous blocks of indices between parents to preserve beneficial sub\u2010structures and accelerate convergence into high\u2010ratio regions.  \n   \u2022 Adaptive Mutation: Introduce multi\u2010point swaps whose count and selection distribution are governed by a success\u2010driven probability model, allowing dynamic intensification and diversification.\n\n2. Surrogate\u2010Guided Candidate Screening  \n   \u2022 Gaussian Process Regression (GPR) surrogate is trained online on computed (A,B,ratio) triples, enabling cheap prediction of the sum\u2010to\u2010difference ratio.  \n   \u2022 In each generation, a large batch of offspring is proposed, but only the top-k candidates by surrogate\u2010predicted ratio proceed to exact convolution/correlation evaluation, reducing expensive FFT/ direct convolution calls by \u226570%.\n\n3. Dynamic Resource Allocation  \n   \u2022 Population and batch sizes self\u2010adjust in response to search progress: early stages favor broad exploration (larger population, larger surrogate batch), ...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\nimport numpy.fft as fft_module\n# Pre\u2010bind exponential and initialize a cache for compute_ratio\nexp = math.exp\nratio_cache = {}\n\n# For reproducibility\nrng = default_rng(42)\n# Pre\u2010bind RNG methods to avoid attribute lookups in tight loops\nrng_random = rng.random\nrng_choice = rng.choice\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in ratio_cache:\n        return ratio_cache[key]\n    if not A_ind.any() or not B_ind.any():\n        ratio_cache[key] = -1.0\n        return -1.0\n    A_arr = A_ind.view(np.int8)\n    B_arr = B_ind.view(np.int8)\n    n = A_arr.size\n    if n > 512:\n        # FFT\u2010based convolution/correlation (pre\u2010imported)\n        m = 2*n - 1\n        fsize = fft_module.next_fast_len(m)\n        fa = fft_module.fft(A_arr, fsize)\n        fb = fft_module.fft(B_arr, fsize)\n        conv = fft_module.ifft(fa * fb).real[:m]\n        corr = fft_module.ifft(fa * np.conj(fb)).real[:m]\n        num_sums = np.count_nonzero(np.round(conv).astype(int))\n        num_diffs = np.count_nonzero(np.round(corr).astype(int))\n    else:\n        # Direct methods for small arrays\n        num_sums = np.count_nonzero(np.convolve(A_arr, B_arr))\n        num_diffs = np.count_nonzero(np.correlate(A_arr, B_arr, mode='full'))\n    if num_diffs == 0:\n        ratio_cache[key] = -1.0\n        return -1.0\n    ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray, multi_swap_prob: float = 0.1) -> np.ndarray:\n    \"\"\"Perform one or two swap moves to maintain constant cardinality.\"\"\"\n    new_ind = ind.copy()\n    # with small probability, do two swaps instead of one\n    swaps = 2 if rng.random() < multi_swap_prob else 1\n    for _ in range(swaps):\n        ones_idx = rng.choice(np.flatnonzero(new_ind))\n        zeros_idx = rng.choice(np.flatnonzero(~new_ind))\n        new_ind[ones_idx] = False\n        new_ind[zeros_idx] = True\n    return new_ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9997        # slower cooling for wider exploration\n\n    # Local search: random single\u2010bit flips\n    max_iter = 40000\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(50):  # batch size = 50\n            # Generate independent proposals for both state vectors\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng.random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate copies to avoid aliasing\n                local_best_A, local_best_B = C_ind.copy(), D_ind.copy()\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng.random() < math.exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.5455\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 2 (Score: 1.2500)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.2500)\n\n\n### Program 3 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5455)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.5312)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nTitle: Adaptive Co\u2010evolutionary Surrogate\u2010Assisted Framework for Sum-to-Difference Ratio Maximization in Binary Indicator Pairs\n\nAbstract:  \nWe present ACSA, a novel, implementable algorithm designed to overcome the 1.5455 performance ceiling of current simulated\u2010annealing and hill\u2010climbing methods for maximizing the ratio |A+B|/|A\u2013B| of binary indicator vector pairs. Our analysis of existing approaches uncovers three core limitations:  \n1. Limited structural exploration due to single\u2010swap or local bit moves,  \n2. Rigid, hand-tuned batch sizes and cooling schedules that fail to adapt to search dynamics,  \n3. High computational overhead from repeated full convolution and correlation evaluations.  \n\nTo address these, ACSA integrates four key innovations:  \n1. Multi-resolution Co-evolution: We maintain two interacting populations\u2014coarse\u2010grain individuals employing block\u2010level flips and fine\u2010grain individuals using bit\u2010level swaps. Promising structures migrate between scales, enabling both global jumps and local refinements.  \n2. Adaptive Differential Evolution Operators: Building on DE/rand/1, we generate candidate pairs via vectorized arithmetic recombination and enforce constant cardinality through projection onto the hyperplane of fixed 1\u2010counts, driving long\u2010range exploration without violating feasibility.  \n3. Deep\u2010Kernel Surrogate with Active Learning: We replace Gaussian processes with a lightweight convolutional neural network that predicts ratios from partial convolution features. Uncertainty\u2010guided sampling ensures only the most informative offspring are evaluated exactly, cutting true evaluations by \u224880%.  \n4. Reinforcement\u2010Learning\u2013Driven Resource Allocation: We frame evaluation budgeting and operator selection as a Markov Decision Process and pre\u2010train a policy on small N instances using policy gradients. The learned policy dynamically balances exploration, surrogate retraining, and local intensification.  \n\nTo accelerate exact evaluations, we implement in...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are targeted changes to reduce Python overhead (attribute lookups, large bytes keys), speed up small\u2010array convolutions, and improve cache key hashing. Each diff is minimal and matches the existing code exactly:\n\n<<<<<<< SEARCH\n# Pre\u2010bind exponential and initialize a cache for compute_ratio\nexp = math.exp\nratio_cache = {}\n=======\n# Pre\u2010bind exponential and initialize a cache for compute_ratio\nexp = math.exp\nratio_cache = {}\n# Pre-bind convolution, correlation, and array operations for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\ncount_nonzero = np.count_nonzero\nround_fn = np.round\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\nrng_choice = rng.choice\n=======\nrng_choice = rng.choice\nflatnonzero = np.flatnonzero\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    key = (A_ind.tobytes(), B_ind.tobytes())\n=======\n    key = (tuple(flatnonzero(A_ind)), tuple(flatnonzero(B_ind)))\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        num_sums = np.count_nonzero(np.round(conv).astype(int))\n        num_diffs = np.count_nonzero(np.round(corr).astype(int))\n=======\n        num_sums = count_nonzero(round_fn(conv).astype(int))\n        num_diffs = count_nonzero(round_fn(corr).astype(int))\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        num_sums = np.count_nonzero(np.convolve(A_arr, B_arr))\n        num_diffs = np.count_nonzero(np.correlate(A_arr, B_arr, mode='full'))\n=======\n        num_sums = count_nonzero(convolve_fn(A_arr, B_arr))\n        num_diffs = count_nonzero(correlate_fn(A_arr, B_arr, mode='full'))\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    swaps = 2 if rng.random() < multi_swap_prob else 1\n=======\n    swaps = 2 if rng_random() < multi_swap_prob else 1\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        ones_idx = rng.choice(np.flatnonzero(new_ind))\n        zeros_idx = rng.choice(np.flatnonzero(~new_ind))\n=======\n        ones_idx = rng_choice(flatnonzero(new_ind))\n        zeros_idx = rng_choice(flatnonzero(~new_ind))\n>>>>>>> REPLACE\n\nExplanation:\n\n- Pre-binding `convolve_fn`, `correlate_fn`, `count_nonzero`, and `round_fn` avoids repeated attribute lookups.\n- Using `tuple(flatnonzero(...))` as cache keys is much faster (and smaller) than serializing full boolean arrays to bytes.\n- Switching to the already\u2010bound `rng_random`, `rng_choice`, and `flatnonzero` speeds up proposal generation in tight loops.\nThese changes should reduce per\u2010iteration overhead and improve overall score."]}}}