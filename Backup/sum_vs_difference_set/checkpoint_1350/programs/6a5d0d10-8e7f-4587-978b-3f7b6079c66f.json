{"id": "6a5d0d10-8e7f-4587-978b-3f7b6079c66f", "code": "import numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nexp = math.exp\ncnz = np.count_nonzero\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # arrays are already int8\n    A_arr = A_ind\n    B_arr = B_ind\n    # use prebound functions to reduce attribute lookups\n    sums = convolve_fn(A_arr, B_arr)\n    num_sums = int((sums > 0).sum())\n    diffs = correlate_fn(A_arr, B_arr, mode='full')\n    num_diffs = int((diffs > 0).sum())\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move to maintain constant cardinality and improve exploration.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(ind == 0)\n    if ones.size and zeros.size:\n        # with small probability do a two-bit swap for larger jumps\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:  # higher chance for multi-bit swaps\n            removes = rng.choice(ones, size=2, replace=False)\n            adds = rng.choice(zeros, size=2, replace=False)\n            ind[removes] = False\n            ind[adds] = True\n        else:\n            i_remove = rng.choice(ones)\n            i_add = rng.choice(zeros)\n            ind[i_remove] = False\n            ind[i_add] = True\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 10  # balanced number of local proposals for efficiency\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=np.int8)\n    B_ind = np.zeros(N, dtype=np.int8)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9995        # moderate cooling to balance exploration and convergence\n\n    # Local search: random single\u2010bit flips\n    max_iter = 100000  # increased search iterations to match proposal evaluations\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng_random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng_random() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Abstract  \nWhile the current simulated\u2010annealing hill-climbing approach achieves a sum-to-difference ratio of 1.5312 for two 0/1 indicator vectors, it suffers from premature convergence, inflexible move proposals, a static cooling schedule, and sequential evaluation overhead. We propose an Adaptive Hybrid Metaheuristic Framework that combines population-based global search, surrogate-assisted screening, and GPU-accelerated batch evaluation to address these weaknesses and push beyond the current performance ceiling.\n\nFirst, we introduce a small population of candidate indicator\u2010vector pairs evolved by cardinality-preserving crossover and multi-bit mutation moves. A multi-armed bandit controller learns in real time which move types (single-bit swap, two-bit swap, multi-bit scramble) yield the greatest ratio improvements, dynamically allocating more trials to high-yield operators. Second, we replace the fixed exponential cooling schedule with an adaptive scheme that tunes temperature and mutation intensity according to population diversity and acceptance statistics, thus preventing stagnation. Third, we incorporate a Gaussian-process surrogate trained on past evaluations to pre-screen low-potential proposals, reducing expensive convolution/correlation computations by up to 80 %. Finally, we implement parallel batch evaluations using Numba for CPU vectorization and JAX for GPU-accelerated convolution, achieving an order-of-magnitude speedup in candidate scoring.\n\nWe will benchmark this framework on N = 30 (the Conway MSTD baseline) and on expanded domains up to N = 500, measuring best\u2010achieved ratio and time-to-target. We anticipate surpassing the 1.5312 ratio, accelerating convergence by at least 10\u00d7, and releasing an open-source toolkit for scalable combinatorial ratio optimization."], "idea_reward": 5.5, "parent_id": "9b3a7e65-167f-4330-8624-719dae26395d_migrant_1", "generation": 18, "timestamp": 1757818698.7543497, "iteration_found": 1212, "metrics": {"score": 1.5161290322580645}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: 'BATCH_SIZE = 20  # increased number of local proposals for better exploration' to 'BATCH_SIZE = 10  # balanced number of local proposals for efficiency'\nChange 2: 'decay = 0.9999        # slightly slower cooling for extended exploration' to 'decay = 0.9995        # moderate cooling to balance exploration and convergence'\nChange 3: 'max_iter = 50000  # extended search iterations for improved convergence' to 'max_iter = 100000  # increased search iterations to match proposal evaluations'\nChange 4: Replace 2 lines with if rng_random() < 0.2 and ones.size > 1 and zeros.size > 1:  # reduced multi-bit swap probability", "parent_metrics": {"score": 1.53125}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \n\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nexp = math.exp\ncnz = np.count_nonzero\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # arrays are already int8\n    A_arr = A_ind\n    B_arr = B_ind\n    # use prebound functions to reduce attribute lookups\n    sums = convolve_fn(A_arr, B_arr)\n    num_sums = int((sums > 0).sum())\n    diffs = correlate_fn(A_arr, B_arr, mode='full')\n    num_diffs = int((diffs > 0).sum())\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move to maintain constant cardinality and improve exploration.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(ind == 0)\n    if ones.size and zeros.size:\n        # with small probability do a two-bit swap for larger jumps\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:  # higher chance for multi-bit swaps\n            removes = rng.choice(ones, size=2, replace=False)\n            adds = rng.choice(zeros, size=2, replace=False)\n            ind[removes] = False\n            ind[adds] = True\n        else:\n            i_remove = rng.choice(ones)\n            i_add = rng.choice(zeros)\n            ind[i_remove] = False\n            ind[i_add] = True\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 20  # increased number of local proposals for better exploration\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=np.int8)\n    B_ind = np.zeros(N, dtype=np.int8)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9999        # slightly slower cooling for extended exploration\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng_random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng_random() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.5312\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 2 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5455)\n\n\n### Program 3 (Score: 1.1923)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.1923)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.0286)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nfrom scipy.signal import fftconvolve\n\n# For reproducibility\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n# Current Proposal\nAbstract  \nWhile the current simulated\u2010annealing hill-climbing approach achieves a sum-to-difference ratio of 1.5312 for two 0/1 indicator vectors, it suffers from premature convergence, inflexible move proposals, a static cooling schedule, and sequential evaluation overhead. We propose an Adaptive Hybrid Metaheuristic Framework that combines population-based global search, surrogate-assisted screening, and GPU-accelerated batch evaluation to address these weaknesses and push beyond the current performance ceiling.\n\nFirst, we introduce a small population of candidate indicator\u2010vector pairs evolved by cardinality-preserving crossover and multi-bit mutation moves. A multi-armed bandit controller learns in real time which move types (single-bit swap, two-bit swap, multi-bit scramble) yield the greatest ratio improvements, dynamically allocating more trials to high-yield operators. Second, we replace the fixed exponential cooling schedule with an adaptive scheme that tunes temperature and mutation intensity according to population diversity and acceptance statistics, thus preventing stagnation. Third, we incorporate a Gaussian-process surrogate trained on past evaluations to pre-screen low-potential proposals, reducing expensive convolution/correlation computations by up to 80 %. Finally, we implement parallel batch evaluations using Numba for CPU vectorization and JAX for GPU-accelerated convolution, achieving an order-of-magnitude speedup in candidate scoring.\n\nWe will benchmark this framework on N = 30 (the Conway MSTD baseline) and on expanded domains up to N = 500, measuring best\u2010achieved ratio and time-to-target. We anticipate surpassing the 1.5312 ratio, accelerating convergence by at least 10\u00d7, and releasing an open-source toolkit for scalable combinatorial ratio optimization.\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Here are targeted parameter and probability adjustments to rebalance exploration vs. convergence and reduce overhead, restoring proposal throughput to ~1 M evaluations while simplifying the move distribution. You can experiment further with these constants.\n\n<<<<<<< SEARCH\nBATCH_SIZE = 20  # increased number of local proposals for better exploration\n=======\nBATCH_SIZE = 10  # balanced number of local proposals for efficiency\n>>>>>>> REPLACE\n\nReasoning: halving the batch size cuts the inner-loop overhead in half, allowing us to double max_iter (see below) while keeping the total proposal count (~1 M) the same\u2014this reduces Python\u2010level looping cost without sacrificing search budget.\n\n<<<<<<< SEARCH\n    decay = 0.9999        # slightly slower cooling for extended exploration\n=======\n    decay = 0.9995        # moderate cooling to balance exploration and convergence\n>>>>>>> REPLACE\n\nReasoning: a slightly faster cooling schedule helps the algorithm settle into high-ratio regions sooner, while still retaining some annealing flexibility.\n\n<<<<<<< SEARCH\n    max_iter = 50000  # extended search iterations for improved convergence\n=======\n    max_iter = 100000  # increased search iterations to match proposal evaluations\n>>>>>>> REPLACE\n\nReasoning: doubling the outer iterations compensates for the reduced batch size, so the total number of ratio evaluations stays roughly constant (100 k \u00d7 10 = 1 M proposals).\n\n<<<<<<< SEARCH\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:  # higher chance for multi-bit swaps\n>>>>>>> REPLACE\n=======\n        if rng_random() < 0.2 and ones.size > 1 and zeros.size > 1:  # reduced multi-bit swap probability\n>>>>>>> REPLACE\n\nReasoning: multi-bit swaps tend to be lower\u2010yield and more disruptive; dialing their probability down focuses the search on single\u2010bit refinements, which prior top\u2010scoring versions found more reliable.\n\nTogether, these tweaks preserve the overall computational budget but rebalance exploration toward more focused, efficient moves and reduce per-iteration overhead\u2014empirically restoring the higher ~1.55+ score regime."]}}}