{"id": "0975eba4-66c3-4dbc-a22f-f0d8413411f8", "code": "import numpy as np\nfrom numpy.random import default_rng\n# Prebind convolution for faster inner-loop calls\nconvolve = np.convolve\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # Precast to int and use prebound convolution\n    A = A_ind.astype(int)\n    B = B_ind.astype(int)\n    # sumset via convolution\n    sums_conv = convolve(A, B)\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via convolution with reversed B\n    diffs_conv = convolve(A, B[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform r-flip on a boolean indicator array for larger jumps\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Randomly flip between 1 and 3 bits to escape local optima,\n    ensuring at least one \u20181\u2019 remains.\n    \"\"\"\n    r = rng.integers(1, 4)  # number of bits to flip\n    positions = rng.choice(len(ind), size=r, replace=False)\n    for pos in positions:\n        ind[pos] = not ind[pos]\n    # ensure at least one '1' remains\n    if not ind.any():\n        ind[rng.choice(positions)] = True\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # Prebind compute_ratio for faster inner\u2010loop calls\n    compute = compute_ratio\n    # cache to memoize ratio computations\n    ratio_cache = {}\n    for _ in range(max_iter):\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n        # propose a balanced add/remove move for A or B using helper\n        if rng.random() < 0.5:\n            C_ind = propose_move(C_ind)\n        else:\n            D_ind = propose_move(D_ind)\n\n        # memoized ratio computation\n        key = (tuple(C_ind), tuple(D_ind))\n        ratio = ratio_cache.get(key)\n        if ratio is None:\n            ratio = compute(C_ind, D_ind)\n            ratio_cache[key] = ratio\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        if ratio > best_ratio:\n            best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: Learning-Guided Multi-Fidelity Block-Recombination for Near-Optimal MSTD Sets\n\nAbstract:\nWe present a fully implementable framework that overcomes the local-optima stagnation, narrow move set, and heavy convolution costs of existing single-bit hill-climbing methods in the search for two 0/1 indicator vectors A, B maximizing |A+B|/|A\u2013B|. Our contributions are four-fold:\n\n1. Reinforcement-Learned Move Policy:\n   \u2022 We frame the search as a Markov decision process: states are (A, B) pairs and actions include multi\u2010bit block swaps, r-flips, and segment recombinations derived from known MSTD templates.  \n   \u2022 A lightweight Proximal Policy Optimization (PPO) agent observes state embeddings\u2014constructed via a small Graph Neural Network that encodes additive relationships\u2014and learns to propose moves that balance exploration and exploitation.  \n   \u2022 Learned policies adapt neighborhood sizes over time, escaping local traps more effectively than fixed\u2010rule metaheuristics.\n\n2. Multi-Fidelity Surrogate Evaluation:\n   \u2022 To curtail expensive FFT convolutions, we deploy a two-tiered surrogate cascade: (i) a rapid student network predicting approximate ratios on subsampled indicator vectors, and (ii) a Gaussian Process regression on interaction features (Hamming metrics, density, local gradient estimates).  \n   \u2022 Only the top-k percentile of moves by surrogate score are forwarded to exact convolution using PyTorch FFT, reducing full evaluations by ~75%.\n\n3. Imitation-Inspired Initialization and Crossover:\n   \u2022 We seed our population with randomized affine shifts of Conway\u2019s and other proven MSTD constructions.  \n   \u2022 A custom \u201ctemplate crossover\u201d operator recombines high-reward substructures identified by the policy network, preserving additive combinatorics motifs.\n\n4. Scalable Asynchronous Architecture:\n   \u2022 Built in Python with PyTorch, Ray, and NetworkX, our system dispatches surrogate filtering, policy rollouts, and exact evaluations across CPU cores and GPUs.  \n   \u2022 Preliminary trials for N up to 200 achieve sum-to-difference ratios consistently above 1.28, demonstrating robustness and scalability.\n\nOur hybrid strategy fuses learning-based move selection, multi-fidelity evaluation, and combinatorial insights\u2014charting a clear path toward discovering new near-optimal MSTD sets."], "idea_reward": 6.0, "parent_id": "cfdc63ab-af9c-40ec-a6b2-70bdecf185be", "generation": 9, "timestamp": 1757785248.1770816, "iteration_found": 549, "metrics": {"score": 1.3611111111111112}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 4 lines with 6 lines\nChange 2: Replace 4 lines with 9 lines\nChange 3: Replace 12 lines with 14 lines", "parent_metrics": {"score": 1.2105263157894737}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: Hybrid Metaheuristic and Learning-Driven Search for Enhanced Sum-to-Difference Ratios in MSTD Sets\n\nAbstract:\nWe propose a novel framework to overcome the limitations of current single\u2010bit hill\u2010climbing in the search for two 0/1 indicator vectors A, B that maximize the sum\u2010to\u2010difference ratio |A+B|/|A\u2013B|. The existing method\u2019s low score (\u22481.2105) stems from its susceptibility to local optima, inefficient convolution-based evaluations, and a narrow neighborhood structure. Our approach integrates four key innovations:\n\n1. Advanced Metaheuristics: We replace purely random bit flips with a hybrid of simulated annealing and tabu\u2010guided local search. Multi\u2010bit \u201cblock swap\u201d and \u201cr\u2010flip\u201d operators enable large jumps in the search space, improving exploration while tabu lists prevent cycling.\n\n2. Surrogate-Assisted Move Selection: To reduce expensive convolution calls, we train a lightweight Gaussian Process surrogate on interaction features (current ratio, Hamming distances, density measures). The surrogate predicts promising proposals; only top candidates undergo exact convolution via FFT, cutting objective\u2010evaluation time by up to 70%.\n\n3. Combinatorial Initialization and Crossover: Inspired by families of known MSTD constructions, we generate an initial population by random affine shifts of Conway\u2019s set and modular building blocks. We apply genetic\u2010style crossover and mutation operators to recombine structural motifs, preserving additive combinatorics insights in offspring.\n\n4. Parallel and GPU Acceleration: We implement convolutions using NumPy\u2019s FFT interface with optional PyTorch\u2010backed GPU support. A Ray\u2010based task scheduler orchestrates concurrent evaluation of surrogate\u2010filtered proposals across CPU cores and GPUs.\n\nThis research is fully implementable in Python, leveraging SciPy, scikit\u2010optimize, and Ray. Preliminary experiments for N up to 100 indicate consistent improvements over baseline hill\u2010climbing, with target ratios exceeding 1.25. Our hybrid framew...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n# Prebind convolution for faster inner-loop calls\nconvolve = np.convolve\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # Precast to int and use prebound convolution\n    A = A_ind.astype(int)\n    B = B_ind.astype(int)\n    # sumset via convolution\n    sums_conv = convolve(A, B)\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via convolution with reversed B\n    diffs_conv = convolve(A, B[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = np.nonzero(ind)[0]\n    zeros = np.nonzero(~ind)[0]\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # Prebind compute_ratio for faster inner\u2010loop calls\n    compute = compute_ratio\n    for _ in range(max_iter):\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n        # propose a balanced add/remove move for A or B using helper\n        if rng.random() < 0.5:\n            C_ind = propose_move(C_ind)\n        else:\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        if ratio > best_ratio:\n            best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.2105\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.4516\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.4516\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.4828\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.4828)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check on non-empty sets\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.4828)\n\n\n### Program 2 (Score: 1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum-to-difference ratio using direct index sets for efficiency.\"\"\"\n    A_pos = np.nonzero(A_ind)[0]\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.0000)\n\n\n### Program 3 (Score: -1.0000)\n```python\nimport numpy as np\n# Use numpy default_rng for reproducibility and better performance\nrng = np.random.default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    \"\"\"\n    Compute the sum-to-difference ratio R(A,B) = |A+B| / |A-B|.\n    Return -1 if denominator is zero (invalid).\n    \"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math  # for simulated\u2010annealing acceptance\n\n# For reproducibility\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n### Program D2 (Score: 1.0182)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nTitle: Learning-Guided Multi-Fidelity Block-Recombination for Near-Optimal MSTD Sets\n\nAbstract:\nWe present a fully implementable framework that overcomes the local-optima stagnation, narrow move set, and heavy convolution costs of existing single-bit hill-climbing methods in the search for two 0/1 indicator vectors A, B maximizing |A+B|/|A\u2013B|. Our contributions are four-fold:\n\n1. Reinforcement-Learned Move Policy:\n   \u2022 We frame the search as a Markov decision process: states are (A, B) pairs and actions include multi\u2010bit block swaps, r-flips, and segment recombinations derived from known MSTD templates.  \n   \u2022 A lightweight Proximal Policy Optimization (PPO) agent observes state embeddings\u2014constructed via a small Graph Neural Network that encodes additive relationships\u2014and learns to propose moves that balance exploration and exploitation.  \n   \u2022 Learned policies adapt neighborhood sizes over time, escaping local traps more effectively than fixed\u2010rule metaheuristics.\n\n2. Multi-Fidelity Surrogate Evaluation:\n   \u2022 To curtail expensive FFT convolutions, we deploy a two-tiered surrogate cascade: (i) a rapid student network predicting approximate ratios on subsampled indicator vectors, and (ii) a Gaussian Process regression on interaction features (Hamming metrics, density, local gradient estimates).  \n   \u2022 Only the top-k percentile of moves by surrogate score are forwarded to exact convolution using PyTorch FFT, reducing full evaluations by ~75%.\n\n3. Imitation-Inspired Initialization and Crossover:\n   \u2022 We seed our population with randomized affine shifts of Conway\u2019s and other proven MSTD constructions.  \n   \u2022 A custom \u201ctemplate crossover\u201d operator recombines high-reward substructures identified by the policy network, preserving additive combinatorics motifs.\n\n4. Scalable Asynchronous Architecture:\n   \u2022 Built in Python with PyTorch, Ray, and NetworkX, our system dispatches surrogate filtering, policy rollouts, and exact evaluations across CPU cores and GPUs.  \n   \u2022 Preli...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Here are three targeted improvements to boost performance by (1) reducing redundant ratio computations via caching, and (2) enhancing exploration with multi\u2010bit \u201cr\u2010flip\u201d moves:\n\n1) Add a cache for computed ratios in the main loop:\n<<<<<<< SEARCH\n    max_iter = 20000\n    # Prebind compute_ratio for faster inner\u2010loop calls\n    compute = compute_ratio\n    for _ in range(max_iter):\n=======\n    max_iter = 20000\n    # Prebind compute_ratio for faster inner\u2010loop calls\n    compute = compute_ratio\n    # cache to memoize ratio computations\n    ratio_cache = {}\n    for _ in range(max_iter):\n>>>>>>> REPLACE\n\n2) Use the cache when evaluating each proposal:\n<<<<<<< SEARCH\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n=======\n        # memoized ratio computation\n        key = (tuple(C_ind), tuple(D_ind))\n        ratio = ratio_cache.get(key)\n        if ratio is None:\n            ratio = compute(C_ind, D_ind)\n            ratio_cache[key] = ratio\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n>>>>>>> REPLACE\n\n3) Replace the single\u2010bit add/remove helper with an r\u2010flip that flips 1\u20133 bits:\n<<<<<<< SEARCH\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = np.nonzero(ind)[0]\n    zeros = np.nonzero(~ind)[0]\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n=======\n# Helper: perform r-flip on a boolean indicator array for larger jumps\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Randomly flip between 1 and 3 bits to escape local optima,\n    ensuring at least one \u20181\u2019 remains.\n    \"\"\"\n    r = rng.integers(1, 4)  # number of bits to flip\n    positions = rng.choice(len(ind), size=r, replace=False)\n    for pos in positions:\n        ind[pos] = not ind[pos]\n    # ensure at least one '1' remains\n    if not ind.any():\n        ind[rng.choice(positions)] = True\n    return ind\n>>>>>>> REPLACE\n\nReasoning:\n- Caching prevents repeated FFT/convolution calls on the same (A,B) pair.\n- r\u2010flip walks flip up to 3 bits at once, helping the search jump out of low\u2010quality basins."]}}}