{"id": "63390eb2-17c0-4cef-a987-695553ed6ee7", "code": "import numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Removed unused prebound convolution and correlation functions\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nexp = math.exp\ncnz = np.count_nonzero\n\n# Cache for computed ratios to avoid redundant evaluations\n# Removed unused ratio_cache for clarity\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # caching disabled: bitwise ops are extremely fast and avoid hash overhead\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        ratio = -1.0\n    else:\n        # arrays are boolean masks\n        A_arr = A_ind\n        B_arr = B_ind\n        # use prebound functions to reduce attribute lookups\n        # Vectorized bitwise operations for faster union/difference counts\n        num_sums = int((A_arr | B_arr).sum())\n        num_diffs = int((A_arr ^ B_arr).sum())\n        if num_diffs == 0:\n            ratio = -1.0\n        else:\n            ratio = num_sums / num_diffs\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move to maintain constant cardinality and improve exploration.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(ind == 0)\n    if ones.size and zeros.size:\n        # with small probability do a two-bit swap for larger jumps\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:  # higher chance for multi-bit swaps\n            removes = rng.choice(ones, size=2, replace=False)\n            adds = rng.choice(zeros, size=2, replace=False)\n            ind[removes] = False\n            ind[adds] = True\n        else:\n            i_remove = rng.choice(ones)\n            i_add = rng.choice(zeros)\n            ind[i_remove] = False\n            ind[i_add] = True\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 20  # increased number of local proposals for better exploration\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9999        # slightly slower cooling for extended exploration\n    # restart logic to escape stagnation\n    no_improve = 0\n    max_no_improve = 5000\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng_random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng_random() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n            no_improve = 0\n        else:\n            no_improve += 1\n        # cool down\n        T *= decay\n        # restart when stagnated\n        if no_improve >= max_no_improve:\n            current_A, current_B = best_A.copy(), best_B.copy()\n            current_ratio = best_ratio\n            T = 1.0\n            no_improve = 0\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: DYNAMO: Dynamically-Adaptive Multi-Fidelity Optimization for Extremal Sum-to-Difference Ratio\n\nAbstract  \nExisting heuristics for maximizing the sum\u2010to\u2010difference ratio on binary indicator pairs plateau around 1.53 on N = 30 benchmarks, primarily due to (i) myopic single\u2010 or double\u2010bit moves, (ii) static surrogate fidelities, and (iii) fixed, hand\u2010tuned temperature schedules. We propose DYNAMO, a unified, implementable framework that overcomes these bottlenecks through four key innovations:\n\n1. Non-Myopic Graph-Based Proposal Engine  \n \u2022 A Graph Transformer policy network trained with reinforcement learning and short-horizon rollouts proposes k-step swap/add/remove sequences, guided by a learned value critic.  \n \u2022 Inspired by Monte-Carlo Tree Search, we evaluate tentative move trajectories at low cost, enabling the policy to discover high-reward neighborhoods that single-bit swaps miss.\n\n2. Hierarchical Multi-Fidelity Surrogates with Bayesian Bandit Control  \n \u2022 Three surrogate tiers\u2014(a) fast bit\u2010count proxies, (b) mid\u2010level GNN ensembles, and (c) high-accuracy random forests with Gaussian\u2010Process uncertainty\u2014provide a spectrum of cost vs. fidelity.  \n \u2022 A Thompson\u2010sampling bandit manager dynamically allocates compute, escalating to higher-cost surrogates only when uncertainty straddles performance thresholds.\n\n3. Differentiable Spectral Warm-Starts  \n \u2022 Continuous spectral relaxations of the indicator pair problem produce fractional seeds whose gradients are back\u2010propagated into the Graph Transformer\u2019s latent state, biasing proposals toward promising subspaces while preserving combinatorial structure.\n\n4. Meta-Learnt Temperature and Resource Scheduler  \n \u2022 An LSTM controller observes surrogate calibration statistics and acceptance histories to adaptively tune inverse\u2010temperature, batch\u2010size, and exploration noise.  \n \u2022 This data-driven schedule replaces static decay rules, maintaining high acceptance rates without manual tuning across N\u2208{30,50,80,120}.\n\nImplementation & Targets  \nWe implement DYNAMO with JAX/Flax for surrogate evaluation, PyTorch for RL components, and Ray for asynchronous parallel search. Bit\u2010packed representations and GPU\u2010accelerated kernels ensure scalability. On N=30\u2013100 benchmarks, we aim to exceed 1.8 ratio within 2\u00d7 the compute budget of prior methods and reach \u22652.0 on larger problem sizes, demonstrating broad applicability to extremal combinatorial optimization."], "idea_reward": 5.67, "parent_id": "95ee8299-261f-4048-a8c0-3cd6f0205f67", "generation": 20, "timestamp": 1757825985.2143788, "iteration_found": 1294, "metrics": {"score": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 3 lines with # Removed unused prebound convolution and correlation functions\nChange 2: 'ratio_cache = {}' to '# Removed unused ratio_cache for clarity'\nChange 3: Replace 2 lines with 2 lines\nChange 4: Replace 3 lines with 3 lines\nChange 5: Replace 2 lines with 2 lines\nChange 6: Replace 6 lines with 9 lines", "parent_metrics": {"score": 1.0}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: GRAFOX: Graph\u2010Adaptive Reinforcement and Fidelity Optimization for Extremal Sum\u2010to\u2010Difference Ratio  \n\nAbstract  \nExisting hill\u2010climbing and annealing\u2010based methods plateau near a 1.53 sum-to-difference ratio on N=30 benchmarks, largely because (i) moves are myopic single- or double-bit swaps, (ii) fidelity of surrogate evaluations is fixed, and (iii) temperature schedules are hand-tuned and static. To overcome these bottlenecks, we propose GRAFOX, a unified meta\u2010optimization framework that combines learned graph representations, adaptive fidelity control, and differentiable warm starts.  \n\nKey innovations:  \n1. Learned Graph\u2010Transformer Proposal Network  \n \u2022 We represent indicator\u2010vector pairs as bipartite graphs and train a small Graph Transformer policy via Reinforcement Learning to propose k\u2010step swap/add/remove sequences.  \n \u2022 The policy is rewarded not only on immediate ratio gains but also on predicted downstream improvement using a value\u2010network critic, enabling non\u2010myopic exploration of high-reward neighborhoods.  \n\n2. Bayesian Multi\u2010Fidelity Surrogate with Embedded Uncertainty  \n \u2022 We replace fixed\u2010cost surrogates with a two\u2010level ensemble\u2014a fast linear proxy and a mid\u2010cost geometric graph net\u2014augmented by a lightweight Gaussian Process uncertainty head.  \n \u2022 A Thompson\u2010sampling controller dynamically allocates evaluations, querying the exact ratio only when the surrogate\u2019s confidence interval straddles a performance threshold.  \n\n3. Differentiable Relaxation Warm\u2010Start & Meta\u2010Learnt Temperature Controller  \n \u2022 Continuous spectral relaxations produce fractional solutions whose gradients initialize the Graph Transformer\u2019s hidden states, biasing proposals toward promising subspaces.  \n \u2022 A small LSTM meta\u2010learner, trained across N\u2208{30,50,80,120}, continuously adjusts inverse\u2010temperature and proposal entropy based on surrogate calibration and acceptance statistics.  \n\nImplementation & Impact  \nWe will implement GRAFOX in JAX for surrogates and PyTorch ...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nexp = math.exp\ncnz = np.count_nonzero\n\n# Cache for computed ratios to avoid redundant evaluations\nratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # caching disabled: bitwise ops are extremely fast and avoid hash overhead\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        ratio = -1.0\n    else:\n        # arrays are already int8\n        A_arr = A_ind\n        B_arr = B_ind\n        # use prebound functions to reduce attribute lookups\n        # Vectorized bitwise operations for faster union/difference counts\n        num_sums = int(cnz(A_arr | B_arr))\n        num_diffs = int(cnz(A_arr != B_arr))\n        if num_diffs == 0:\n            ratio = -1.0\n        else:\n            ratio = num_sums / num_diffs\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move to maintain constant cardinality and improve exploration.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(ind == 0)\n    if ones.size and zeros.size:\n        # with small probability do a two-bit swap for larger jumps\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:  # higher chance for multi-bit swaps\n            removes = rng.choice(ones, size=2, replace=False)\n            adds = rng.choice(zeros, size=2, replace=False)\n            ind[removes] = False\n            ind[adds] = True\n        else:\n            i_remove = rng.choice(ones)\n            i_add = rng.choice(zeros)\n            ind[i_remove] = False\n            ind[i_add] = True\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 20  # increased number of local proposals for better exploration\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=np.int8)\n    B_ind = np.zeros(N, dtype=np.int8)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9999        # slightly slower cooling for extended exploration\n    # restart logic to escape stagnation\n    no_improve = 0\n    max_no_improve = 5000\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng_random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng_random() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n            no_improve = 0\n        else:\n            no_improve += 1\n        # cool down\n        T *= decay\n        # restart when stagnated\n        if no_improve >= max_no_improve:\n            current_A, current_B = best_A.copy(), best_B.copy()\n            current_ratio = best_ratio\n            T = 1.0\n            no_improve = 0\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.0000\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 2 (Score: 1.3704)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.3704)\n\n\n### Program 3 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\nimport functools\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5455)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.5000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Prebind convolution and correlation for compute_ratio speed\nconvolve_fn = np.convolve\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: 1.3478)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nTitle: DYNAMO: Dynamically-Adaptive Multi-Fidelity Optimization for Extremal Sum-to-Difference Ratio\n\nAbstract  \nExisting heuristics for maximizing the sum\u2010to\u2010difference ratio on binary indicator pairs plateau around 1.53 on N = 30 benchmarks, primarily due to (i) myopic single\u2010 or double\u2010bit moves, (ii) static surrogate fidelities, and (iii) fixed, hand\u2010tuned temperature schedules. We propose DYNAMO, a unified, implementable framework that overcomes these bottlenecks through four key innovations:\n\n1. Non-Myopic Graph-Based Proposal Engine  \n \u2022 A Graph Transformer policy network trained with reinforcement learning and short-horizon rollouts proposes k-step swap/add/remove sequences, guided by a learned value critic.  \n \u2022 Inspired by Monte-Carlo Tree Search, we evaluate tentative move trajectories at low cost, enabling the policy to discover high-reward neighborhoods that single-bit swaps miss.\n\n2. Hierarchical Multi-Fidelity Surrogates with Bayesian Bandit Control  \n \u2022 Three surrogate tiers\u2014(a) fast bit\u2010count proxies, (b) mid\u2010level GNN ensembles, and (c) high-accuracy random forests with Gaussian\u2010Process uncertainty\u2014provide a spectrum of cost vs. fidelity.  \n \u2022 A Thompson\u2010sampling bandit manager dynamically allocates compute, escalating to higher-cost surrogates only when uncertainty straddles performance thresholds.\n\n3. Differentiable Spectral Warm-Starts  \n \u2022 Continuous spectral relaxations of the indicator pair problem produce fractional seeds whose gradients are back\u2010propagated into the Graph Transformer\u2019s latent state, biasing proposals toward promising subspaces while preserving combinatorial structure.\n\n4. Meta-Learnt Temperature and Resource Scheduler  \n \u2022 An LSTM controller observes surrogate calibration statistics and acceptance histories to adaptively tune inverse\u2010temperature, batch\u2010size, and exploration noise.  \n \u2022 This data-driven schedule replaces static decay rules, maintaining high acceptance rates without manual tuning across N\u2208{30,50,80,120}.\n\nImpl...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Here are a set of targeted diffs to both clean up unused bits for readability/maintainability and reduce copy\u2010overhead in the inner loop for performance:\n\n<<<<<<< SEARCH\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n=======\n# Removed unused prebound convolution and correlation functions\n>>>>>>> REPLACE\n\nReasoning: those two were never used, so removing them trims dead code.\n\n<<<<<<< SEARCH\nratio_cache = {}\n=======\n# Removed unused ratio_cache for clarity\n>>>>>>> REPLACE\n\nReasoning: `ratio_cache` is declared but not consulted\u2014delete to avoid confusion.\n\n<<<<<<< SEARCH\n    A_ind = np.zeros(N, dtype=np.int8)\n    B_ind = np.zeros(N, dtype=np.int8)\n=======\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n>>>>>>> REPLACE\n\nReasoning: using boolean masks makes bitwise operations clearer, and removes the misleading \u201cint8\u201d comment in `compute_ratio`.\n\n<<<<<<< SEARCH\n        # arrays are already int8\n        A_arr = A_ind\n        B_arr = B_ind\n=======\n        # arrays are boolean masks\n        A_arr = A_ind\n        B_arr = B_ind\n>>>>>>> REPLACE\n\nReasoning: adjust the comment to match the new `bool` dtype.\n\n<<<<<<< SEARCH\n        num_sums = int(cnz(A_arr | B_arr))\n        num_diffs = int(cnz(A_arr != B_arr))\n=======\n        num_sums = int((A_arr | B_arr).sum())\n        num_diffs = int((A_arr ^ B_arr).sum())\n>>>>>>> REPLACE\n\nReasoning:  \n- Switch to `bool.sum()` which is straightforward on boolean arrays.  \n- Use bitwise XOR (`^`) instead of elementwise `!=` for clarity and slight speed gain in NumPy.\n\n<<<<<<< SEARCH\n        # Generate candidate proposals by copying both state vectors once\n        C_ind, D_ind = current_A.copy(), current_B.copy()\n        if rng_random() < 0.5:\n            C_ind = propose_move(C_ind)\n        else:\n            D_ind = propose_move(D_ind)\n=======\n        # Generate candidate proposals with only one copy to reduce overhead\n        if rng_random() < 0.5:\n            C_ind = current_A.copy()\n            C_ind = propose_move(C_ind)\n            D_ind = current_B  # reuse without copy\n        else:\n            C_ind = current_A  # reuse without copy\n            D_ind = current_B.copy()\n            D_ind = propose_move(D_ind)\n>>>>>>> REPLACE\n\nReasoning:  \nOriginally each local proposal cloned both `A` and `B`\u2014that\u2019s two full array copies per proposal. We only need to copy the vector we\u2019re about to mutate. This halves the number of array allocations/copies and yields a measurable speed\u2010up in the tight inner loop."]}}}