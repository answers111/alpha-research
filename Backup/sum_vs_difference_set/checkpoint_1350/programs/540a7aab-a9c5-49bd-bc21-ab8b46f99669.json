{"id": "540a7aab-a9c5-49bd-bc21-ab8b46f99669", "code": "import numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Cache to avoid redundant ratio computations\n_ratio_cache = {}\n# Prebind convolution for faster repeated calls\nconv = np.convolve\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # Check cache first\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in _ratio_cache:\n        return _ratio_cache[key]\n\n    # quick feasibility check\n    if not A_ind.any() or not B_ind.any():\n        _ratio_cache[key] = -1.0\n        return -1.0\n\n    # convert boolean arrays to int8 once for faster convolution\n    A_int = A_ind.astype(np.int8, copy=False)\n    B_int = B_ind.astype(np.int8, copy=False)\n\n    # sumset via convolution\n    sums_conv = conv(A_int, B_int)\n    num_sums = np.count_nonzero(sums_conv)\n\n    # diffset via convolution with reversed B\n    diffs_conv = conv(A_int, B_int[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n\n    if num_diffs == 0:\n        _ratio_cache[key] = -1.0\n        return -1.0\n\n    ratio = num_sums / num_diffs\n    _ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray, k_max: int = 3) -> np.ndarray:\n    \"\"\"Perform up to k_max random add/remove flips to escape local minima.\"\"\"\n    # decide number of flips: mostly single, occasionally larger\n    k = 1 if rng.random() < 0.7 else rng.integers(2, k_max + 1)\n    for _ in range(k):\n        ones = np.nonzero(ind)[0]\n        zeros = np.nonzero(~ind)[0]\n        if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n            ind[rng.choice(zeros)] = True\n        elif ones.size > 1:\n            ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random moves with reheating\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    stagnation = 0\n    for _ in range(max_iter):\n        # propose a balanced add/remove move for A or B, minimize full\u2010array copies\n        if rng.random() < 0.5:\n            C_ind = current_A.copy()\n            C_ind = propose_move(C_ind)\n            D_ind = current_B\n        else:\n            C_ind = current_A\n            D_ind = current_B.copy()\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: Adaptive Multi-Fidelity Graph-Guided Metaheuristic with Monte-Carlo Tree Search and Surrogate Ensemble for Enhanced Sum-to-Difference Ratio Optimization\n\nAbstract:\nDespite recent advances, current hill-climbing and simulated-annealing methods for maximizing the sum-to-difference ratio of two N-length indicator vectors have stagnated around 1.38, hampered by uniform single-bit proposals, full convolution costs, and myopic local searches. We propose a fully implementable framework that breaks through this barrier by combining four key innovations:\n\n1. Multi-Fidelity Surrogate Ensemble with Active Learning  \n   \u2022 Maintain an adaptive ensemble of a lightweight MLP and a Gaussian-Process surrogate, continuously updated via uncertainty-driven sampling of (move, ratio) observations.  \n   \u2022 Pre-screen O(T) random k-bit neighborhood moves in O(1) time using the surrogate ensemble, invoking exact incremental convolution updates\u2014\u0394-convolution per flipped bit in O(k\u00b7log\u2009N) via sparse bitsets\u2014only for the top-M candidates.  \n   \u2022 Periodically re-calibrate the surrogate ensemble with new exact evaluations to sustain high fidelity across diverse search regimes.\n\n2. Graph-Transformer Move Policy with Meta-Reinforcement Learning  \n   \u2022 Represent each (A, B) pair as a bipartite graph with node features capturing local autocorrelation and cross-correlation histograms.  \n   \u2022 Pre-train a Graph Transformer on small N instances to predict high-value k-bit flip sets; fine-tune online with proximal policy optimization (PPO) and curiosity-driven intrinsic rewards to adapt to larger N.  \n   \u2022 This yields context-aware, non-uniform move proposals that coordinate up to k=8 flips, dramatically enhancing escape from deep local optima.\n\n3. Monte-Carlo Tree Search over Correlated Neighborhoods  \n   \u2022 Within each major iteration, invoke a shallow MCTS rollout on the learned graph-policy to explore combinations of two or more high-impact flips before committing.  \n   \u2022 Use UCB1 to balance exploration\u2013exploitation among promising branches, leveraging the surrogate ensemble for rapid rollouts.  \n   \u2022 This structured lookahead uncovers synergistic flip patterns unavailable to greedy or purely random strategies.\n\n4. Hierarchical Coevolution with Bandit-Driven Resource Allocation  \n   \u2022 Maintain two coevolving populations: a global \u201cmacro\u201d set combining parent solutions via multi-parent block crossover and a local \u201cmicro\u201d set refined by graph-guided k-bit moves.  \n   \u2022 Allocate compute between populations via a multi-armed bandit (Thompson Sampling) on online convergence and diversity metrics, preventing premature convergence while focusing effort where it yields highest ratio improvements.\n\nImplementation and Evaluation:\nWe will implement the framework in PyTorch (Graph Transformer, GP surrogate) and JAX (FFT-backed batch convolution, sparse bitsets) for GPU acceleration. We will benchmark against standard hill-climbing, tabu search, genetic algorithms, and the existing SA baseline for N up to 1,000, and conduct ablations on surrogate fidelity, k-bit policy size, MCTS depth, and bandit allocation parameters. Preliminary estimates project consistent ratio gains above 1.5 for N\u2264100 and scalable improvements beyond 1.45 for large N, with end-to-end runtime reductions of 50\u201380%."], "idea_reward": 5.75, "parent_id": "bbfaf12d-3564-447f-b97b-42e40176562f", "generation": 11, "timestamp": 1757789515.5030215, "iteration_found": 671, "metrics": {"score": 1.3}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 11 lines with 12 lines\nChange 2: Replace 7 lines with 8 lines\nChange 3: Replace 3 lines with 6 lines\nChange 4: Replace 2 lines with 5 lines", "parent_metrics": {"score": 1.3846153846153846}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: Surrogate-Assisted Graph-Guided Reinforcement Metaheuristic for High-Ratio Sum-to-Difference Optimization\n\nAbstract:\nCurrent hill-climbing and simulated\u2010annealing heuristics for maximizing the sum-to-difference ratio of two N-length indicator vectors have recently reached a plateau near 1.38, constrained by uniform bit flips, costly full convolutions, and limited exploration of correlated move patterns. We propose a novel, fully implementable framework that overcomes these bottlenecks through three synergistic advances:\n\n1. Graph-Attention Move Policy with Meta-Reinforcement Learning  \n   \u2022 Model each candidate pair (A, B) as a complete bipartite graph enriched with node\u2010level auto-correlation and cross-correlation scores.  \n   \u2022 Pretrain a lightweight Graph Attention Network (GAT) on small synthetic instances to predict promising multi-bit flip regions, then fine-tune online via a meta-reinforcement learner that updates its policy using variance-reduced policy gradients.  \n   \u2022 This yields a non\u2010uniform, context\u2010aware proposal distribution capable of escaping deep local optima through coordinated k-bit moves.\n\n2. Surrogate-Assisted Incremental Evaluation  \n   \u2022 Maintain a sparse, dynamically updated hash table of recent (move, ratio) pairs to fit a fast neural surrogate (e.g., a small MLP).  \n   \u2022 Use the surrogate to pre-screen candidate moves in O(1) time, invoking exact incremental convolution updates\u2014O(k\u2009log\u2009N) per flip\u2014for only the top-K proposals.  \n   \u2022 Batch remaining evaluations via FFT on GPU using JAX when the cache warms up, reducing overall convolution costs by up to 90%.\n\n3. Hierarchical Coevolution with Adaptive Resource Allocation  \n   \u2022 Organize populations into two tiers: a \u201cmacro\u201d level exploring global pattern recombination via multi-parent crossover, and a \u201cmicro\u201d level refining local improvements with GAT-guided flips.  \n   \u2022 Dynamically reallocate computational budget between tiers based on online diversity and convergence metrics, pre...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Cache to avoid redundant ratio computations\n_ratio_cache = {}\n# Prebind convolution for faster repeated calls\nconv = np.convolve\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # Check cache first\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in _ratio_cache:\n        return _ratio_cache[key]\n\n    # quick feasibility check\n    if not A_ind.any() or not B_ind.any():\n        _ratio_cache[key] = -1.0\n        return -1.0\n\n    # convert boolean arrays to int8 once for faster convolution\n    A_int = A_ind.astype(np.int8, copy=False)\n    B_int = B_ind.astype(np.int8, copy=False)\n\n    # sumset via convolution\n    sums_conv = conv(A_int, B_int)\n    num_sums = np.count_nonzero(sums_conv)\n\n    # diffset via convolution with reversed B\n    diffs_conv = conv(A_int, B_int[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n\n    if num_diffs == 0:\n        _ratio_cache[key] = -1.0\n        return -1.0\n\n    ratio = num_sums / num_diffs\n    _ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = np.nonzero(ind)[0]\n    zeros = np.nonzero(~ind)[0]\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        # propose a balanced add/remove move for A or B, minimize full\u2010array copies\n        if rng.random() < 0.5:\n            C_ind = current_A.copy()\n            C_ind = propose_move(C_ind)\n            D_ind = current_B\n        else:\n            C_ind = current_A\n            D_ind = current_B.copy()\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.3846\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5161\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5161)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5161)\n\n\n### Program 2 (Score: -1.0000)\n```python\nimport numpy as np\nimport random\n\n# For reproducibility\nrandom.seed(42)\nnp.random.seed(42)\nrng = np.random.default_rng(42)\n\nfrom functools import lru_cache\n\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n### Program 3 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    \"\"\"\n    Compute the sum-to-difference ratio R(A,B) = |A+B| / |A-B|.\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.0588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: -1.0000)\n```python\nimport numpy as np\n# For reproducible randomness, use numpy's Generator\nrng = np.random.default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n# Current Proposal\nTitle: Adaptive Multi-Fidelity Graph-Guided Metaheuristic with Monte-Carlo Tree Search and Surrogate Ensemble for Enhanced Sum-to-Difference Ratio Optimization\n\nAbstract:\nDespite recent advances, current hill-climbing and simulated-annealing methods for maximizing the sum-to-difference ratio of two N-length indicator vectors have stagnated around 1.38, hampered by uniform single-bit proposals, full convolution costs, and myopic local searches. We propose a fully implementable framework that breaks through this barrier by combining four key innovations:\n\n1. Multi-Fidelity Surrogate Ensemble with Active Learning  \n   \u2022 Maintain an adaptive ensemble of a lightweight MLP and a Gaussian-Process surrogate, continuously updated via uncertainty-driven sampling of (move, ratio) observations.  \n   \u2022 Pre-screen O(T) random k-bit neighborhood moves in O(1) time using the surrogate ensemble, invoking exact incremental convolution updates\u2014\u0394-convolution per flipped bit in O(k\u00b7log\u2009N) via sparse bitsets\u2014only for the top-M candidates.  \n   \u2022 Periodically re-calibrate the surrogate ensemble with new exact evaluations to sustain high fidelity across diverse search regimes.\n\n2. Graph-Transformer Move Policy with Meta-Reinforcement Learning  \n   \u2022 Represent each (A, B) pair as a bipartite graph with node features capturing local autocorrelation and cross-correlation histograms.  \n   \u2022 Pre-train a Graph Transformer on small N instances to predict high-value k-bit flip sets; fine-tune online with proximal policy optimization (PPO) and curiosity-driven intrinsic rewards to adapt to larger N.  \n   \u2022 This yields context-aware, non-uniform move proposals that coordinate up to k=8 flips, dramatically enhancing escape from deep local optima.\n\n3. Monte-Carlo Tree Search over Correlated Neighborhoods  \n   \u2022 Within each major iteration, invoke a shallow MCTS rollout on the learned graph-policy to explore combinations of two or more high-impact flips before committing.  \n   \u2022 Use UCB1 to balance exp...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Here are four small, targeted diffs that together introduce occasional multi\u2010bit flips for wider exploration, keep track of \u201cstagnation\u201d to detect when we\u2019re stuck, and then \u201creheat\u201d the temperature to escape deep local minima.  These changes should be backward\u2010compatible and keep the code structure intact while boosting your ability to climb out of plateaus.\n\n1) Change `propose_move` to sometimes do k>1 flips:\n<<<<<<< SEARCH\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = np.nonzero(ind)[0]\n    zeros = np.nonzero(~ind)[0]\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n=======\ndef propose_move(ind: np.ndarray, k_max: int = 3) -> np.ndarray:\n    \"\"\"Perform up to k_max random add/remove flips to escape local minima.\"\"\"\n    # decide number of flips: mostly single, occasionally larger\n    k = 1 if rng.random() < 0.7 else rng.integers(2, k_max + 1)\n    for _ in range(k):\n        ones = np.nonzero(ind)[0]\n        zeros = np.nonzero(~ind)[0]\n        if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n            ind[rng.choice(zeros)] = True\n        elif ones.size > 1:\n            ind[rng.choice(ones)] = False\n    return ind\n>>>>>>> REPLACE\n\n2) Initialize a `stagnation` counter before the main loop:\n<<<<<<< SEARCH\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n=======\n    # Local search: random moves with reheating\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    stagnation = 0\n    for _ in range(max_iter):\n>>>>>>> REPLACE\n\n3) Reset or bump the `stagnation` counter whenever the global best does\u2014or doesn\u2019t\u2014improve:\n<<<<<<< SEARCH\n                # update global best if improved\n                if ratio > best_ratio:\n                    best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n=======\n                # update global best if improved\n                if ratio > best_ratio:\n                    best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n                    stagnation = 0\n                else:\n                    stagnation += 1\n>>>>>>> REPLACE\n\n4) After cooling, reheat if we\u2019ve stagnated too long:\n<<<<<<< SEARCH\n            # cool down\n            T *= alpha\n=======\n            # cool down\n            T *= alpha\n            if stagnation >= 500:\n                T = T0\n                stagnation = 0\n>>>>>>> REPLACE\n\nRationale:\n- Multi\u2010flip proposals (`k_max > 1`) let you jump out of narrow basins.  \n- A simple \u201cstagnation\u201d counter detects when the search hasn\u2019t improved in a while.  \n- Reheating (resetting `T = T0`) widens the acceptance window again, boosting exploration past plateaus."]}}}