{"id": "d13f0777-2b9d-46e7-8db0-e0a8643053cb", "code": "import numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_int: np.ndarray, B_int: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B| using int8 arrays to avoid casting. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_int.any() or not B_int.any():\n        return -1.0\n    # direct convolution for moderate N\n    sums_conv = np.convolve(A_int, B_int)\n    num_sums = np.count_nonzero(sums_conv)\n    diffs_conv = np.convolve(A_int, B_int[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly flip one bit in the int8 indicator, ensuring at least one '1' remains.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(ind == 0)\n    # decide to add or remove a '1'\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = 1\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = 0\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    # Use int8 indicators to avoid repeated casting\n    A_int = np.zeros(N, dtype=np.int8)\n    B_int = np.zeros(N, dtype=np.int8)\n    A_int[CONWAY_MSTD_INIT] = 1\n    B_int[:] = A_int\n\n    # Evaluate initial ratio with int arrays\n    best_ratio = compute_ratio(A_int, B_int)\n    best_A, best_B = A_int.copy(), B_int.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        C_int = current_A.copy()\n        D_int = current_B.copy()\n        # propose a balanced add/remove move for A or B using helper\n        if rng.random() < 0.5:\n            C_int = propose_move(C_int)\n        else:\n            D_int = propose_move(D_int)\n\n        ratio = compute_ratio(C_int, D_int)\n        if ratio < 0:\n            continue\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept new current (int arrays)\n            current_A, current_B, current_ratio = C_int.copy(), D_int.copy(), ratio\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_int.copy(), D_int.copy()\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_int, B_int = main()\n    print(\"A_int:\", A_int)\n    print(\"B_int:\", B_int)\n", "language": "python", "proposal": ["Title: Reinforcement\u2010Guided Surrogate\u2010Assisted Evolutionary Metaheuristic with Online Diversity Control for Sum-to-Difference Ratio Maximization\n\nAbstract:\nExisting simulated-annealing and graph-guided hill-climbing approaches for maximizing the sum-to-difference ratio of two N-length indicator vectors have plateaued near 1.38, primarily due to memoryless proposal schemes, expensive exact convolution evaluations (O(N\u00b2)), sluggish adaptation to dynamic landscapes, and rapid loss of population diversity. We propose a novel, fully implementable framework that integrates four synergistic components:\n\n1. Deep Reinforcement Move Proposal  \n   \u2022 Replace static GCN proposals with a lightweight actor-critic agent trained via proximal policy optimization (PPO) on multi-scale synthetic instances.  \n   \u2022 The policy conditions on compressed summary statistics (sparse auto-correlation sketches, current ratio) to propose k-bit flips with learned nonuniform probabilities, enabling strategic escapes from deep local basins.\n\n2. Surrogate-Assisted Evaluation  \n   \u2022 Introduce a Gaussian process (GP) surrogate model augmented with random feature maps to predict ratio changes in O(d\u00b2) time (d\u226aN), periodically retrained online with exact evaluations.  \n   \u2022 Use an adaptive multi-fidelity schedule: cheap surrogate pre-screens candidates; only promising moves incur incremental convolutional cost (O(k log N)) via sparse updates and batched GPU FFT.\n\n3. Adaptive Diversity Control  \n   \u2022 Employ a streaming clustering mechanism in Hamming space to identify emerging search niches and auto-tune niche radii based on cluster compactness.  \n   \u2022 Allocate computational budgets to underexplored clusters via a Boltzmann-weighted scheduler, preserving exploration without manual parameter tuning.\n\n4. Meta-Parameter Bayesian Optimization  \n   \u2022 Automate tuning of surrogate-fidelity thresholds, PPO learning rates, and niche allocation parameters via Bayesian optimization across low-dimensional meta-spaces, leveraging past runs on N\u2208{30,50,100}.\n\nImplementation will leverage PyTorch for PPO and GP surrogates, JAX for incremental and batched FFT evaluations, and Ray Tune for meta-optimization. We will benchmark against SA, tabu search, genetic algorithms, and the prior GCN-hybrid on N=30,50,100,200, measuring final ratio, runtime, and memory. Ablation studies will isolate the impact of PPO proposals, surrogate screening, and diversity control. We anticipate surpassing ratio 1.40 on N\u2264100, scaling efficiently to N\u2265200, and offering a general blueprint for combinatorial ratio maximization in additive\u2013combinatorics."], "idea_reward": 5.67, "parent_id": "be3ed9a6-54c5-45f4-8d9c-06144d24b6c6", "generation": 10, "timestamp": 1757787900.7513242, "iteration_found": 624, "metrics": {"score": 1.3846153846153846}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 17 lines with 13 lines\nChange 2: Replace 11 lines with 10 lines\nChange 3: Replace 11 lines with 11 lines\nChange 4: Replace 22 lines with 18 lines\nChange 5: Replace 4 lines with 4 lines", "parent_metrics": {"score": 1.3846153846153846}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: Hybrid Evolutionary\u2013Graph\u2010guided Metaheuristic for Efficient Sum-to-Difference Ratio Maximization\n\nAbstract:\nThe current simulated\u2010annealing\u2010driven hill\u2010climbing approach plateaus at a ratio of \u22481.27 for two N-length indicator vectors, largely due to (i) uniform, memoryless bit\u2010flip proposals that struggle to escape deep local optima, (ii) costly full\u2010length convolutions (O(N\u00b2)) at each move, and (iii) limited population diversity. We propose a novel, implementable framework that overcomes these limitations by integrating three core innovations:\n\n1. Graph Neural Move Proposer:\n   \u2022 Represent the pair (A, B) as a bipartite interaction graph whose nodes correspond to bit positions and edges encode current contributions to sums and differences.\n   \u2022 Train a lightweight graph\u2010convolutional network (GCN) offline on small synthetic instances to predict high\u2010impact bit flips, then fine\u2010tune online via a contextual multi\u2010armed bandit that balances exploration of uncertain positions and exploitation of high\u2010reward moves.\n   \u2022 This yields a nonuniform, history\u2010aware proposal distribution that rapidly navigates challenging basins.\n\n2. Incremental Convolutional Evaluation:\n   \u2022 Replace full re\u2010convolution with incremental update rules: maintain sparse representations of sumset and diffset counts and update them in O(k log N) per flip (k = number of affected auto\u2010correlations), reducing per\u2010move cost by orders of magnitude for moderate sparsity.\n   \u2022 Leverage FFT\u2010based batch updates on GPU (via JAX) when population\u2010level evaluations exceed a threshold, ensuring scalability to N\u226b100.\n\n3. Evolutionary Hybridization with Niching:\n   \u2022 Maintain a small population of candidate pairs, periodically applying multi\u2010parent crossover that preserves high\u2010order indicator patterns identified by the GCN.\n   \u2022 Introduce adaptive niching: cluster current elites in Hamming space and allocate search budgets to underexplored niches, thereby maintaining diversity and preventing premature conv...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # convert boolean arrays to int8 once for faster convolution\n    A_int = A_ind.astype(np.int8, copy=False)\n    B_int = B_ind.astype(np.int8, copy=False)\n    # sumset via convolution\n    sums_conv = np.convolve(A_int, B_int)\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via convolution with reversed B\n    diffs_conv = np.convolve(A_int, B_int[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = np.nonzero(ind)[0]\n    zeros = np.nonzero(~ind)[0]\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        C_ind = current_A.copy()\n        D_ind = current_B.copy()\n        # propose a balanced add/remove move for A or B using helper\n        if rng.random() < 0.5:\n            C_ind = propose_move(C_ind)\n        else:\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.3846\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5000\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5000)\n\n\n### Program 2 (Score: 1.2692)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.2692)\n\n\n### Program 3 (Score: 1.3478)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math  # for simulated\u2010annealing acceptance\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors via set operations\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B| using set operations. \n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.3478)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.2692)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: -1.0000)\n```python\nimport numpy as np\n\n# For reproducibility\nrng = np.random.default_rng(42)\n\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n# Current Proposal\nTitle: Reinforcement\u2010Guided Surrogate\u2010Assisted Evolutionary Metaheuristic with Online Diversity Control for Sum-to-Difference Ratio Maximization\n\nAbstract:\nExisting simulated-annealing and graph-guided hill-climbing approaches for maximizing the sum-to-difference ratio of two N-length indicator vectors have plateaued near 1.38, primarily due to memoryless proposal schemes, expensive exact convolution evaluations (O(N\u00b2)), sluggish adaptation to dynamic landscapes, and rapid loss of population diversity. We propose a novel, fully implementable framework that integrates four synergistic components:\n\n1. Deep Reinforcement Move Proposal  \n   \u2022 Replace static GCN proposals with a lightweight actor-critic agent trained via proximal policy optimization (PPO) on multi-scale synthetic instances.  \n   \u2022 The policy conditions on compressed summary statistics (sparse auto-correlation sketches, current ratio) to propose k-bit flips with learned nonuniform probabilities, enabling strategic escapes from deep local basins.\n\n2. Surrogate-Assisted Evaluation  \n   \u2022 Introduce a Gaussian process (GP) surrogate model augmented with random feature maps to predict ratio changes in O(d\u00b2) time (d\u226aN), periodically retrained online with exact evaluations.  \n   \u2022 Use an adaptive multi-fidelity schedule: cheap surrogate pre-screens candidates; only promising moves incur incremental convolutional cost (O(k log N)) via sparse updates and batched GPU FFT.\n\n3. Adaptive Diversity Control  \n   \u2022 Employ a streaming clustering mechanism in Hamming space to identify emerging search niches and auto-tune niche radii based on cluster compactness.  \n   \u2022 Allocate computational budgets to underexplored clusters via a Boltzmann-weighted scheduler, preserving exploration without manual parameter tuning.\n\n4. Meta-Parameter Bayesian Optimization  \n   \u2022 Automate tuning of surrogate-fidelity thresholds, PPO learning rates, and niche allocation parameters via Bayesian optimization across low-dimensional meta-spaces,...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are targeted diffs to reduce per\u2010iteration overhead by carrying int8 indicator arrays throughout (thus removing repeated boolean\u2192int casts), and by simplifying the move\u2010proposal to work directly on ints. This should lower the constant factors in your hot loop and improve the overall score.\n\nDiff 1: Change `compute_ratio` signature to take int8 directly  \n<<<<<<< SEARCH\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # convert boolean arrays to int8 once for faster convolution\n    A_int = A_ind.astype(np.int8, copy=False)\n    B_int = B_ind.astype(np.int8, copy=False)\n    # sumset via convolution\n    sums_conv = np.convolve(A_int, B_int)\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via convolution with reversed B\n    diffs_conv = np.convolve(A_int, B_int[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n=======\ndef compute_ratio(A_int: np.ndarray, B_int: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B| using int8 arrays to avoid casting. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_int.any() or not B_int.any():\n        return -1.0\n    # direct convolution for moderate N\n    sums_conv = np.convolve(A_int, B_int)\n    num_sums = np.count_nonzero(sums_conv)\n    diffs_conv = np.convolve(A_int, B_int[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n>>>>>>> REPLACE\n\nDiff 2: Update `propose_move` to flip int bits directly  \n<<<<<<< SEARCH\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = np.nonzero(ind)[0]\n    zeros = np.nonzero(~ind)[0]\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n=======\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly flip one bit in the int8 indicator, ensuring at least one '1' remains.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(ind == 0)\n    # decide to add or remove a '1'\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = 1\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = 0\n    return ind\n>>>>>>> REPLACE\n\nDiff 3: Initialize and track int8 arrays in `main`, and swap all uses of `A_ind`/`B_ind` for `A_int`/`B_int`  \n<<<<<<< SEARCH\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n=======\n    # Use int8 indicators to avoid repeated casting\n    A_int = np.zeros(N, dtype=np.int8)\n    B_int = np.zeros(N, dtype=np.int8)\n    A_int[CONWAY_MSTD_INIT] = 1\n    B_int[:] = A_int\n\n    # Evaluate initial ratio with int arrays\n    best_ratio = compute_ratio(A_int, B_int)\n    best_A, best_B = A_int.copy(), B_int.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n>>>>>>> REPLACE\n\nDiff 4: In the main loop, rename and use `C_int`/`D_int` and call updated routines  \n<<<<<<< SEARCH\n        C_ind = current_A.copy()\n        D_ind = current_B.copy()\n        # propose a balanced add/remove move for A or B using helper\n        if rng.random() < 0.5:\n            C_ind = propose_move(C_ind)\n        else:\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n=======\n        C_int = current_A.copy()\n        D_int = current_B.copy()\n        # propose a balanced add/remove move for A or B using helper\n        if rng.random() < 0.5:\n            C_int = propose_move(C_int)\n        else:\n            D_int = propose_move(D_int)\n\n        ratio = compute_ratio(C_int, D_int)\n        if ratio < 0:\n            continue\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < np.exp(delta / T):\n            # accept new current (int arrays)\n            current_A, current_B, current_ratio = C_int.copy(), D_int.copy(), ratio\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_int.copy(), D_int.copy()\n        T *= alpha\n>>>>>>> REPLACE\n\nDiff 5: Final print to reflect int arrays  \n<<<<<<< SEARCH\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n=======\nif __name__ == \"__main__\":\n    A_int, B_int = main()\n    print(\"A_int:\", A_int)\n    print(\"B_int:\", B_int)\n>>>>>>> REPLACE\n\nThese minimal changes eliminate repeated casting overhead, keep all operations on compact int8 arrays, and should noticeably speed up the convolution\u2010heavy search loop."]}}}