{"id": "8015f1d2-c2db-4c9d-81b1-8e8003cea818", "code": "import numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    \"\"\"Compute sum-to-difference ratio via sparse set enumeration. Return -1 if invalid.\"\"\"\n    # identify nonzero positions\n    a = set(np.flatnonzero(A_ind))\n    b = set(np.flatnonzero(B_ind))\n    # build the sum\u2010 and diff\u2010supports\n    sums = {x + y for x in a for y in b}\n    diffs = {x - y for x in a for y in b}\n    if not diffs:\n        return -1\n    return len(sums) / len(diffs)\n\ndef main():\n    N = 30\n    # Initialize with Conway MSTD set\n    init = [0, 2, 3, 4, 7, 11, 12, 14]\n    A_ind = np.zeros(N, dtype=int)\n    A_ind[init] = 1\n    B_ind = A_ind.copy()\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    for iter_num in range(1, max_iter + 1):\n        if iter_num % 5000 == 0:\n            print(f\"Iteration {iter_num}/{max_iter}, best_ratio={best_ratio:.4f}\")\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n        # Randomly flip one bit in C_ind or D_ind\n        target = C_ind if rng.random() < 0.5 else D_ind\n        idx = rng.integers(N)\n\n        # prevent empty\u2010set flips: if this would be the last 1, skip it\n        if target.sum() == 1 and target[idx] == 1:\n            continue\n        target[idx] ^= 1\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals\n        if ratio < 0:\n            continue\n        if ratio > best_ratio:\n            best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: An Active, Self-Supervised Hybrid Framework for Scalable Extremal Sum\u2010to\u2010Difference\u2010Ratio Optimization\n\nAbstract  \nDespite recent advances in surrogate\u2010guided reinforcement learning and MCTS, current local\u2010search approaches for maximizing the sum\u2010to\u2010difference ratio R(A,B) on {0,\u2026,N\u22121} suffer from three critical limitations: (i) high invalid\u2010proposal rates (R=\u22121) for N>30, (ii) inefficient exploration leading to premature convergence, and (iii) poor utilization of small\u2010N optimal solutions during large\u2010N search. We propose a novel, implementable methodology that addresses these weaknesses through four key innovations:\n\n1. Active Uncertainty\u2010Aware Surrogate Refinement  \n   \u2022 Ensemble Gaussian\u2010Bayesian Surrogates: Replace a single Transformer\u2010GNN with an ensemble of lightweight GNNs to quantify predictive uncertainty.  \n   \u2022 Adaptive Data Acquisition: Use Bayesian Active Learning to selectively generate exact\u2010solver labels for high\u2010uncertainty candidates, reducing global-call volume by \u226580% while driving MAE below 0.005.\n\n2. Differentiable Continuous\u2010Discrete Hybrid Optimization  \n   \u2022 Variational Autoencoding of Indicator Pairs: Encode (A,B) into a low\u2010dimensional latent space that captures block\u2010structure invariants.  \n   \u2022 Gradient\u2010Based Latent Refinement: Couple differentiable surrogate predictions with spectral gradients to navigate toward high\u2010R regions, projecting back to discrete sets via Gumbel\u2010Softmax annealing.  \n   \u2022 Constraint\u2010Penalized Objective: Integrate a soft\u2010penalty term for zero\u2010denominator avoidance, cutting invalid\u2010proposal rates to <0.05%.\n\n3. Curriculum\u2010Driven Meta\u2010Reinforcement Proposer  \n   \u2022 Self\u2010Play Across Increasing N Tasks: Pretrain a PPO\u2010based proposer on a sequence of growing N problems, distilling recurrent flip\u2010patterns into a shared policy network.  \n   \u2022 Adversarial Robustification: Introduce targeted \u201cperturb\u2010and\u2010refine\u201d episodes to ensure the proposer generalizes to unseen block\u2010configurations at large N.\n\n4. Constraint\u2010Aware Monte Carlo Tree Search  \n   \u2022 Dynamic Progressive Widening: Guided by surrogate\u2010uncertainty scores, expand promising branches first and prune low\u2010probability flips, yielding a 3\u00d7 speedup in local refinement.  \n   \u2022 Online Exact\u2010Certification Loop: For top\u2010K candidate sets, trigger CP\u2010SAT calls with incremental symmetry\u2010cuts, enabling guaranteed R improvements or optimality proofs up to N=200 on the fly.\n\nImplementation and Evaluation  \nWe will implement the framework in PyTorch (surrogates, autoencoders), JAX (gradient steps), Ray Tune (meta\u2010RL), and Or\u2010Tools CP\u2010SAT. On benchmark instances N={50,100,200,500,1000}, we expect to:  \n\u2013 Reduce invalid\u2010proposal rates from >10% to <0.05%  \n\u2013 Achieve 5\u201310\u00d7 faster convergence to record\u2010high R values  \n\u2013 Lower exact\u2010solver calls by >80% via active learning  \n\u2013 Demonstrate R gains of 4\u20138% over existing state\u2010of\u2010the\u2010art for N\u2265200  \n\nThis integrated, self\u2010supervised pipeline leverages uncertainty quantification, differentiable latent optimization, curriculum learning, and online certification to establish a scalable new paradigm for extremal discrete\u2010structure optimization."], "idea_reward": 6.5, "parent_id": "a2ce221b-b1c0-4c40-8f8a-1202a357b591", "generation": 4, "timestamp": 1757784739.617398, "iteration_found": 536, "metrics": {"error": -1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 2 lines with 2 lines\nChange 2: Replace 3 lines with 2 lines\nChange 3: Replace 12 lines with 11 lines\nChange 4: Replace 7 lines with 7 lines", "parent_metrics": {"error": -1.0}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: A Progressive Multi-Fidelity Surrogate-Guided Reinforcement-MCTS Framework for Extremal Sum-to-Difference Ratio Sets\n\nAbstract  \nWe propose a next-generation, end-to-end methodology for discovering extremal sum-to-difference-ratio (R) subsets A,B\u2286{0,\u2026,N\u22121} that simultaneously (i) eliminates the \u201c\u20131\u201d invalid outputs plaguing blind local search for N>30, (ii) accelerates convergence by an order of magnitude, and (iii) scales from N=30 to N\u2248500 while discovering new record-high ratios. Our key innovations address three core weaknesses of the current approach\u2014absence of continuous guidance, inability to leverage small-N optima, and lack of modular scaling\u2014by integrating insights from surrogate-assisted evolution, deep reinforcement learning, Monte-Carlo tree search (MCTS), and exact optimization:\n\n1. Progressive Multi-Fidelity Surrogate Training  \n  \u2022 Small-N Enumeration Data: Exhaustively solve R-maximization via Gurobi for N\u226450 to build a library of certified optima.  \n  \u2022 Graph-Transformer Surrogate: Train a Transformer-augmented graph neural network on set-pair encodings, achieving MAE<0.01 for R-prediction across N\u2208[30,200].  \n  \u2022 Hierarchical Latent Decomposition: Factor indicator vectors into contiguous blocks, enabling the surrogate to generalize local sum/difference patterns to larger N.\n\n2. Surrogate-Guided Reinforcement Learning Proposer  \n  \u2022 Gumbel-Softmax Relaxation: Parameterize A,B selections in continuous space and train a policy network via proximal-policy optimization to propose high-R flips in latent space.  \n  \u2022 Soft \u201cEnvironment\u201d Rewards: Use the surrogate\u2019s R-estimate as dense feedback, biasing the policy toward promising regions and reducing invalid proposals to <0.1%.\n\n3. MCTS-Driven Local Refinement  \n  \u2022 Block-wise Tree Search: Within each candidate\u2019s neighborhood, run MCTS over block flip sequences, guided by surrogate priors to exploit promising substructures.  \n  \u2022 Diversity Promotion: Augment the search with an entropy-based bonus t...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nimport random\n\n# For reproducibility\nrandom.seed(42)\nnp.random.seed(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    \"\"\"\n    Compute the sum-to-difference ratio R(A,B) = |A+B| / |A-B|.\n    Return -1 if denominator is zero (invalid).\n    \"\"\"\n    S = np.correlate(A_ind, B_ind, mode='full')\n    D = np.correlate(A_ind, B_ind[::-1], mode='full')\n    s_count = np.count_nonzero(S)\n    d_count = np.count_nonzero(D)\n    if d_count == 0:\n        return -1  # invalid difference set\n    return s_count / d_count\n\ndef main():\n    N = 30\n    # Initialize with Conway MSTD set\n    init = [0, 2, 3, 4, 7, 11, 12, 14]\n    A_ind = np.zeros(N, dtype=int)\n    A_ind[init] = 1\n    B_ind = A_ind.copy()\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    for iter_num in range(1, max_iter + 1):\n        if iter_num % 5000 == 0:\n            print(f\"Iteration {iter_num}/{max_iter}, best_ratio={best_ratio:.4f}\")\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n        # Randomly flip one bit in C_ind or D_ind\n        target = C_ind if random.random() < 0.5 else D_ind\n        idx = random.randrange(N)\n        target[idx] ^= 1\n\n        # skip invalids\n        if C_ind.sum() == 0 or D_ind.sum() == 0:\n            continue\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals\n        if ratio < 0:\n            continue\n        if ratio > best_ratio:\n            best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- error: -1.0000\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: error. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.4516\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.4516\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.4828\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.4828)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check on non-empty sets\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.4828)\n\n\n### Program 2 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n# removed unused Numba import for maintainability\n\n# For reproducibility\nrng = default_rng(42)\n\n@njit(fastmath=True, cache=True)\ndef compute_ratio_numba(A_ind, B_ind):\n    # boolean A_ind/B_ind -> mark sum/difference supports\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n### Program 3 (Score: -1.0000)\n```python\nimport numpy as np\nimport numba\n\n# For reproducibility\nrng = np.random.default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\n# now explicitly count unique sums and differences\n@numba.njit(cache=True, nogil=True)\ndef compute_ratio(A_ind, B_ind):\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: -1.0000)\n```python\nimport numpy as np\n\n# For reproducibility\nrng = np.random.default_rng(42)\n\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n### Program D2 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n# Current Proposal\nTitle: An Active, Self-Supervised Hybrid Framework for Scalable Extremal Sum\u2010to\u2010Difference\u2010Ratio Optimization\n\nAbstract  \nDespite recent advances in surrogate\u2010guided reinforcement learning and MCTS, current local\u2010search approaches for maximizing the sum\u2010to\u2010difference ratio R(A,B) on {0,\u2026,N\u22121} suffer from three critical limitations: (i) high invalid\u2010proposal rates (R=\u22121) for N>30, (ii) inefficient exploration leading to premature convergence, and (iii) poor utilization of small\u2010N optimal solutions during large\u2010N search. We propose a novel, implementable methodology that addresses these weaknesses through four key innovations:\n\n1. Active Uncertainty\u2010Aware Surrogate Refinement  \n   \u2022 Ensemble Gaussian\u2010Bayesian Surrogates: Replace a single Transformer\u2010GNN with an ensemble of lightweight GNNs to quantify predictive uncertainty.  \n   \u2022 Adaptive Data Acquisition: Use Bayesian Active Learning to selectively generate exact\u2010solver labels for high\u2010uncertainty candidates, reducing global-call volume by \u226580% while driving MAE below 0.005.\n\n2. Differentiable Continuous\u2010Discrete Hybrid Optimization  \n   \u2022 Variational Autoencoding of Indicator Pairs: Encode (A,B) into a low\u2010dimensional latent space that captures block\u2010structure invariants.  \n   \u2022 Gradient\u2010Based Latent Refinement: Couple differentiable surrogate predictions with spectral gradients to navigate toward high\u2010R regions, projecting back to discrete sets via Gumbel\u2010Softmax annealing.  \n   \u2022 Constraint\u2010Penalized Objective: Integrate a soft\u2010penalty term for zero\u2010denominator avoidance, cutting invalid\u2010proposal rates to <0.05%.\n\n3. Curriculum\u2010Driven Meta\u2010Reinforcement Proposer  \n   \u2022 Self\u2010Play Across Increasing N Tasks: Pretrain a PPO\u2010based proposer on a sequence of growing N problems, distilling recurrent flip\u2010patterns into a shared policy network.  \n   \u2022 Adversarial Robustification: Introduce targeted \u201cperturb\u2010and\u2010refine\u201d episodes to ensure the proposer generalizes to unseen block\u2010configurations at large N.\n\n4. Constraint\u2010Aw...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are four targeted diffs to (1) simplify and speed up the ratio computation by using Python\u2010set operations instead of full convolutions, (2) switch to a single NumPy RNG for both performance and readability, and (3) eliminate proposals that would empty a set so you never hit the \u201c\u20131\u201d invalid case. Together these should reduce overhead, improve maintainability, and drive your invalid\u2010proposal rate to zero.\n\n<<<<<<< SEARCH\nimport numpy as np\nimport random\n=======\nimport numpy as np\nfrom numpy.random import default_rng\n>>>>>>> REPLACE\n\nExplanation: remove the built\u2010in `random` module, use `default_rng` for both speed and reproducibility.\n\n<<<<<<< SEARCH\n# For reproducibility\nrandom.seed(42)\nnp.random.seed(42)\n=======\n# For reproducibility\nrng = default_rng(42)\n>>>>>>> REPLACE\n\nExplanation: initialize a single, high\u2010quality random number generator.\n\n<<<<<<< SEARCH\ndef compute_ratio(A_ind, B_ind):\n    \"\"\"\n    Compute the sum-to-difference ratio R(A,B) = |A+B| / |A-B|.\n    Return -1 if denominator is zero (invalid).\n    \"\"\"\n    S = np.correlate(A_ind, B_ind, mode='full')\n    D = np.correlate(A_ind, B_ind[::-1], mode='full')\n    s_count = np.count_nonzero(S)\n    d_count = np.count_nonzero(D)\n    if d_count == 0:\n        return -1  # invalid difference set\n    return s_count / d_count\n=======\ndef compute_ratio(A_ind, B_ind):\n    \"\"\"Compute sum-to-difference ratio via sparse set enumeration. Return -1 if invalid.\"\"\"\n    # identify nonzero positions\n    a = set(np.flatnonzero(A_ind))\n    b = set(np.flatnonzero(B_ind))\n    # build the sum\u2010 and diff\u2010supports\n    sums = {x + y for x in a for y in b}\n    diffs = {x - y for x in a for y in b}\n    if not diffs:\n        return -1\n    return len(sums) / len(diffs)\n>>>>>>> REPLACE\n\nExplanation: for sparse indicator arrays this reduces memory churn and is typically faster than full convolution\u2009\u2014\u2009especially when the sets remain fairly small.\n\n<<<<<<< SEARCH\n        target = C_ind if random.random() < 0.5 else D_ind\n        idx = random.randrange(N)\n        target[idx] ^= 1\n\n        # skip invalids\n        if C_ind.sum() == 0 or D_ind.sum() == 0:\n            continue\n=======\n        target = C_ind if rng.random() < 0.5 else D_ind\n        idx = rng.integers(N)\n\n        # prevent empty\u2010set flips: if this would be the last 1, skip it\n        if target.sum() == 1 and target[idx] == 1:\n            continue\n        target[idx] ^= 1\n>>>>>>> REPLACE\n\nExplanation: use `rng` for branch choice and index selection, and never flip the last 1 to 0\u2009\u2014\u2009this guarantees neither `C_ind` nor `D_ind` ever goes empty, so you will never see the invalid \u201c\u20131\u201d case.\n\nThese small, surgical replacements should remove virtually all \u201c\u22121\u201d errors, cut per\u2010iteration overhead, and make the code more maintainable."]}}}