{"id": "d52b6354-9490-4e05-b562-46f6faf21ed4", "code": "import numpy as np\n\n# For reproducibility\nrng = np.random.default_rng(42)\n# Caching disabled for clarity\n# _ratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    \"\"\"\n    Compute the sum-to-difference ratio R(A,B) = |A+B| / |A-B|.\n    Return -1 if denominator is zero (invalid).\n    Uses caching and correlation to speed up repeated calls.\n    \"\"\"\n    # caching disabled; compute fresh each call\n    # Compute sumset and difference via FFT\u2010fallback for larger N\n    length = len(A_ind) + len(B_ind) - 1\n    if len(A_ind) > 128:\n        fa = np.fft.rfft(A_ind, n=length)\n        fb = np.fft.rfft(B_ind, n=length)\n        S = (np.fft.irfft(fa * fb, n=length) > 0.5)\n        fa_full = np.fft.fft(A_ind, n=length)\n        fb_full = np.fft.fft(B_ind, n=length)\n        D = (np.fft.ifft(fa_full * np.conj(fb_full), n=length).real > 0.5)\n    else:\n        S = np.convolve(A_ind, B_ind, mode='full')\n        D = np.correlate(A_ind, B_ind, mode='full')\n    # Count unique sums and positive differences\n    s_count = np.count_nonzero(S)\n    d_count = np.count_nonzero(D)\n    if d_count == 0:\n        ratio = -1\n    else:\n        ratio = s_count / d_count\n    # caching disabled\n    return ratio\n\ndef main():\n    N = 30\n    # Initialize with Conway MSTD set\n    init = [0, 2, 3, 4, 7, 11, 12, 14]\n    A_ind = np.zeros(N, dtype=bool)\n    A_ind[init] = True\n    B_ind = A_ind.copy()\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # --- simulated annealing setup ---\n    T0 = 1.0\n    current_ratio = best_ratio\n    # -----------------------------------\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    for iter_num in range(1, max_iter + 1):\n        if iter_num % 5000 == 0:\n            print(f\"Iteration {iter_num}/{max_iter}, best_ratio={best_ratio:.4f}\")\n        # In\u2010place single\u2010bit flip (with revert if rejected), avoiding full copies\n        target_arr = A_ind if rng.random() < 0.5 else B_ind\n        idx = rng.integers(0, N)\n        target_arr[idx] ^= 1\n        # Reject empty\u2010set flips immediately\n        if target_arr.sum() == 0:\n            target_arr[idx] ^= 1\n            continue\n        new_ratio = compute_ratio(A_ind, B_ind)\n        # simple Metropolis acceptance\n        delta = new_ratio - current_ratio\n        # exponential cooling schedule\n        T = T0 * (0.99 ** iter_num)\n        if delta >= 0 or rng.random() < np.exp(delta / max(T,1e-8)):\n            # accept\n            current_ratio = new_ratio\n            if new_ratio > best_ratio:\n                best_ratio = new_ratio\n                best_A, best_B = A_ind.copy(), B_ind.copy()\n        else:\n            # reject: revert flip\n            target_arr[idx] ^= 1\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Research Abstract\n\nWe aim to overcome the stagnation (error = \u20131.0) and poor scalability of the existing single\u2010flip, cache\u2010assisted hill\u2010climbing method for finding extremal sum\u2010dominant pairs A,B\u2282{0,\u2026,N\u20131} that maximize R(A,B)=|A+B|/|A\u2013B|. Our proposal introduces a unified, modular metaheuristic framework combining (1) constant\u2010time incremental updates, (2) diversified neighborhood exploration, and (3) adaptive control, to systematically escape local traps and scale to N\u226b50.\n\n1. Identification of Weaknesses  \n\u2022 Ratio degeneracy: Single\u2010bit flips can collapse a set to empty, forcing R=\u20131 and costly restarts.  \n\u2022 O(N\u00b2) recomputation: Convolution/correlation on the full indicator vectors dominates runtime, preventing N>50.  \n\u2022 Greedy hill\u2010climbing: Strict \u201caccept\u2010if\u2010better\u201d policy and small neighborhoods lock the search into poor optima.\n\n2. Novel Technical Improvements  \na. Bit\u2010Vector Shift\u2010Or Updates  \n   \u2013 Represent A,B as machine\u2010word bitsets. Maintain sum\u2010set and difference\u2010set bit\u2010masks S,D of length 2N.  \n   \u2013 On toggling bit i of A or B, update S,D by bitwise shifts and XOR/AND operations in O(W) (W=words), independent of N\u2019s magnitude.  \n   \u2013 Update |A+B| and |A\u2013B| in O(1) via hardware\u2010accelerated popcounts.  \nb. Rich Mixed Neighborhoods  \n   \u2013 Define multi\u2010bit \u201cblock flips\u201d that add or remove contiguous intervals in A or B to traverse larger regions quickly.  \n   \u2013 Incorporate swap moves (transfer a base element from A to B or vice versa) preserving |A|+|B| constant, avoiding empty\u2010set degeneracy.  \nc. Adaptive Metaheuristic Control  \n   \u2013 Integrate simulated annealing, tabu search, and population\u2010based genetic operations under a bandit\u2010driven operator\u2010selection layer, which learns on\u2010the\u2010fly the most promising move types.  \n   \u2013 Employ reactive restart policies based on runtime progress indicators (e.g., ratio plateau length) to re\u2010seed from best\u2010known MSTD templates or diverse random seeds.\n\n3. Inspirations from Proven Methods  \n\u2022 Sparse update schemes in high\u2010performance bitwise graph algorithms inspire our O(W) incremental S,D management.  \n\u2022 Adaptive operator selection\u2014successfully applied in SAT and TSP solvers\u2014guides our dynamic choice among flips, swaps, crossovers, and restarts.  \n\u2022 Tabu tenure and block\u2010move concepts from combinatorial optimization inform our strategies to avoid cycling and escape deep local minima.\n\n4. Implementation and Evaluation Plan  \n\u2013 Prototype core update routines in C++ using CPU\u2010intrinsics; wrap in Python for high\u2010level control.  \n\u2013 Benchmark on N={50,100,200,500} against the baseline code, measuring best\u2010found R(A,B), time\u2010to\u2010first\u2010improvement, and search diversity.  \n\u2013 Conduct ablation studies isolating each component: shift\u2010or updates, block\u2010moves, adaptive control, and template\u2010seeding.  \n\u2013 Release code and datasets to the computational combinatorics community.\n\nBy combining constant\u2010time local updates, large\u2010step neighborhood moves, and adaptive multi\u2010strategy control, we expect to break through the current performance ceiling, discover new high\u2010ratio MSTD pairs for substantially larger N, and provide a general toolkit for extremal additive\u2010combinatorics search problems."], "idea_reward": 6.5, "parent_id": "967bbf32-2928-441f-85e1-96d3be1652fd", "generation": 8, "timestamp": 1757777564.3801806, "iteration_found": 324, "metrics": {"error": -1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 2 lines with 2 lines\nChange 2: Replace 4 lines with # caching disabled; compute fresh each call\nChange 3: Replace 6 lines with 15 lines\nChange 4: Replace 3 lines with 2 lines\nChange 5: Replace 2 lines with 2 lines", "parent_metrics": {"error": -1.0}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nResearch Abstract\n\nWe propose to enhance the automated discovery of pairs of finite subsets A,B\u2282{0,\u2026,N\u22121} that maximize the \u201csum\u2010to\u2010difference\u201d ratio R(A,B)=|A+B|/|A\u2212B|, addressing the stagnation (error=\u22121.0) and scalability limitations of the current single\u2010flip hill\u2010climbing approach. Our contributions are:\n\n1. Critical Analysis of Current Weaknesses  \n   \u2022 Ratio degeneracy: When one set becomes empty the ratio is forced to \u22121 and search restarts inefficiency are high.  \n   \u2022 Global convolution/correlation: Every bit\u2010flip recomputes full\u2010length discrete convolution and correlation in O(N\u00b2), dominating run\u2010time and preventing large N.  \n   \u2022 Pure hill\u2010climbing: Always rejecting non\u2010improving flips traps the search in poor local optima. Single\u2010bit flips limit exploration of richer neighborhoods.\n\n2. Novel Technical Improvements  \n   a. Incremental Set\u2010Update Engine  \n     \u2013 Maintain explicit sum\u2010 and difference\u2010indicator bit\u2010vectors S,D of length 2N for (A+B),(A\u2212B).  \n     \u2013 On toggling A[i] (or B[j]), update S and D in O(|B|+|A|) by adding/removing contributions only at positions i+B or i\u2212B and B\u2212i (resp.), avoiding full convolution/correlation.  \n     \u2013 Track counts |A+B|,|A\u2212B| in O(1) per update to compute R(A,B) instantly.  \n   b. Escaping Local Optima via Simulated Annealing  \n     \u2013 Replace strict accept\u2010only\u2010improvements with a temperature\u2010driven Metropolis criterion, permitting controlled downhill moves to explore new basins.  \n     \u2013 Schedule temperature T(t) to decay slowly over iterations, balancing exploration and exploitation.  \n   c. Population\u2010Based Genetic Diversification  \n     \u2013 Maintain a small population of candidate pairs {A_k,B_k}.  \n     \u2013 Perform uniform\u2010crossover on indicator vectors and occasional multi\u2010bit mutations to explore large structural changes.  \n     \u2013 Periodically re\u2010seed with known MSTD\u2010family constructions (e.g. Conway\u2019s templates) inflated or translated, ensuring retention of proven high\u2010ratio patterns.  \n\n3. Inspirations from...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\n\n# For reproducibility\nrng = np.random.default_rng(42)\n# Global cache for computed ratios to avoid redundant convolution/correlation\n_ratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    \"\"\"\n    Compute the sum-to-difference ratio R(A,B) = |A+B| / |A-B|.\n    Return -1 if denominator is zero (invalid).\n    Uses caching and correlation to speed up repeated calls.\n    \"\"\"\n    # Check cache\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in _ratio_cache:\n        return _ratio_cache[key]\n    # Compute sumset via convolution and difference via correlation\n    S = np.convolve(A_ind, B_ind, mode='full')\n    D = np.correlate(A_ind, B_ind, mode='full')\n    # Count unique sums and positive differences\n    s_count = (S > 0).sum()\n    d_count = (D > 0).sum()\n    if d_count == 0:\n        ratio = -1\n    else:\n        ratio = s_count / d_count\n    # Store result\n    _ratio_cache[key] = ratio\n    return ratio\n\ndef main():\n    N = 30\n    # Initialize with Conway MSTD set\n    init = [0, 2, 3, 4, 7, 11, 12, 14]\n    A_ind = np.zeros(N, dtype=bool)\n    A_ind[init] = True\n    B_ind = A_ind.copy()\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # --- simulated annealing setup ---\n    T0 = 1.0\n    current_ratio = best_ratio\n    # -----------------------------------\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    for iter_num in range(1, max_iter + 1):\n        if iter_num % 5000 == 0:\n            print(f\"Iteration {iter_num}/{max_iter}, best_ratio={best_ratio:.4f}\")\n        # In\u2010place single\u2010bit flip (with revert if rejected), avoiding full copies\n        target_arr = A_ind if rng.random() < 0.5 else B_ind\n        idx = rng.integers(0, N)\n        target_arr[idx] ^= 1\n        # Reject empty\u2010set flips immediately\n        if target_arr.sum() == 0:\n            target_arr[idx] ^= 1\n            continue\n        new_ratio = compute_ratio(A_ind, B_ind)\n        # simple Metropolis acceptance\n        delta = new_ratio - current_ratio\n        # linearly cooling: T = T0*(1 \u2212 iter/max_iter)\n        T = T0 * (1.0 - iter_num / max_iter)\n        if delta >= 0 or rng.random() < np.exp(delta / max(T,1e-8)):\n            # accept\n            current_ratio = new_ratio\n            if new_ratio > best_ratio:\n                best_ratio = new_ratio\n                best_A, best_B = A_ind.copy(), B_ind.copy()\n        else:\n            # reject: revert flip\n            target_arr[idx] ^= 1\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- error: -1.0000\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: error. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.4138\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.4516\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.4828\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.4828)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check on non-empty sets\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.4828)\n\n\n### Program 2 (Score: -1.0000)\n```python\nimport numpy as np\nimport random\n\n# For reproducibility\nrandom.seed(42)\nnp.random.seed(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    \"\"\"Vectorized convolution for sum-to-difference ratio\"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n### Program 3 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    \"\"\"\n    Compute the sum-to-difference ratio R(A,B) = |A+B| / |A-B|.\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.3478)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: 1.4138)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nResearch Abstract\n\nWe aim to overcome the stagnation (error = \u20131.0) and poor scalability of the existing single\u2010flip, cache\u2010assisted hill\u2010climbing method for finding extremal sum\u2010dominant pairs A,B\u2282{0,\u2026,N\u20131} that maximize R(A,B)=|A+B|/|A\u2013B|. Our proposal introduces a unified, modular metaheuristic framework combining (1) constant\u2010time incremental updates, (2) diversified neighborhood exploration, and (3) adaptive control, to systematically escape local traps and scale to N\u226b50.\n\n1. Identification of Weaknesses  \n\u2022 Ratio degeneracy: Single\u2010bit flips can collapse a set to empty, forcing R=\u20131 and costly restarts.  \n\u2022 O(N\u00b2) recomputation: Convolution/correlation on the full indicator vectors dominates runtime, preventing N>50.  \n\u2022 Greedy hill\u2010climbing: Strict \u201caccept\u2010if\u2010better\u201d policy and small neighborhoods lock the search into poor optima.\n\n2. Novel Technical Improvements  \na. Bit\u2010Vector Shift\u2010Or Updates  \n   \u2013 Represent A,B as machine\u2010word bitsets. Maintain sum\u2010set and difference\u2010set bit\u2010masks S,D of length 2N.  \n   \u2013 On toggling bit i of A or B, update S,D by bitwise shifts and XOR/AND operations in O(W) (W=words), independent of N\u2019s magnitude.  \n   \u2013 Update |A+B| and |A\u2013B| in O(1) via hardware\u2010accelerated popcounts.  \nb. Rich Mixed Neighborhoods  \n   \u2013 Define multi\u2010bit \u201cblock flips\u201d that add or remove contiguous intervals in A or B to traverse larger regions quickly.  \n   \u2013 Incorporate swap moves (transfer a base element from A to B or vice versa) preserving |A|+|B| constant, avoiding empty\u2010set degeneracy.  \nc. Adaptive Metaheuristic Control  \n   \u2013 Integrate simulated annealing, tabu search, and population\u2010based genetic operations under a bandit\u2010driven operator\u2010selection layer, which learns on\u2010the\u2010fly the most promising move types.  \n   \u2013 Employ reactive restart policies based on runtime progress indicators (e.g., ratio plateau length) to re\u2010seed from best\u2010known MSTD templates or diverse random seeds.\n\n3. Inspirations from Proven Methods  \n\u2022 Sparse update schemes in...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are a few small, targeted changes that should both simplify the code path and reduce per\u2010iteration overhead\u2014removing the expensive array\u2010to\u2010bytes hashing, replacing full O(N\u00b2) convolutions with an FFT\u2010fallback for large N, and tightening up the cooling schedule for faster convergence.\n\nEach change is given as an exact SEARCH/REPLACE diff.\n\n1) Disable the global byte\u2010cache (it adds hashing overhead and in practice rarely hits in SA):  \n<<<<<<< SEARCH\n# Global cache for computed ratios to avoid redundant convolution/correlation\n_ratio_cache = {}\n=======\n# Caching disabled for clarity\n# _ratio_cache = {}\n>>>>>>> REPLACE\n\n2) Remove the cache\u2010lookup logic inside `compute_ratio` and just recompute each time:  \n<<<<<<< SEARCH\n    # Check cache\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in _ratio_cache:\n        return _ratio_cache[key]\n=======\n    # caching disabled; compute fresh each call\n>>>>>>> REPLACE\n\n3) Swap in an FFT\u2010based convolution/correlation for large N, and use `np.count_nonzero` instead of `(\u2026>0).sum()`:  \n<<<<<<< SEARCH\n    # Compute sumset via convolution and difference via correlation\n    S = np.convolve(A_ind, B_ind, mode='full')\n    D = np.correlate(A_ind, B_ind, mode='full')\n    # Count unique sums and positive differences\n    s_count = (S > 0).sum()\n    d_count = (D > 0).sum()\n=======\n    # Compute sumset and difference via FFT\u2010fallback for larger N\n    length = len(A_ind) + len(B_ind) - 1\n    if len(A_ind) > 128:\n        fa = np.fft.rfft(A_ind, n=length)\n        fb = np.fft.rfft(B_ind, n=length)\n        S = (np.fft.irfft(fa * fb, n=length) > 0.5)\n        fa_full = np.fft.fft(A_ind, n=length)\n        fb_full = np.fft.fft(B_ind, n=length)\n        D = (np.fft.ifft(fa_full * np.conj(fb_full), n=length).real > 0.5)\n    else:\n        S = np.convolve(A_ind, B_ind, mode='full')\n        D = np.correlate(A_ind, B_ind, mode='full')\n    # Count unique sums and positive differences\n    s_count = np.count_nonzero(S)\n    d_count = np.count_nonzero(D)\n>>>>>>> REPLACE\n\n4) Remove the cache\u2010store at the end of `compute_ratio`:  \n<<<<<<< SEARCH\n    # Store result\n    _ratio_cache[key] = ratio\n    return ratio\n=======\n    # caching disabled\n    return ratio\n>>>>>>> REPLACE\n\n5) Switch from a linear cooling schedule to a simple exponential decay (often converges faster in practice):  \n<<<<<<< SEARCH\n        # linearly cooling: T = T0*(1 \u2212 iter/max_iter)\n        T = T0 * (1.0 - iter_num / max_iter)\n=======\n        # exponential cooling schedule\n        T = T0 * (0.99 ** iter_num)\n>>>>>>> REPLACE\n\nReasoning summary:\n\n\u2022 Removing the byte\u2010based cache eliminates a costly hash and memory\u2010growth path that rarely helps in a highly stochastic SA loop.  \n\u2022 FFT\u2010based convolution for N>128 reduces the asymptotic cost from O(N\u00b2) to O(N\u2009log\u2009N), while small N still uses direct conv/corr.  \n\u2022 `np.count_nonzero` on boolean arrays is slightly cheaper than `(\u2026>0).sum()`.  \n\u2022 An exponential cooling schedule often provides more gradual temperature decay, helping to escape plateaus earlier."]}}}