{"id": "142970ff-ee7f-4dfd-80f0-a8264e39f9d2", "code": "import numpy as np\nfrom numpy.random import default_rng\nimport numpy.fft as fft\n\n# FFT-based convolution helper for performance improvement\ndef conv_fft(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    \"\"\"Perform convolution using FFT, returning integer-valued result.\"\"\"\n    n = a.size + b.size - 1\n    fa = fft.fft(a, n)\n    fb = fft.fft(b, n)\n    return np.rint(np.real(fft.ifft(fa * fb))).astype(int)\n\n# For reproducibility\nrng = default_rng(42)\n# Probability for multi-bit flip moves to help escape local optima\nMULTI_FLIP_PROB = 0.05\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # sumset via FFT-based convolution\n    a_int = A_ind.astype(int)\n    b_int = B_ind.astype(int)\n    sums_conv = conv_fft(a_int, b_int)\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via FFT-based convolution with reversed B\n    diffs_conv = conv_fft(a_int, b_int[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add/remove bits; occasional multi-bit flips to escape local optima.\"\"\"\n    ones = np.nonzero(ind)[0]\n    zeros = np.nonzero(~ind)[0]\n    # Multi-bit flip with small probability\n    if rng.random() < MULTI_FLIP_PROB and ones.size > 0 and zeros.size > 0:\n        # Flip one 1->0 and one 0->1 in a single move\n        ind[rng.choice(ones)] = False\n        ind[rng.choice(zeros)] = True\n    else:\n        # Single-bit add or remove\n        # if too few ones or by random choice, add\n        if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n            ind[rng.choice(zeros)] = True\n        # else remove (only if more than one one)\n        elif ones.size > 1:\n            ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    for _ in range(max_iter):\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n        # propose a balanced add/remove move for A or B using helper\n        if rng.random() < 0.5:\n            C_ind = propose_move(C_ind)\n        else:\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        if ratio > best_ratio:\n            best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: A Memetic Evolutionary Framework for Maximizing the Sum\u2010to\u2010Difference Ratio in Indicator Vectors\n\nAbstract:\nWe propose a novel, implementable research plan to overcome the limitations of current single\u2010bit hill\u2010climbing methods in optimizing the sum\u2010to\u2010difference ratio |A\u2009+\u2009B|\u2009/\u2009|A\u2009\u2212\u2009B| for binary indicator vectors. Our objectives are to (1) break free from local optima inherent in simple random\u2010walk hill\u2010climbing, (2) accelerate convolutional evaluations for large vector lengths, and (3) incorporate proven mechanisms from evolutionary computation.\n\n1. Identified Weaknesses\n- Reliance on single\u2010bit flips leads to slow convergence and entrapment in suboptimal plateaux.\n- No mechanism to escape local optima once reached.\n- Direct convolution (O(N^2)) limits scalability to larger N.\n- Absence of population\u2010based diversity reduces exploration capacity.\n\n2. Novel Algorithmic Backbone\nWe introduce a memetic algorithm that combines:\n- A Genetic Algorithm (GA) core with binary\u2010vector crossover and adaptive mutation rates to maintain exploratory diversity.\n- Local refinement via hill\u2010climbing or simulated annealing applied selectively to top GA individuals (memetic component).\n- Tabu\u2010search\u2010inspired prevention of immediate bit\u2010reversal moves, avoiding short\u2010cycle revisits.\n\n3. Convolutional Evaluation Acceleration\n- Replace direct numpy.convolve calls with FFT\u2010based convolution (using numpy.fft or pyFFTW) to reduce per\u2010evaluation cost from O(N^2) to O(N\u2009log\u2009N), enabling batch evaluation of candidate populations.\n- Explore GPU offloading (via CuPy) for very large N, ensuring practical runtime for population sizes of several hundred.\n\n4. Adaptive and Hierarchical Search Enhancements\n- Multi\u2010scale move operators: allow 1\u2013k bit flips in one proposal to traverse landscape barriers.\n- Dynamic scheduling of mutation and local\u2010search intensity based on diversity and improvement rates.\n- Surrogate\u2010model guidance: train a lightweight regression model on past (A, B) pairs to predict promising regions of set\u2010space, seeding new offsprings.\n\n5. Implementation and Evaluation Plan\n- Develop a modular Python codebase leveraging numpy, scipy, and optionally CuPy.\n- Benchmark on N\u2009=\u200930 against the 1.2105 baseline, targeting a \u226510% improvement in best\u2010found ratio.\n- Ablation studies: quantify contributions of GA crossover, memetic local\u2010search, FFT acceleration, and surrogate seeding.\n- Release open\u2010source package with reproducible experiments and extensible operator APIs.\n\nExpected Outcomes\nWe anticipate our framework to:\n\u2022 Achieve significantly higher sum\u2010to\u2010difference ratios by escaping local minima.\n\u2022 Scale efficiently to larger N, broadening applicability in additive combinatorics research.\n\u2022 Provide a template for combining evolutionary and local\u2010search strategies in combinatorial optimization.\n\nBy integrating successful elements from genetic algorithms, memetic search, and convolutional acceleration, this proposal offers a concrete, rigorous pathway to surpass current performance metrics while maintaining originality and practical implementability."], "idea_reward": 6.0, "parent_id": "e91bc0e2-58b1-48d3-91aa-9b3333b4455b_migrant_4_migrant_3", "generation": 8, "timestamp": 1757790498.0561693, "iteration_found": 700, "metrics": {"score": 1.3}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace from numpy.random import default_rng with 10 lines\nChange 2: Replace 2 lines with 4 lines\nChange 3: Replace 6 lines with 8 lines\nChange 4: Replace 11 lines with 18 lines", "parent_metrics": {"score": 1.2105263157894737}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \n\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # sumset via convolution\n    sums_conv = np.convolve(A_ind.astype(int), B_ind.astype(int))\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via convolution with reversed B\n    diffs_conv = np.convolve(A_ind.astype(int), B_ind[::-1].astype(int))\n    num_diffs = np.count_nonzero(diffs_conv)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = np.nonzero(ind)[0]\n    zeros = np.nonzero(~ind)[0]\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    for _ in range(max_iter):\n        C_ind = best_A.copy()\n        D_ind = best_B.copy()\n        # propose a balanced add/remove move for A or B using helper\n        if rng.random() < 0.5:\n            C_ind = propose_move(C_ind)\n        else:\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        if ratio > best_ratio:\n            best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.2105\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5161\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5161\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5161)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5161)\n\n\n### Program 2 (Score: 1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# Hyperparameters for advanced local search\nMULTI_FLIP_PROB = 0.05      # chance to try a 2\u2010bit block swap instead of a single flip\nWORSE_ACCEPT_PROB = 0.01    # small probability to accept a non\u2010improving move\n# Performance tuning: switch to FFT at this problem size\nFFT_THRESHOLD = 512\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.0000)\n\n\n### Program 3 (Score: 1.3704)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.3704)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.3846)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: 1.2692)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n# Convolution binding removed; using direct index arithmetic in compute_ratio\n# convolve = np.convolve  # unused\n\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nTitle: A Memetic Evolutionary Framework for Maximizing the Sum\u2010to\u2010Difference Ratio in Indicator Vectors\n\nAbstract:\nWe propose a novel, implementable research plan to overcome the limitations of current single\u2010bit hill\u2010climbing methods in optimizing the sum\u2010to\u2010difference ratio |A\u2009+\u2009B|\u2009/\u2009|A\u2009\u2212\u2009B| for binary indicator vectors. Our objectives are to (1) break free from local optima inherent in simple random\u2010walk hill\u2010climbing, (2) accelerate convolutional evaluations for large vector lengths, and (3) incorporate proven mechanisms from evolutionary computation.\n\n1. Identified Weaknesses\n- Reliance on single\u2010bit flips leads to slow convergence and entrapment in suboptimal plateaux.\n- No mechanism to escape local optima once reached.\n- Direct convolution (O(N^2)) limits scalability to larger N.\n- Absence of population\u2010based diversity reduces exploration capacity.\n\n2. Novel Algorithmic Backbone\nWe introduce a memetic algorithm that combines:\n- A Genetic Algorithm (GA) core with binary\u2010vector crossover and adaptive mutation rates to maintain exploratory diversity.\n- Local refinement via hill\u2010climbing or simulated annealing applied selectively to top GA individuals (memetic component).\n- Tabu\u2010search\u2010inspired prevention of immediate bit\u2010reversal moves, avoiding short\u2010cycle revisits.\n\n3. Convolutional Evaluation Acceleration\n- Replace direct numpy.convolve calls with FFT\u2010based convolution (using numpy.fft or pyFFTW) to reduce per\u2010evaluation cost from O(N^2) to O(N\u2009log\u2009N), enabling batch evaluation of candidate populations.\n- Explore GPU offloading (via CuPy) for very large N, ensuring practical runtime for population sizes of several hundred.\n\n4. Adaptive and Hierarchical Search Enhancements\n- Multi\u2010scale move operators: allow 1\u2013k bit flips in one proposal to traverse landscape barriers.\n- Dynamic scheduling of mutation and local\u2010search intensity based on diversity and improvement rates.\n- Surrogate\u2010model guidance: train a lightweight regression model on past (A, B) pairs to pre...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are a few targeted diffs that introduce FFT\u2010based convolution for faster |A+B| and |A\u2212B| computations, and add a small chance of a \u201ctwo\u2010bit flip\u201d to help escape local minima. These should improve both runtime and solution quality without a full rewrite.\n\n<<<<<<< SEARCH\nfrom numpy.random import default_rng\n=======\nfrom numpy.random import default_rng\nimport numpy.fft as fft\n\n# FFT-based convolution helper for performance improvement\ndef conv_fft(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    \"\"\"Perform convolution using FFT, returning integer-valued result.\"\"\"\n    n = a.size + b.size - 1\n    fa = fft.fft(a, n)\n    fb = fft.fft(b, n)\n    return np.rint(np.real(fft.ifft(fa * fb))).astype(int)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n# For reproducibility\nrng = default_rng(42)\n=======\n# For reproducibility\nrng = default_rng(42)\n# Probability for multi-bit flip moves to help escape local optima\nMULTI_FLIP_PROB = 0.05\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    # sumset via convolution\n    sums_conv = np.convolve(A_ind.astype(int), B_ind.astype(int))\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via convolution with reversed B\n    diffs_conv = np.convolve(A_ind.astype(int), B_ind[::-1].astype(int))\n    num_diffs = np.count_nonzero(diffs_conv)\n=======\n    # sumset via FFT-based convolution\n    a_int = A_ind.astype(int)\n    b_int = B_ind.astype(int)\n    sums_conv = conv_fft(a_int, b_int)\n    num_sums = np.count_nonzero(sums_conv)\n    # diffset via FFT-based convolution with reversed B\n    diffs_conv = conv_fft(a_int, b_int[::-1])\n    num_diffs = np.count_nonzero(diffs_conv)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = np.nonzero(ind)[0]\n    zeros = np.nonzero(~ind)[0]\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n=======\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add/remove bits; occasional multi-bit flips to escape local optima.\"\"\"\n    ones = np.nonzero(ind)[0]\n    zeros = np.nonzero(~ind)[0]\n    # Multi-bit flip with small probability\n    if rng.random() < MULTI_FLIP_PROB and ones.size > 0 and zeros.size > 0:\n        # Flip one 1->0 and one 0->1 in a single move\n        ind[rng.choice(ones)] = False\n        ind[rng.choice(zeros)] = True\n    else:\n        # Single-bit add or remove\n        # if too few ones or by random choice, add\n        if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n            ind[rng.choice(zeros)] = True\n        # else remove (only if more than one one)\n        elif ones.size > 1:\n            ind[rng.choice(ones)] = False\n    return ind\n>>>>>>> REPLACE\n\nExplanation of changes:\n- **FFT convolution (conv_fft)** cuts down the convolution cost from O(N\u00b2) to O(N\u2009log\u2009N), which matters as N grows or when evaluating many candidates.\n- **MULTI_FLIP_PROB** and the two\u2010bit flip in `propose_move` help the local search avoid getting stuck in tight local optima by occasionally making larger jumps.\n- These are small, localized edits that should integrate directly into your current code."]}}}