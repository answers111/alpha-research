{"id": "b0baa765-ba17-4ae2-915d-748f3a89b3ce", "code": "import numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nexp = math.exp\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # arrays are already int8\n    A_arr = A_ind\n    B_arr = B_ind\n    # compute Minkowski sum and difference via convolution/correlation\n    conv = convolve_fn(A_arr, B_arr)\n    corr = correlate_fn(A_arr, B_arr, mode='full')\n    num_sums = np.count_nonzero(conv)\n    num_diffs = np.count_nonzero(corr)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move to maintain constant cardinality and improve exploration.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(ind == 0)\n    if ones.size and zeros.size:\n        # with small probability do a two-bit swap for larger jumps\n        if rng_random() < 0.1 and ones.size > 1 and zeros.size > 1:  # reduced multi-bit swaps\n            removes = rng.choice(ones, size=2, replace=False)\n            adds = rng.choice(zeros, size=2, replace=False)\n            ind[removes] = False\n            ind[adds] = True\n        else:\n            i_remove = rng.choice(ones)\n            i_add = rng.choice(zeros)\n            ind[i_remove] = False\n            ind[i_add] = True\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 20  # tuned for better explore/exploit balance\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=np.int8)\n    B_ind = np.zeros(N, dtype=np.int8)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.99995       # even slower cooling to deepen search before freezing out\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            r = rng_random()\n            # 10% chance to tweak both sets at once (escape plateaus),\n            # else split roughly 55/45 between A\u2010only and B\u2010only\n            if r < 0.10:\n                C_ind = propose_move(C_ind)\n                D_ind = propose_move(D_ind)\n            elif r < 0.55:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng_random() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: Surrogate\u2010Assisted, GPU\u2010Accelerated Memetic Search for Enhanced MSTD Ratios\n\nAbstract:\nWe propose a novel combinatorial\u2010optimization framework to improve the sum\u2010to\u2010difference ratio |A+B|/|A\u2212B| of two 0/1 indicator sets (MSTD ratio), building on existing simulated\u2010annealing and hill\u2010climbing methods that currently plateau at \u22481.5455. Our analysis identifies three primary limitations in the state\u2010of\u2010the\u2010art: (1) costly Python\u2010loop enumeration of Minkowski sums and differences, (2) premature convergence of single\u2010chain annealing, and (3) unguided random\u2010walk proposals with low directional bias.\n\nTo address (1), we replace nested loops with FFT\u2010based convolution and correlation of bit\u2010packed arrays on GPU, reducing the ratio\u2010evaluation cost from O(n^2) to O(n\u2009log\u2009n) and enabling 50\u00d7 faster throughput. For (2), we embed the local annealing search in a multi\u2010population memetic algorithm: each subpopulation evolves via genetic crossover and targeted \u201cmulti\u2010swap\u201d mutations, followed by short, high\u2010temperature annealing runs to refine promising individuals and preserve diversity. To mitigate (3), we develop an online surrogate model\u2014a lightweight graph\u2010convolutional network\u2014that is trained in situ on features of (A,B) pairs and their exact ratios. This surrogate quickly predicts the expected gain of candidate moves, allowing us to pre\u2010screen 90% of proposals without full evaluation and reserve the FFT\u2010accelerated ratio computation for only the top 10%.\n\nAdditional innovations include a tabu mechanism to prevent cycling among recent high\u2010density configurations and an adaptive multi\u2010armed\u2010bandit scheduler that allocates computational budget to the most fruitful mutation types. All components rely on standard Python libraries (NumPy, PyTorch) and are released as an open\u2010source package. We anticipate that this hybrid approach will (a) push achievable MSTD ratios beyond 1.60, (b) scale to larger universe sizes (N\u2009>\u200950), and (c) serve as a template for surrogate\u2010guided, GPU\u2010accelerated combinatorial searches in other discrete optimization domains."], "idea_reward": 6.5, "parent_id": "f9202256-5638-48f8-8eda-b619101dae7d_migrant_1_migrant_0_migrant_1", "generation": 19, "timestamp": 1757828062.5023854, "iteration_found": 1327, "metrics": {"score": 1.53125}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 18 lines with 16 lines\nChange 2: 'BATCH_SIZE = 30  # increased number of local proposals (higher diversity)' to 'BATCH_SIZE = 20  # tuned for better explore/exploit balance'\nChange 3: 'if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:  # higher chance for multi-bit swaps' to 'if rng_random() < 0.1 and ones.size > 1 and zeros.size > 1:  # reduced multi-bit swaps'\nChange 4: Replace 2 lines with exp = math.exp", "parent_metrics": {"score": 1.5454545454545454}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \n\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nexp = math.exp\ncnz = np.count_nonzero\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # arrays are already int8\n    A_arr = A_ind\n    B_arr = B_ind\n    # compute Minkowski sum and difference sets directly via small Python loops\n    A_idxs = np.flatnonzero(A_arr)\n    B_idxs = np.flatnonzero(B_arr)\n    sums = {i + j for i in A_idxs for j in B_idxs}\n    diffs = {i - j for i in A_idxs for j in B_idxs}\n    num_sums = len(sums)\n    num_diffs = len(diffs)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move to maintain constant cardinality and improve exploration.\"\"\"\n    ones = np.flatnonzero(ind)\n    zeros = np.flatnonzero(ind == 0)\n    if ones.size and zeros.size:\n        # with small probability do a two-bit swap for larger jumps\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:  # higher chance for multi-bit swaps\n            removes = rng.choice(ones, size=2, replace=False)\n            adds = rng.choice(zeros, size=2, replace=False)\n            ind[removes] = False\n            ind[adds] = True\n        else:\n            i_remove = rng.choice(ones)\n            i_add = rng.choice(zeros)\n            ind[i_remove] = False\n            ind[i_add] = True\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 30  # increased number of local proposals (higher diversity)\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=np.int8)\n    B_ind = np.zeros(N, dtype=np.int8)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.99995       # even slower cooling to deepen search before freezing out\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            r = rng_random()\n            # 10% chance to tweak both sets at once (escape plateaus),\n            # else split roughly 55/45 between A\u2010only and B\u2010only\n            if r < 0.10:\n                C_ind = propose_move(C_ind)\n                D_ind = propose_move(D_ind)\n            elif r < 0.55:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng_random() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.5455\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 2 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5455)\n\n\n### Program 3 (Score: -1.0000)\n```python\nimport numpy as np\n\n# For reproducibility, use NumPy\u2019s Generator\nrng = np.random.default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    # Use support\u2010based sums/differences for sparse vectors\n    a_pos = np.nonzero(A_ind)[0]\n    b_pos = np.nonzero(B_ind)[0]\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: 1.5161)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nTitle: Surrogate\u2010Assisted, GPU\u2010Accelerated Memetic Search for Enhanced MSTD Ratios\n\nAbstract:\nWe propose a novel combinatorial\u2010optimization framework to improve the sum\u2010to\u2010difference ratio |A+B|/|A\u2212B| of two 0/1 indicator sets (MSTD ratio), building on existing simulated\u2010annealing and hill\u2010climbing methods that currently plateau at \u22481.5455. Our analysis identifies three primary limitations in the state\u2010of\u2010the\u2010art: (1) costly Python\u2010loop enumeration of Minkowski sums and differences, (2) premature convergence of single\u2010chain annealing, and (3) unguided random\u2010walk proposals with low directional bias.\n\nTo address (1), we replace nested loops with FFT\u2010based convolution and correlation of bit\u2010packed arrays on GPU, reducing the ratio\u2010evaluation cost from O(n^2) to O(n\u2009log\u2009n) and enabling 50\u00d7 faster throughput. For (2), we embed the local annealing search in a multi\u2010population memetic algorithm: each subpopulation evolves via genetic crossover and targeted \u201cmulti\u2010swap\u201d mutations, followed by short, high\u2010temperature annealing runs to refine promising individuals and preserve diversity. To mitigate (3), we develop an online surrogate model\u2014a lightweight graph\u2010convolutional network\u2014that is trained in situ on features of (A,B) pairs and their exact ratios. This surrogate quickly predicts the expected gain of candidate moves, allowing us to pre\u2010screen 90% of proposals without full evaluation and reserve the FFT\u2010accelerated ratio computation for only the top 10%.\n\nAdditional innovations include a tabu mechanism to prevent cycling among recent high\u2010density configurations and an adaptive multi\u2010armed\u2010bandit scheduler that allocates computational budget to the most fruitful mutation types. All components rely on standard Python libraries (NumPy, PyTorch) and are released as an open\u2010source package. We anticipate that this hybrid approach will (a) push achievable MSTD ratios beyond 1.60, (b) scale to larger universe sizes (N\u2009>\u200950), and (c) serve as a template for surrogate\u2010guided, GP...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are a few small, targeted changes to speed up the inner ratio\u2010computation (and remove the Python\u2010loop bottleneck), slightly reduce batch size for a better explore/exploit balance, and tone down the multi\u2010swap probability to match the previous top\u2010performing behavior. Each diff strictly follows the SEARCH/REPLACE format.\n\n1) Replace the manual set\u2010based Minkowski\u2010sum/diff with convolution/correlation:\n\n<<<<<<< SEARCH\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # arrays are already int8\n    A_arr = A_ind\n    B_arr = B_ind\n    # compute Minkowski sum and difference sets directly via small Python loops\n    A_idxs = np.flatnonzero(A_arr)\n    B_idxs = np.flatnonzero(B_arr)\n    sums = {i + j for i in A_idxs for j in B_idxs}\n    diffs = {i - j for i in A_idxs for j in B_idxs}\n    num_sums = len(sums)\n    num_diffs = len(diffs)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n=======\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    # arrays are already int8\n    A_arr = A_ind\n    B_arr = B_ind\n    # compute Minkowski sum and difference via convolution/correlation\n    conv = convolve_fn(A_arr, B_arr)\n    corr = correlate_fn(A_arr, B_arr, mode='full')\n    num_sums = np.count_nonzero(conv)\n    num_diffs = np.count_nonzero(corr)\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n>>>>>>> REPLACE\n\nThis removes the O(n\u00b2) Python loops and leverages the prebound `convolve_fn`/`correlate_fn` for an O(n\u2009log\u2009n) or optimized path.\n\n2) Reduce the batch size from 30 to 20 to slightly speed up each iteration and avoid diminishing returns:\n\n<<<<<<< SEARCH\nBATCH_SIZE = 30  # increased number of local proposals (higher diversity)\n=======\nBATCH_SIZE = 20  # tuned for better explore/exploit balance\n>>>>>>> REPLACE\n\n3) Tone down the multi\u2010swap probability from 0.3 back to 0.1, matching earlier successful runs:\n\n<<<<<<< SEARCH\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:  # higher chance for multi-bit swaps\n=======\n        if rng_random() < 0.1 and ones.size > 1 and zeros.size > 1:  # reduced multi-bit swaps\n>>>>>>> REPLACE\n\n4) Finally, remove the unused alias for `np.count_nonzero` to keep the code clean:\n\n<<<<<<< SEARCH\nexp = math.exp\ncnz = np.count_nonzero\n=======\nexp = math.exp\n>>>>>>> REPLACE\n\nTogether, these tweaks should give you a substantial speed boost on `compute_ratio` (now leveraging vectorized routines), slightly tighter annealing/HC parameters, and improved code readability/maintainability."]}}}