{"id": "d77dd970-31da-4084-bf82-e39b0985e10f", "code": "import numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Removed caching to reduce per\u2010call overhead\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nrng_choice = rng.choice\nexp = math.exp\ncnz = np.count_nonzero\nany_fn = np.any\nflatnonzero_fn = np.flatnonzero\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check for empty sets\n    if not any_fn(A_ind) or not any_fn(B_ind):\n        return -1.0\n    num_sums = cnz(convolve_fn(A_ind, B_ind))\n    num_diffs = cnz(correlate_fn(A_ind, B_ind, mode='full'))\n    return -1.0 if num_diffs == 0 else num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move to maintain constant cardinality and improve exploration.\"\"\"\n    ones = flatnonzero_fn(ind)\n    zeros = flatnonzero_fn(1 - ind)\n    if ones.size and zeros.size:\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:\n            removes = rng_choice(ones, size=2, replace=False)\n            adds = rng_choice(zeros, size=2, replace=False)\n            ind[removes] = 0\n            ind[adds] = 1\n        else:\n            i_remove = rng_choice(ones)\n            i_add = rng_choice(zeros)\n            ind[i_remove] = 0\n            ind[i_add] = 1\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 20  # increased number of local proposals for better exploration\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    # use boolean arrays for lighter\u2010weight, faster bit\u2010ops\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = 1\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9999        # even slower cooling to escape local optima\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    # Pre-bind heavy functions once per outer loop\n    _compute_ratio = compute_ratio\n    _rand        = rng_random\n    _exp         = exp\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            r = _rand()\n            # With some probability, mutate both for deeper exploration\n            if r < 0.2:\n                C_ind = propose_move(C_ind)\n                D_ind = propose_move(D_ind)\n            elif r < 0.6:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = _compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or _rand() < _exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: A Hybrid Evolutionary-Surrogate Framework for Maximizing Sum\u2010to\u2010Difference Ratios in Binary Indicator Sets\n\nAbstract:\nWe address key limitations of existing hill\u2010climbing and simulated\u2010annealing methods for maximizing the sum-to-difference ratio |A+B|/|A\u2212B| over 0/1 indicator vectors: slow convergence, vulnerability to local optima, and high per\u2010evaluation cost. Building on successful elements of batch proposal schemes and adaptive cooling, we propose a novel, implementable framework that combines evolutionary search, learned surrogate models, and GPU\u2010accelerated ratio estimation.\n\n1. Identified Weaknesses  \n\u2022 Local\u2010optima stagnation under simple bit\u2010swap proposals and single\u2010chain annealing schedules.  \n\u2022 High computational overhead from repeated full convolutions and correlations in ratio evaluations.  \n\u2022 Limited diversity in proposals due to fixed\u2010size batch sampling from the current state.\n\n2. Technical Innovations  \n\u2022 Surrogate\u2010Accelerated Evaluation: Train a lightweight neural or Gaussian\u2010process surrogate to predict the sum\u2010to\u2010difference ratio from low\u2010dimensional summary statistics of (A,B). Use the surrogate to pre\u2010screen candidate moves, invoking the exact conv/corr only on top\u2010K proposals per batch.  \n\u2022 Multi\u2010Population Evolutionary Scheme: Maintain multiple subpopulations of (A,B) vectors evolving in parallel, each using specialized mutation operators (1-swap, 2-swap, block exchange). Periodically exchange elite individuals across populations to escape local traps.  \n\u2022 Adaptive Proposal Distribution: Instead of uniform random swaps, learn a per\u2010bit importance weight via reinforcement\u2010learning\u2013style credit assignment, biasing moves towards positions historically contributing most to ratio improvements.  \n\u2022 GPU\u2010Accelerated Convolutions: Implement convolution/correlation ratio computation using cuFFT or tensor\u2010core\u2013optimized dot\u2010products, reducing per\u2010evaluation latency by up to 10\u00d7 for large N.  \n\u2022 Dynamic Cooling and Restart Strategies: Replace fixed exponential decay with an adaptive schedule that increases temperature upon convergence stalls and triggers controlled random\u2010restart from elite solutions.\n\n3. Expected Outcomes  \nOur experiments on N=30\u2013100 dimensional indicator sets anticipate:  \n\u2022 30\u201350% faster convergence to high\u2010quality solutions (ratio >1.7) under fixed computational budgets.  \n\u2022 Improved final ratios (target \u22651.8) through effective exploration of the search space.  \n\u2022 Scalable performance via GPU acceleration, enabling real\u2010time parameter sweeps.\n\n4. Implementation Plan  \nPhase 1: Develop the surrogate model pipeline and validate its prediction accuracy on random (A,B) samples.  \nPhase 2: Integrate surrogate screening into a multi\u2010population genetic algorithm, benchmark against the existing hill\u2010climbing code.  \nPhase 3: Implement GPU kernels for accelerated ratio computation and adaptive cooling controls.  \nPhase 4: Conduct ablation studies on mutation operators and temperature schedules; finalize parameter settings for robust performance.\n\nThis hybrid approach leverages proven evolutionary strategies, modern surrogate modeling, and hardware acceleration to overcome the current method\u2019s bottlenecks, promising significant improvements in both solution quality and runtime efficiency."], "idea_reward": 6.2, "parent_id": "2a717284-57de-4ebe-b3c5-b7d9b857335d_migrant_1", "generation": 19, "timestamp": 1757818455.9007201, "iteration_found": 1207, "metrics": {"score": 1.5454545454545454}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 2 lines with # Removed caching to reduce per\u2010call overhead\nChange 2: Replace 14 lines with 8 lines\nChange 3: Replace 2 lines with 3 lines\nChange 4: Replace 3 lines with 7 lines\nChange 5: 'r = rng_random()' to 'r = _rand()'\nChange 6: 'ratio = compute_ratio(C_ind, D_ind)' to 'ratio = _compute_ratio(C_ind, D_ind)'\nChange 7: 'if delta > 0 or rng_random() < exp(delta / T):' to 'if delta > 0 or _rand() < _exp(delta / T):'", "parent_metrics": {"score": 1.5454545454545454}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \n\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Caching dictionary to memoize ratio computations\nratio_cache = {}\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nrng_choice = rng.choice\nexp = math.exp\ncnz = np.count_nonzero\nany_fn = np.any\nflatnonzero_fn = np.flatnonzero\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in ratio_cache:\n        return ratio_cache[key]\n    # quick check\n    if not any_fn(A_ind) or not any_fn(B_ind):\n        ratio = -1.0\n    else:\n        num_sums = cnz(convolve_fn(A_ind, B_ind))\n        num_diffs = cnz(correlate_fn(A_ind, B_ind, mode='full'))\n        ratio = -1.0 if num_diffs == 0 else num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move to maintain constant cardinality and improve exploration.\"\"\"\n    ones = flatnonzero_fn(ind)\n    zeros = flatnonzero_fn(1 - ind)\n    if ones.size and zeros.size:\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:\n            removes = rng_choice(ones, size=2, replace=False)\n            adds = rng_choice(zeros, size=2, replace=False)\n            ind[removes] = 0\n            ind[adds] = 1\n        else:\n            i_remove = rng_choice(ones)\n            i_add = rng_choice(zeros)\n            ind[i_remove] = 0\n            ind[i_add] = 1\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 20  # increased number of local proposals for better exploration\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=np.int8)\n    B_ind = np.zeros(N, dtype=np.int8)\n    A_ind[CONWAY_MSTD_INIT] = 1\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9999        # even slower cooling to escape local optima\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            r = rng_random()\n            # With some probability, mutate both for deeper exploration\n            if r < 0.2:\n                C_ind = propose_move(C_ind)\n                D_ind = propose_move(D_ind)\n            elif r < 0.6:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng_random() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.5455\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 2 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\n# Cache for computed ratios to avoid redundant convolution calls\nratio_cache = {}\n\n# For reproducibility and performance\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 3 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\n# Cache for computed ratios to avoid redundant convolution calls\nratio_cache = {}\n\n# For reproducibility and performance\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5455)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.3000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: 1.3704)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math  # added for simulated annealing acceptance\n\n# For reproducibility\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nTitle: A Hybrid Evolutionary-Surrogate Framework for Maximizing Sum\u2010to\u2010Difference Ratios in Binary Indicator Sets\n\nAbstract:\nWe address key limitations of existing hill\u2010climbing and simulated\u2010annealing methods for maximizing the sum-to-difference ratio |A+B|/|A\u2212B| over 0/1 indicator vectors: slow convergence, vulnerability to local optima, and high per\u2010evaluation cost. Building on successful elements of batch proposal schemes and adaptive cooling, we propose a novel, implementable framework that combines evolutionary search, learned surrogate models, and GPU\u2010accelerated ratio estimation.\n\n1. Identified Weaknesses  \n\u2022 Local\u2010optima stagnation under simple bit\u2010swap proposals and single\u2010chain annealing schedules.  \n\u2022 High computational overhead from repeated full convolutions and correlations in ratio evaluations.  \n\u2022 Limited diversity in proposals due to fixed\u2010size batch sampling from the current state.\n\n2. Technical Innovations  \n\u2022 Surrogate\u2010Accelerated Evaluation: Train a lightweight neural or Gaussian\u2010process surrogate to predict the sum\u2010to\u2010difference ratio from low\u2010dimensional summary statistics of (A,B). Use the surrogate to pre\u2010screen candidate moves, invoking the exact conv/corr only on top\u2010K proposals per batch.  \n\u2022 Multi\u2010Population Evolutionary Scheme: Maintain multiple subpopulations of (A,B) vectors evolving in parallel, each using specialized mutation operators (1-swap, 2-swap, block exchange). Periodically exchange elite individuals across populations to escape local traps.  \n\u2022 Adaptive Proposal Distribution: Instead of uniform random swaps, learn a per\u2010bit importance weight via reinforcement\u2010learning\u2013style credit assignment, biasing moves towards positions historically contributing most to ratio improvements.  \n\u2022 GPU\u2010Accelerated Convolutions: Implement convolution/correlation ratio computation using cuFFT or tensor\u2010core\u2013optimized dot\u2010products, reducing per\u2010evaluation latency by up to 10\u00d7 for large N.  \n\u2022 Dynamic Cooling and Restart Strategies: Replace fix...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are a few small, targeted diffs that together should reduce per\u2010iteration overhead (by cutting out the expensive cache machinery and avoiding some global lookups) and also switch to a more compact boolean representation for your indicator arrays.  Each change is accompanied by a brief rationale.\n\n1) Remove the heavy caching logic in `compute_ratio`.  In practice nearly every proposal is new, so the cost of building the dict key often outweighs any cache hit.  \n2) Change your indicator arrays from `int8` to `bool` \u2014 fewer bytes, faster bit\u2010ops.  \n3) Pre\u2010bind the three hot functions (`compute_ratio`, `rng_random`, `exp`) once before your inner loop so you only pay the global lookup penalty once per outer iteration.\n\n```diff\n<<<<<<< SEARCH\n# Caching dictionary to memoize ratio computations\nratio_cache = {}\n=======\n# Removed caching to reduce per\u2010call overhead\n>>>>>>> REPLACE\n```\n\n```diff\n<<<<<<< SEARCH\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in ratio_cache:\n        return ratio_cache[key]\n    # quick check\n    if not any_fn(A_ind) or not any_fn(B_ind):\n        ratio = -1.0\n    else:\n        num_sums = cnz(convolve_fn(A_ind, B_ind))\n        num_diffs = cnz(correlate_fn(A_ind, B_ind, mode='full'))\n        ratio = -1.0 if num_diffs == 0 else num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n=======\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check for empty sets\n    if not any_fn(A_ind) or not any_fn(B_ind):\n        return -1.0\n    num_sums = cnz(convolve_fn(A_ind, B_ind))\n    num_diffs = cnz(correlate_fn(A_ind, B_ind, mode='full'))\n    return -1.0 if num_diffs == 0 else num_sums / num_diffs\n>>>>>>> REPLACE\n```\n\n```diff\n<<<<<<< SEARCH\n    A_ind = np.zeros(N, dtype=np.int8)\n    B_ind = np.zeros(N, dtype=np.int8)\n=======\n    # use boolean arrays for lighter\u2010weight, faster bit\u2010ops\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n>>>>>>> REPLACE\n```\n\n```diff\n<<<<<<< SEARCH\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    for _ in range(max_iter):\n=======\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    # Pre-bind heavy functions once per outer loop\n    _compute_ratio = compute_ratio\n    _rand        = rng_random\n    _exp         = exp\n    for _ in range(max_iter):\n>>>>>>> REPLACE\n```\n\n```diff\n<<<<<<< SEARCH\n            r = rng_random()\n=======\n            r = _rand()\n>>>>>>> REPLACE\n```\n\n```diff\n<<<<<<< SEARCH\n            ratio = compute_ratio(C_ind, D_ind)\n=======\n            ratio = _compute_ratio(C_ind, D_ind)\n>>>>>>> REPLACE\n```\n\n```diff\n<<<<<<< SEARCH\n        if delta > 0 or rng_random() < exp(delta / T):\n=======\n        if delta > 0 or _rand() < _exp(delta / T):\n>>>>>>> REPLACE\n```\n\nExplanation of benefits:\n\n- Dropping the cache removes both the `tobytes()` allocations and the dictionary\u2010lookup bookkeeping on almost every proposal.  \n- Switching to `bool` cuts memory bandwidth and gives NumPy a more natural type for 0/1 operations.  \n- Pulling out `compute_ratio`, `rng_random`, and `exp` into local names avoids repeated global\u2010variable lookups inside the main search loop.  \n\nThese modest tweaks often add up to measurable speed\u2010ups in an inner\u2010heavy Metropolis/hill\u2010climbing routine without substantially altering the algorithmic structure."]}}}