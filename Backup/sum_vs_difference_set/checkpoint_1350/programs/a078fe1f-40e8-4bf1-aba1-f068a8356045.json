{"id": "a078fe1f-40e8-4bf1-aba1-f068a8356045", "code": "import numpy as np\nconvolve = np.convolve\nflatnonzero = np.flatnonzero\nfrom numpy.random import default_rng\nimport math\nimport numpy.fft as fft\n\n# threshold for switching to FFT-based convolution\nFFT_THRESHOLD = 128\n\ndef fft_convolve(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    n = a.size + b.size - 1\n    fa = fft.rfft(a, n)\n    fb = fft.rfft(b, n)\n    conv = fft.irfft(fa * fb, n)\n    return np.rint(conv).astype(int)\n\n# For reproducibility\nrng = default_rng(42)\n# cache to avoid redundant ratio computations\n_ratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # check cache for redundant computations using sparse index tuples\n    idxA = tuple(flatnonzero(A_ind))\n    idxB = tuple(flatnonzero(B_ind))\n    key = (idxA, idxB)\n    if key in _ratio_cache:\n        return _ratio_cache[key]\n    # convert to int8 bit vectors for convolution-based counts\n    A_int = A_ind.astype(np.int8)\n    B_int = B_ind.astype(np.int8)\n    total_len = A_int.size + B_int.size - 1\n    # choose convolution method based on size threshold\n    if total_len > FFT_THRESHOLD:\n        conv_s = fft_convolve(A_int, B_int)\n    else:\n        conv_s = convolve(A_int, B_int)\n    sum_count = np.count_nonzero(conv_s)\n    # |A\u2212B| count via convolution with reversed B\n    B_rev = B_int[::-1]\n    if total_len > FFT_THRESHOLD:\n        conv_d = fft_convolve(A_int, B_rev)\n    else:\n        conv_d = convolve(A_int, B_rev)\n    diff_count = np.count_nonzero(conv_d)\n    if diff_count == 0:\n        _ratio_cache[key] = -1.0\n        return -1.0\n    ratio = sum_count / diff_count\n    _ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = flatnonzero(ind)\n    zeros = flatnonzero(~ind)\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    # pre\u2010bind hot functions\n    random = rng.random\n    exp = math.exp\n    for _ in range(max_iter):\n        # propose a balanced add/remove move by copying only one side\n        if random() < 0.5:\n            C_ind = current_A.copy()\n            D_ind = current_B\n            C_ind = propose_move(C_ind)\n        else:\n            C_ind = current_A\n            D_ind = current_B.copy()\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or random() < exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: Multi-Scale Contrastive Graph Transformer\u2013Guided Hybrid RL\u2013Evolutionary Framework with Differentiable Macro-Induction and Probabilistic Streaming Approximation for Scalable Sum-to-Difference Maximization\n\nAbstract:\nDespite recent gains from surrogate-guided PPO and hierarchical search, existing methods plateau at a sum-to-difference ratio of \u22481.38 for large N, due to (i) static surrogates that misalign with high-dimensional dynamics, (ii) greedy one-step flips that fail to capture longer-range dependencies, and (iii) exact ratio computations that become a runtime bottleneck. We propose a four\u2010pronged, implementable framework to surpass a 1.50 ratio while preserving sublinear scaling:\n\n1. Multi-Scale Contrastive Graph Transformer Surrogate\n \u2022 Pretrain a Graph Transformer on small-N instances via contrastive learning to capture motif embeddings; progressively fine-tune on larger N using layer-wise domain adaptation losses.\n \u2022 Incorporate an uncertainty-aware selector that routes only high-ambiguity queries to exact ratio calls, yielding amortized O(log\u2009N) surrogate updates.\n\n2. Hybrid RL\u2013Evolutionary Macro Move Planner\n \u2022 Merge on-policy actor-critic RL with a customized evolutionary crossover: learned bit-motif representations guide recombination of elite solution pools.\n \u2022 Use straight-through Gumbel-Softmax to induce differentiable macro-flip templates, enabling backpropagation through k-step planning.\n\n3. Differentiable Macro-Induction via Variational Motif Grammar\n \u2022 Model recurring block patterns with a hierarchical variational autoencoder that outputs a compact grammar of macro-moves.\n \u2022 Train a secondary critic to value-rank these macros, dynamically balancing their selection against fine-grained flips based on expected improvement.\n\n4. Probabilistic Streaming Approximation for Ratio Evaluation\n \u2022 Develop a Monte Carlo bit-sampling estimator that approximates |A\u2009\u00b1\u2009B| in O(1) expected time per update, with tunable confidence bounds.\n \u2022 Fall back on a GPU-accelerated, Fenwick-tree\u2013backed exact engine in <5% of cases, reducing full recomputation by >90%.\n\nWe will implement in JAX/PyTorch with custom CUDA modules and benchmark on N\u2208{100,500,1\u2009000,5\u2009000,10\u2009000}. By combining multi-scale surrogates, hybrid planning, differentiable macro induction, and probabilistic streaming, we expect to exceed a 1.50 sum-to-difference ratio and maintain sublinear runtime growth."], "idea_reward": 5.67, "parent_id": "16ce5685-0ad0-49df-9647-4a871ac978d5", "generation": 15, "timestamp": 1757801864.8148322, "iteration_found": 967, "metrics": {"score": 1.3846153846153846}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 5 lines with 16 lines\nChange 2: Replace 9 lines with 16 lines", "parent_metrics": {"score": 1.3846153846153846}, "island": 4}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: Adaptive Graph-Surrogate Guided Hierarchical RL with Dynamic Macro-Move Induction and Streaming Ratio Computation for Scalable Sum-to-Difference Maximization\n\nAbstract:\nCurrent hill-climbing and surrogate-guided PPO methods plateau at a sum-to-difference score of \u22481.38, hindered by static surrogates misaligned for large N, greedy one-step move policies, and costly ratio evaluations. We propose an end-to-end framework that overcomes these bottlenecks through four key innovations:\n\n1. Adaptive Graph-Surrogate with Continual Domain Alignment  \n   \u2022 A graph neural network surrogate is meta-trained on a curriculum of small to mid-sized N and continually fine-tuned online via adversarial domain-shift detection.  \n   \u2022 A multi-fidelity evaluation controller engages high-precision ratio computations only in high-uncertainty regions, reducing unnecessary cost.\n\n2. Hierarchical Model-Based RL with Deep Multi-Step Planning  \n   \u2022 A learned transition model of bit-move dynamics is coupled with an MCTS planner accelerated by a value-function critic, enabling k-step lookahead on local flips and grammar-driven macro moves.  \n   \u2022 Intrinsic exploration bonuses are derived from the surrogate\u2019s epistemic uncertainty and novelty in state-action visitation.\n\n3. Dynamic Macro-Move Induction via Motif Priors  \n   \u2022 We automatically mine recurring substructure motifs from elite solutions using Bayesian nonparametrics, forming a compact grammar of block-flip templates.  \n   \u2022 A contextual policy network dynamically balances execution of induced macro moves versus fine-grained flips, guided by a learned utility predictor.\n\n4. Streaming Ratio Computation with Bit-Parallel Pipelines  \n   \u2022 We implement an incremental update engine that maintains Fenwick-tree indices and GPU-accelerated bitset convolutions, delivering amortized O(log\u2009N) update time and 10\u00d7 speed-up over full recomputation.\n\nWe will prototype in JAX/PyTorch with custom CUDA kernels, evaluating on N\u2208{50,100,200,500,1\u2009000,...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nconvolve = np.convolve\nflatnonzero = np.flatnonzero\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n# cache to avoid redundant ratio computations\n_ratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # check cache for redundant computations using sparse index tuples\n    idxA = tuple(flatnonzero(A_ind))\n    idxB = tuple(flatnonzero(B_ind))\n    key = (idxA, idxB)\n    if key in _ratio_cache:\n        return _ratio_cache[key]\n    # convert to int8 bit vectors for convolution-based counts\n    A_int = A_ind.astype(np.int8)\n    B_int = B_ind.astype(np.int8)\n    # |A+B| count via direct convolution\n    conv_s = np.convolve(A_int, B_int)\n    sum_count = np.count_nonzero(conv_s)\n    # |A\u2212B| count via convolution with reversed B\n    B_rev = B_int[::-1]\n    conv_d = np.convolve(A_int, B_rev)\n    diff_count = np.count_nonzero(conv_d)\n    if diff_count == 0:\n        _ratio_cache[key] = -1.0\n        return -1.0\n    ratio = sum_count / diff_count\n    _ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = flatnonzero(ind)\n    zeros = flatnonzero(~ind)\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    # pre\u2010bind hot functions\n    random = rng.random\n    exp = math.exp\n    for _ in range(max_iter):\n        # propose a balanced add/remove move by copying only one side\n        if random() < 0.5:\n            C_ind = current_A.copy()\n            D_ind = current_B\n            C_ind = propose_move(C_ind)\n        else:\n            C_ind = current_A\n            D_ind = current_B.copy()\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or random() < exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.3846\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5455\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5455\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 2 (Score: 1.4333)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math  # added for simulated annealing acceptance\nexp = math.exp\n\n# FFT\u2010based convolution for O(N log N) ratio evaluations\nimport numpy.fft as fft\ndef fft_convolve(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    if a.size < 64:  # use direct convolution for small sizes\n        return np.convolve(a.astype(int), b.astype(int))\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.4333)\n\n\n### Program 3 (Score: 1.3846)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.3846)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.3529)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# pre-bind heavy functions for faster access inside compute_ratio\nconv = np.convolve\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n# Initialize random number generator for reproducibility\nrng = default_rng(42)\n\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n# Current Proposal\nTitle: Multi-Scale Contrastive Graph Transformer\u2013Guided Hybrid RL\u2013Evolutionary Framework with Differentiable Macro-Induction and Probabilistic Streaming Approximation for Scalable Sum-to-Difference Maximization\n\nAbstract:\nDespite recent gains from surrogate-guided PPO and hierarchical search, existing methods plateau at a sum-to-difference ratio of \u22481.38 for large N, due to (i) static surrogates that misalign with high-dimensional dynamics, (ii) greedy one-step flips that fail to capture longer-range dependencies, and (iii) exact ratio computations that become a runtime bottleneck. We propose a four\u2010pronged, implementable framework to surpass a 1.50 ratio while preserving sublinear scaling:\n\n1. Multi-Scale Contrastive Graph Transformer Surrogate\n \u2022 Pretrain a Graph Transformer on small-N instances via contrastive learning to capture motif embeddings; progressively fine-tune on larger N using layer-wise domain adaptation losses.\n \u2022 Incorporate an uncertainty-aware selector that routes only high-ambiguity queries to exact ratio calls, yielding amortized O(log\u2009N) surrogate updates.\n\n2. Hybrid RL\u2013Evolutionary Macro Move Planner\n \u2022 Merge on-policy actor-critic RL with a customized evolutionary crossover: learned bit-motif representations guide recombination of elite solution pools.\n \u2022 Use straight-through Gumbel-Softmax to induce differentiable macro-flip templates, enabling backpropagation through k-step planning.\n\n3. Differentiable Macro-Induction via Variational Motif Grammar\n \u2022 Model recurring block patterns with a hierarchical variational autoencoder that outputs a compact grammar of macro-moves.\n \u2022 Train a secondary critic to value-rank these macros, dynamically balancing their selection against fine-grained flips based on expected improvement.\n\n4. Probabilistic Streaming Approximation for Ratio Evaluation\n \u2022 Develop a Monte Carlo bit-sampling estimator that approximates |A\u2009\u00b1\u2009B| in O(1) expected time per update, with tunable confidence bounds.\n \u2022 Fall back on a GPU...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["I\u2019m introducing an FFT\u2010based fallback for large convolutions in `compute_ratio`, which will reduce the amortized cost of the |A\u00b1B| counts for bigger N. This should significantly speed up the inner\u2010loop evaluations and allow more search iterations, boosting the sum\u2010to\u2010difference score.\n\n<<<<<<< SEARCH\nimport numpy as np\nconvolve = np.convolve\nflatnonzero = np.flatnonzero\nfrom numpy.random import default_rng\nimport math\n=======\nimport numpy as np\nconvolve = np.convolve\nflatnonzero = np.flatnonzero\nfrom numpy.random import default_rng\nimport math\nimport numpy.fft as fft\n\n# threshold for switching to FFT-based convolution\nFFT_THRESHOLD = 128\n\ndef fft_convolve(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    n = a.size + b.size - 1\n    fa = fft.rfft(a, n)\n    fb = fft.rfft(b, n)\n    conv = fft.irfft(fa * fb, n)\n    return np.rint(conv).astype(int)\n>>>>>>> REPLACE\n\n\u2014and inside `compute_ratio`, replace the direct `np.convolve` calls with a size\u2010check that dispatches to either the existing direct convolution or the new FFT path:\n\n<<<<<<< SEARCH\n    # convert to int8 bit vectors for convolution-based counts\n    A_int = A_ind.astype(np.int8)\n    B_int = B_ind.astype(np.int8)\n    # |A+B| count via direct convolution\n    conv_s = np.convolve(A_int, B_int)\n    sum_count = np.count_nonzero(conv_s)\n    # |A\u2212B| count via convolution with reversed B\n    B_rev = B_int[::-1]\n    conv_d = np.convolve(A_int, B_rev)\n=======\n    # convert to int8 bit vectors for convolution-based counts\n    A_int = A_ind.astype(np.int8)\n    B_int = B_ind.astype(np.int8)\n    total_len = A_int.size + B_int.size - 1\n    # choose convolution method based on size threshold\n    if total_len > FFT_THRESHOLD:\n        conv_s = fft_convolve(A_int, B_int)\n    else:\n        conv_s = convolve(A_int, B_int)\n    sum_count = np.count_nonzero(conv_s)\n    # |A\u2212B| count via convolution with reversed B\n    B_rev = B_int[::-1]\n    if total_len > FFT_THRESHOLD:\n        conv_d = fft_convolve(A_int, B_rev)\n    else:\n        conv_d = convolve(A_int, B_rev)\n>>>>>>> REPLACE\n\nThese changes should reduce costly full\u2010size convolutions for large N, speeding up `compute_ratio` and thus the overall hill\u2010climbing loop."]}}}