{"id": "9520f35f-fd9f-49db-8808-a95e3e074123", "code": "import numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Removed caching to reduce per\u2010call overhead\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nrng_choice = rng.choice\nexp = math.exp\ncnz = np.count_nonzero\nany_fn = np.any\nflatnonzero_fn = np.flatnonzero\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B| using Python sets. Return \u22121.0 if infeasible.\"\"\"\n    # quick check for empty sets\n    if not any_fn(A_ind) or not any_fn(B_ind):\n        return -1.0\n    # extract indices of ones\n    A_idxs = flatnonzero_fn(A_ind)\n    B_idxs = flatnonzero_fn(B_ind)\n    # compute sumset and diffset via Python sets for small N\n    sums = {i + j for i in A_idxs for j in B_idxs}\n    diffs = {i - j for i in A_idxs for j in B_idxs}\n    num_sums = len(sums)\n    num_diffs = len(diffs)\n    return -1.0 if num_diffs == 0 else num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray, ones: np.ndarray, zeros: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move using precomputed one- and zero-indices.\"\"\"\n    if ones.size and zeros.size:\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:\n            removes = rng_choice(ones, size=2, replace=False)\n            adds = rng_choice(zeros, size=2, replace=False)\n            ind[removes] = False\n            ind[adds] = True\n        else:\n            i_remove = rng_choice(ones)\n            i_add = rng_choice(zeros)\n            ind[i_remove] = False\n            ind[i_add] = True\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 20  # increased number of local proposals for better exploration\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    # use boolean arrays for lighter\u2010weight, faster bit\u2010ops\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = 1\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    # Initialize counters for adaptive temperature control\n    no_improve = 0\n    plateau_limit = 5000  # iterations to trigger a temperature boost\n    decay = 0.9999        # even slower cooling to escape local optima\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    # Pre-bind heavy functions once per outer loop\n    _compute_ratio = compute_ratio\n    _rand        = rng_random\n    _exp         = exp\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        # Precompute indices once per outer iteration\n        ones_A = flatnonzero_fn(current_A)\n        zeros_A = flatnonzero_fn(~current_A)\n        ones_B = flatnonzero_fn(current_B)\n        zeros_B = flatnonzero_fn(~current_B)\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            r = _rand()\n            # With some probability, mutate both for deeper exploration\n            if r < 0.2:\n                C_ind = propose_move(C_ind, ones_A, zeros_A)\n                D_ind = propose_move(D_ind, ones_B, zeros_B)\n            elif r < 0.6:\n                C_ind = propose_move(C_ind, ones_A, zeros_A)\n            else:\n                D_ind = propose_move(D_ind, ones_B, zeros_B)\n            ratio = _compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or _rand() < _exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best and manage no\u2010improvement counter\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n            no_improve = 0\n        else:\n            no_improve += 1\n        # adaptive temperature boost when stuck\n        if no_improve > plateau_limit:\n            T *= 1.5       # momentarily increase T to escape plateau\n            no_improve = 0\n        # regular cooldown\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: A Multi\u2010Fidelity Bayesian\u2010Ensemble Graph\u2010Surrogate and Adaptive Deep\u2010RL\u2010Evolutionary Framework for Optimizing Sum-to-Difference Ratios in Binary Indicator Sets\n\nAbstract:\nDespite recent advances, hill-climbing and simulated-annealing methods for maximizing the sum-to-difference ratio |A+B|/|A\u2013B| on N-dimensional binary indicator pairs stagnate near 1.5455, suffer local entrapment, and incur high per-evaluation costs. We propose a unified, implementable framework that addresses these weaknesses by combining (1) multi-fidelity surrogate modeling with uncertainty quantification, (2) adaptive operator learning via deep reinforcement, and (3) resource-aware, population-based search. Our key contributions are:\n\n1. Multi-Fidelity Bayesian-Ensemble Graph Surrogate  \n \u2022 Develop an ensemble of lightweight graph neural networks that ingest bit-interaction graphs to predict candidate ratios with calibrated uncertainty.  \n \u2022 Introduce a two-stage fidelity pipeline: first, a fast stochastic bit-sampling estimate prunes 90% of unlikely moves; second, the GNN ensemble computes predictive mean and variance to drive an upper-confidence\u2010bound (UCB) acquisition.  \n \u2022 Online ensemble update via bootstrapped residual learning to maintain >85% fidelity on novel subspaces and reduce exact convolution calls by 6\u00d7.\n\n2. Adaptive Deep-RL Move Operator  \n \u2022 Model move proposals as an MDP over bit\u2010graph embeddings and action primitives (single\u2010swap, block\u2010swap, multi\u2010flip).  \n \u2022 Train a compact policy network with Proximal Policy Optimization using surrogate-based rewards augmented by acquisition gain (variance reduction).  \n \u2022 Integrate a lightweight critic to predict long\u2010term improvement probability, enabling dynamic adjustment of mutation magnitudes to escape plateaus and refine solutions.\n\n3. Contextual Resource Allocation Across Subpopulations  \n \u2022 Instantiate multiple subpopulations, each seeded with distinct mutation operator mixes (e.g., CMA-ES kernels, differential\u2010block swaps, learned policy moves).  \n \u2022 Employ a contextual Thompson-Sampling meta-learner that allocates evaluation budget to subpopulations based on recent improvement trajectories and diversity entropy.  \n \u2022 Periodic structured migration of elites to maintain diversity and prevent premature convergence.\n\n4. High-Throughput GPU-Accelerated Pipeline  \n \u2022 Implement all surrogate predictions, bit-sampling evaluations, and policy\u2010network inference in JAX/Flax with XLA fusion and mixed\u2010precision to achieve 10\u00d7 per-evaluation speedups for N\u2265100.  \n \u2022 Coordinate asynchronous batched evaluations via a minimal Dask interface, enabling near-linear scaling across up to 16 GPUs.\n\nIn experiments over N=30\u2013500, our framework is expected to (i) reach ratios \u22651.8 in one\u2010fifth the time of prior methods, (ii) consistently exceed 1.9 final ratios under equal compute budgets, and (iii) exhibit robust scalability with sublinear overhead on multi-GPU setups. A staged implementation plan\u2014(1) ensemble surrogate prototyping, (2) RL-operator integration, (3) meta-learner orchestration, (4) large-scale benchmarking\u2014ensures reproducibility and rapid adoption for high-performance combinatorial optimization in binary indicator spaces."], "idea_reward": 5.75, "parent_id": "01b504f9-b84a-4c04-87dc-c4c5f378ef33", "generation": 22, "timestamp": 1757828337.9313576, "iteration_found": 1331, "metrics": {"score": 1.5454545454545454}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 9 lines with 15 lines\nChange 2: Replace 17 lines with 22 lines", "parent_metrics": {"score": 1.5454545454545454}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: A Hierarchical Surrogate\u2010Assisted Deep\u2010Reinforcement Evolutionary Framework for Optimizing Sum-to-Difference Ratios in Binary Indicator Sets\n\nAbstract:\nWe address three key limitations of existing hill-climbing and simulated-annealing based searches for maximizing |A+B|/|A\u2212B| over binary indicator pairs\u2014plateaued convergence (~1.5455), entrapment in local optima, and high per-evaluation cost\u2014by integrating advances from surrogate modeling, deep reinforcement learning, and population-based heuristics into a unified, easily\u2010implementable pipeline. Our contributions are:\n\n1. Graph-Structured Surrogate Model  \n \u2022 Replace summary-statistic regressors with a lightweight graph neural network (GNN) that directly ingests bit-vector adjacency and interaction graphs to predict both global ratio improvements and per-bit \u201cimpact scores.\u201d  \n \u2022 Pre-train the surrogate on a diverse archive of labeled proposals, then fine-tune online to maintain 90% prediction accuracy on new candidate moves.  \n \u2022 Use top-K filtering to achieve a 5\u00d7 reduction in exact GPU-accelerated convolution/correlation calls.\n\n2. Multi-Population Evolution with Adaptive Operator Learning  \n \u2022 Launch parallel subpopulations combining evolutionary strategies (CMA-ES, differential evolution, multi-swap hill-climbing) to promote diverse search trajectories.  \n \u2022 Employ a meta-learner based on contextual Thompson sampling to allocate compute to the most promising subpopulations and mutation operators in real time.  \n \u2022 Periodically exchange elite individuals via structured migrations to prevent premature convergence.\n\n3. Deep Reinforcement Move Proposer  \n \u2022 Frame per-bit swap/add/remove selection as a Markov decision process and train a small policy network (two\u2010layer MLP) via Proximal Policy Optimization using surrogate feedback as a reward signal.  \n \u2022 Dynamically adjust mutation magnitudes (single\u2010bit up to block swaps) based on learned state embeddings, enabling both fine\u2010grained refinements and bold esc...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Removed caching to reduce per\u2010call overhead\n\n# Prebind convolution and correlation functions for speed\nconvolve_fn = np.convolve\ncorrelate_fn = np.correlate\n\n# For reproducibility\nrng = default_rng(42)\nrng_random = rng.random\nrng_choice = rng.choice\nexp = math.exp\ncnz = np.count_nonzero\nany_fn = np.any\nflatnonzero_fn = np.flatnonzero\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check for empty sets\n    if not any_fn(A_ind) or not any_fn(B_ind):\n        return -1.0\n    num_sums = cnz(convolve_fn(A_ind, B_ind))\n    num_diffs = cnz(correlate_fn(A_ind, B_ind, mode='full'))\n    return -1.0 if num_diffs == 0 else num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray, ones: np.ndarray, zeros: np.ndarray) -> np.ndarray:\n    \"\"\"Perform a swap or multi-swap move using precomputed one- and zero-indices.\"\"\"\n    if ones.size and zeros.size:\n        if rng_random() < 0.3 and ones.size > 1 and zeros.size > 1:\n            removes = rng_choice(ones, size=2, replace=False)\n            adds = rng_choice(zeros, size=2, replace=False)\n            ind[removes] = False\n            ind[adds] = True\n        else:\n            i_remove = rng_choice(ones)\n            i_add = rng_choice(zeros)\n            ind[i_remove] = False\n            ind[i_add] = True\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\nBATCH_SIZE = 20  # increased number of local proposals for better exploration\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    # use boolean arrays for lighter\u2010weight, faster bit\u2010ops\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = 1\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    # Initialize counters for adaptive temperature control\n    no_improve = 0\n    plateau_limit = 5000  # iterations to trigger a temperature boost\n    decay = 0.9999        # even slower cooling to escape local optima\n\n    # Local search: random single\u2010bit flips\n    max_iter = 50000  # extended search iterations for improved convergence\n    # Pre-bind heavy functions once per outer loop\n    _compute_ratio = compute_ratio\n    _rand        = rng_random\n    _exp         = exp\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        # Precompute indices once per outer iteration\n        ones_A = flatnonzero_fn(current_A)\n        zeros_A = flatnonzero_fn(~current_A)\n        ones_B = flatnonzero_fn(current_B)\n        zeros_B = flatnonzero_fn(~current_B)\n        for _ in range(BATCH_SIZE):  # batch size configurable\n            # Generate candidate proposals by copying both state vectors once\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            r = _rand()\n            # With some probability, mutate both for deeper exploration\n            if r < 0.2:\n                C_ind = propose_move(C_ind, ones_A, zeros_A)\n                D_ind = propose_move(D_ind, ones_B, zeros_B)\n            elif r < 0.6:\n                C_ind = propose_move(C_ind, ones_A, zeros_A)\n            else:\n                D_ind = propose_move(D_ind, ones_B, zeros_B)\n            ratio = _compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate references directly (no extra copies)\n                local_best_A, local_best_B = C_ind, D_ind\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or _rand() < _exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best and manage no\u2010improvement counter\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n            no_improve = 0\n        else:\n            no_improve += 1\n        # adaptive temperature boost when stuck\n        if no_improve > plateau_limit:\n            T *= 1.5       # momentarily increase T to escape plateau\n            no_improve = 0\n        # regular cooldown\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.5455\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 2 (Score: 1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n# using numpy's exp for simulated annealing; no need for `math` import\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B| using bitwise ops. Return \u22121.0 if infeasible.\"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.0000)\n\n\n### Program 3 (Score: 1.4516)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check on non-empty sets\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.4516)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# Prebind convolution and correlation functions for speed\n# ... (truncated)\n```\nKey features: Alternative approach to error\n\n\n### Program D2 (Score: 1.4138)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nTitle: A Multi\u2010Fidelity Bayesian\u2010Ensemble Graph\u2010Surrogate and Adaptive Deep\u2010RL\u2010Evolutionary Framework for Optimizing Sum-to-Difference Ratios in Binary Indicator Sets\n\nAbstract:\nDespite recent advances, hill-climbing and simulated-annealing methods for maximizing the sum-to-difference ratio |A+B|/|A\u2013B| on N-dimensional binary indicator pairs stagnate near 1.5455, suffer local entrapment, and incur high per-evaluation costs. We propose a unified, implementable framework that addresses these weaknesses by combining (1) multi-fidelity surrogate modeling with uncertainty quantification, (2) adaptive operator learning via deep reinforcement, and (3) resource-aware, population-based search. Our key contributions are:\n\n1. Multi-Fidelity Bayesian-Ensemble Graph Surrogate  \n \u2022 Develop an ensemble of lightweight graph neural networks that ingest bit-interaction graphs to predict candidate ratios with calibrated uncertainty.  \n \u2022 Introduce a two-stage fidelity pipeline: first, a fast stochastic bit-sampling estimate prunes 90% of unlikely moves; second, the GNN ensemble computes predictive mean and variance to drive an upper-confidence\u2010bound (UCB) acquisition.  \n \u2022 Online ensemble update via bootstrapped residual learning to maintain >85% fidelity on novel subspaces and reduce exact convolution calls by 6\u00d7.\n\n2. Adaptive Deep-RL Move Operator  \n \u2022 Model move proposals as an MDP over bit\u2010graph embeddings and action primitives (single\u2010swap, block\u2010swap, multi\u2010flip).  \n \u2022 Train a compact policy network with Proximal Policy Optimization using surrogate-based rewards augmented by acquisition gain (variance reduction).  \n \u2022 Integrate a lightweight critic to predict long\u2010term improvement probability, enabling dynamic adjustment of mutation magnitudes to escape plateaus and refine solutions.\n\n3. Contextual Resource Allocation Across Subpopulations  \n \u2022 Instantiate multiple subpopulations, each seeded with distinct mutation operator mixes (e.g., CMA-ES kernels, differential\u2010block swaps, ...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are two targeted patches to reduce per\u2010call overhead and memory allocations, both of which should help improve the exploration budget and thus the final ratio.\n\nPatch 1: Replace the costly NumPy convolution/correlation with small\u2010N Python\u2010set logic in `compute_ratio`.\n\n<<<<<<< SEARCH\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # quick check for empty sets\n    if not any_fn(A_ind) or not any_fn(B_ind):\n        return -1.0\n    num_sums = cnz(convolve_fn(A_ind, B_ind))\n    num_diffs = cnz(correlate_fn(A_ind, B_ind, mode='full'))\n    return -1.0 if num_diffs == 0 else num_sums / num_diffs\n=======\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B| using Python sets. Return \u22121.0 if infeasible.\"\"\"\n    # quick check for empty sets\n    if not any_fn(A_ind) or not any_fn(B_ind):\n        return -1.0\n    # extract indices of ones\n    A_idxs = flatnonzero_fn(A_ind)\n    B_idxs = flatnonzero_fn(B_ind)\n    # compute sumset and diffset via Python sets for small N\n    sums = {i + j for i in A_idxs for j in B_idxs}\n    diffs = {i - j for i in A_idxs for j in B_idxs}\n    num_sums = len(sums)\n    num_diffs = len(diffs)\n    return -1.0 if num_diffs == 0 else num_sums / num_diffs\n>>>>>>> REPLACE\n\nReasoning: For N\u224830, building two Python sets of size |A|\u00d7|B| is often faster than the overhead of `np.convolve`/`np.correlate` and a separate `count_nonzero`. We avoid kernel launches and scratch arrays in NumPy for each evaluation.\n\n---\n\nPatch 2: Reuse two pre\u2010allocated boolean buffers for proposal generation rather than allocating a fresh copy on every batch iteration.\n\n<<<<<<< SEARCH\n            for _ in range(BATCH_SIZE):  # batch size configurable\n                # Generate candidate proposals by copying both state vectors once\n                C_ind, D_ind = current_A.copy(), current_B.copy()\n                r = _rand()\n                # With some probability, mutate both for deeper exploration\n                if r < 0.2:\n                    C_ind = propose_move(C_ind, ones_A, zeros_A)\n                    D_ind = propose_move(D_ind, ones_B, zeros_B)\n                elif r < 0.6:\n                    C_ind = propose_move(C_ind, ones_A, zeros_A)\n                else:\n                    D_ind = propose_move(D_ind, ones_B, zeros_B)\n                ratio = _compute_ratio(C_ind, D_ind)\n                if ratio > local_best_ratio:\n                    local_best_ratio = ratio\n                    # Store best candidate references directly (no extra copies)\n                    local_best_A, local_best_B = C_ind, D_ind\n=======\n            # reuse arrays for proposals to reduce allocations\n            C_ind = np.empty_like(current_A)\n            D_ind = np.empty_like(current_B)\n            for _ in range(BATCH_SIZE):  # batch size configurable\n                # copy current state into proposal arrays\n                C_ind[:] = current_A\n                D_ind[:] = current_B\n                r = _rand()\n                # With some probability, mutate both for deeper exploration\n                if r < 0.2:\n                    propose_move(C_ind, ones_A, zeros_A)\n                    propose_move(D_ind, ones_B, zeros_B)\n                elif r < 0.6:\n                    propose_move(C_ind, ones_A, zeros_A)\n                else:\n                    propose_move(D_ind, ones_B, zeros_B)\n                ratio = _compute_ratio(C_ind, D_ind)\n                if ratio > local_best_ratio:\n                    local_best_ratio = ratio\n                    # copy best into separate arrays to preserve state\n                    local_best_A = C_ind.copy()\n                    local_best_B = D_ind.copy()\n>>>>>>> REPLACE\n\nReasoning: By pre\u2010allocating `C_ind`/`D_ind` once per outer iteration and using in\u2010place mutations plus slice\u2010copies, we drastically cut down on Python object allocation and garbage collection pressure in the inner loop. This lets us execute more proposals per second and should push the score upward."]}}}