{"id": "c1f27b76-0c25-4c14-ab5d-31f4d8f2db46", "code": "import numpy as np\nfrom numpy.random import default_rng\nimport math\nimport numpy.fft as fft_module\n# Pre\u2010bind exponential and initialize a cache for compute_ratio\nexp = math.exp\nratio_cache = {}\n\n# For reproducibility\nrng = default_rng(42)\n# Pre\u2010bind RNG methods to avoid attribute lookups in tight loops\nrng_random = rng.random\nrng_choice = rng.choice\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in ratio_cache:\n        return ratio_cache[key]\n    if not A_ind.any() or not B_ind.any():\n        ratio_cache[key] = -1.0\n        return -1.0\n    A_arr = A_ind.view(np.int8)\n    B_arr = B_ind.view(np.int8)\n    n = A_arr.size\n    if n > 512:\n        # FFT\u2010based convolution/correlation (pre\u2010imported)\n        m = 2*n - 1\n        fsize = fft_module.next_fast_len(m)\n        fa = fft_module.fft(A_arr, fsize)\n        fb = fft_module.fft(B_arr, fsize)\n        conv = fft_module.ifft(fa * fb).real[:m]\n        corr = fft_module.ifft(fa * np.conj(fb)).real[:m]\n        num_sums = np.count_nonzero(np.round(conv).astype(int))\n        num_diffs = np.count_nonzero(np.round(corr).astype(int))\n    else:\n        # Direct methods for small arrays\n        num_sums = np.count_nonzero(np.convolve(A_arr, B_arr))\n        num_diffs = np.count_nonzero(np.correlate(A_arr, B_arr, mode='full'))\n    if num_diffs == 0:\n        ratio_cache[key] = -1.0\n        return -1.0\n    ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray, multi_swap_prob: float = 0.1) -> np.ndarray:\n    \"\"\"Perform one or two swap moves to maintain constant cardinality.\"\"\"\n    new_ind = ind.copy()\n    # with small probability, do two swaps instead of one\n    swaps = 2 if rng.random() < multi_swap_prob else 1\n    for _ in range(swaps):\n        ones_idx = rng.choice(np.flatnonzero(new_ind))\n        zeros_idx = rng.choice(np.flatnonzero(~new_ind))\n        new_ind[ones_idx] = False\n        new_ind[zeros_idx] = True\n    return new_ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9997        # slower cooling for wider exploration\n\n    # Local search: random single\u2010bit flips\n    max_iter = 40000\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(50):  # batch size = 50\n            # Generate independent proposals for both state vectors\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng.random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate copies to avoid aliasing\n                local_best_A, local_best_B = C_ind.copy(), D_ind.copy()\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng.random() < math.exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: Hybrid Evolutionary\u2013Surrogate Framework for Maximizing Sum\u2010to\u2010Difference Ratios in Indicator Vector Pairs\n\nAbstract:  \nWe propose a novel, implementable framework to overcome the limitations of the current simulated\u2010annealing\u2013hill\u2010climbing approach for optimizing the sum\u2010to\u2010difference ratio |A+B|/|A\u2013B| of 0/1 indicator vector pairs. Our analysis of the existing method (score=1.5455) reveals three primary weaknesses: (1) reliance on single\u2010bit swaps limits exploration of distant basins; (2) static batch sizes and cooling schedules hinder adaptive search; (3) repeated full convolution/correlation evaluations incur high computational cost. To address these, we integrate insights from successful evolutionary strategies and surrogate\u2010accelerated metaheuristics:\n\n1. Problem\u2010Specific Evolutionary Operators  \n   \u2022 Population Initialization: Generate a diverse pool of indicator pairs via randomized cardinality\u2010preserving perturbations of known high\u2010quality seeds (e.g., Conway\u2019s MSTD).  \n   \u2022 Structured Crossover: Exchange contiguous blocks of indices between parents to preserve beneficial sub\u2010structures and accelerate convergence into high\u2010ratio regions.  \n   \u2022 Adaptive Mutation: Introduce multi\u2010point swaps whose count and selection distribution are governed by a success\u2010driven probability model, allowing dynamic intensification and diversification.\n\n2. Surrogate\u2010Guided Candidate Screening  \n   \u2022 Gaussian Process Regression (GPR) surrogate is trained online on computed (A,B,ratio) triples, enabling cheap prediction of the sum\u2010to\u2010difference ratio.  \n   \u2022 In each generation, a large batch of offspring is proposed, but only the top-k candidates by surrogate\u2010predicted ratio proceed to exact convolution/correlation evaluation, reducing expensive FFT/ direct convolution calls by \u226570%.\n\n3. Dynamic Resource Allocation  \n   \u2022 Population and batch sizes self\u2010adjust in response to search progress: early stages favor broad exploration (larger population, larger surrogate batch), while later stages intensify local improvements around top individuals (smaller batch, focused local search).  \n   \u2022 Cooling and mutation rates are replaced by reinforcement\u2010learning\u2013tuned control policies that balance exploration vs. exploitation, learned from prior runs on small\u2010scale instances.\n\n4. Efficient Incremental Convolution Updates  \n   \u2022 We derive an incremental update scheme for the sum and difference counts that exploits the sparse swap structure: when few bits change, update only affected convolution lags in O(s) time (s = number of swapped indices), avoiding full FFTs for large N.\n\nExpected Outcomes and Implementation:  \nOur hybrid framework is projected to surpass the current 1.5455 benchmark through richer population diversity, surrogate\u2010driven efficiency, and adaptive search control. We will implement the system in Python, leveraging NumPy and scikit\u2010learn for surrogate modeling, and optional Numba or PyTorch acceleration for convolution updates. Experimentation will cover vector lengths up to N=1000, with ablation studies to quantify the impact of each component. This work promises a generalizable, high\u2010performance solution for combinatorial ratio maximization in indicator spaces."], "idea_reward": 6.25, "parent_id": "0eea3e7e-a9bf-474e-ad81-e16c0260b024_migrant_2_migrant_1", "generation": 17, "timestamp": 1757826305.6690445, "iteration_found": 1299, "metrics": {"score": 1.5454545454545454}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 3 lines with 7 lines\nChange 2: Replace 25 lines with 31 lines\nChange 3: Replace 2 lines with 5 lines", "parent_metrics": {"score": 1.5454545454545454}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \n\n\n# Previous Program:\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    A_arr = A_ind.view(np.int8)\n    B_arr = B_ind.view(np.int8)\n    n = A_arr.size\n    if n > 512:\n        # FFT\u2010based convolution/correlation for large arrays\n        from numpy.fft import next_fast_len, fft, ifft\n        m = 2*n - 1\n        fsize = next_fast_len(m)\n        fa = fft(A_arr, fsize)\n        fb = fft(B_arr, fsize)\n        conv = ifft(fa * fb).real[:m]\n        corr = ifft(fa * np.conj(fb)).real[:m]\n        num_sums = np.count_nonzero(np.round(conv).astype(int))\n        num_diffs = np.count_nonzero(np.round(corr).astype(int))\n    else:\n        # Direct methods for small arrays\n        num_sums = np.count_nonzero(np.convolve(A_arr, B_arr))\n        num_diffs = np.count_nonzero(np.correlate(A_arr, B_arr, mode='full'))\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray, multi_swap_prob: float = 0.1) -> np.ndarray:\n    \"\"\"Perform one or two swap moves to maintain constant cardinality.\"\"\"\n    new_ind = ind.copy()\n    # with small probability, do two swaps instead of one\n    swaps = 2 if rng.random() < multi_swap_prob else 1\n    for _ in range(swaps):\n        ones_idx = rng.choice(np.flatnonzero(new_ind))\n        zeros_idx = rng.choice(np.flatnonzero(~new_ind))\n        new_ind[ones_idx] = False\n        new_ind[zeros_idx] = True\n    return new_ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    # Initialize simulated annealing\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n    T = 1.0\n    decay = 0.9997        # slower cooling for wider exploration\n\n    # Local search: random single\u2010bit flips\n    max_iter = 40000\n    for _ in range(max_iter):\n        # batch multiple proposals to improve exploration\n        local_best_ratio = -1.0\n        local_best_A = None\n        local_best_B = None\n        # Use current state for generating local proposals to enhance search diversity\n        for _ in range(50):  # batch size = 50\n            # Generate independent proposals for both state vectors\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng.random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate copies to avoid aliasing\n                local_best_A, local_best_B = C_ind.copy(), D_ind.copy()\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n        if delta > 0 or rng.random() < math.exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n        # update global best\n        if current_ratio > best_ratio:\n            best_ratio, best_A, best_B = current_ratio, current_A, current_B\n        # cool down\n        T *= decay\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.5455\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5588\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5588)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\nimport functools\n\nrng = default_rng(42)\nrng_random = rng.random\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5588)\n\n\n### Program 2 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n\n# Cache for computed ratios to avoid redundant convolution calls\nratio_cache = {}\n\n# For reproducibility and performance\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5455)\n\n\n### Program 3 (Score: -1.0000)\n```python\nimport numpy as np\n# Use numpy default_rng for reproducibility and better performance\nrng = np.random.default_rng(42)\n# Cache for computed ratios to avoid redundant computations\n_ratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind, B_ind):\n    \"\"\"\n    Compute the sum-to-difference ratio R(A,B) = |A+B| / |A-B|.\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: 1.5455)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n# Pre\u2010bind exponential for simulated\u2010annealing checks\nexp = math.exp\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nTitle: Hybrid Evolutionary\u2013Surrogate Framework for Maximizing Sum\u2010to\u2010Difference Ratios in Indicator Vector Pairs\n\nAbstract:  \nWe propose a novel, implementable framework to overcome the limitations of the current simulated\u2010annealing\u2013hill\u2010climbing approach for optimizing the sum\u2010to\u2010difference ratio |A+B|/|A\u2013B| of 0/1 indicator vector pairs. Our analysis of the existing method (score=1.5455) reveals three primary weaknesses: (1) reliance on single\u2010bit swaps limits exploration of distant basins; (2) static batch sizes and cooling schedules hinder adaptive search; (3) repeated full convolution/correlation evaluations incur high computational cost. To address these, we integrate insights from successful evolutionary strategies and surrogate\u2010accelerated metaheuristics:\n\n1. Problem\u2010Specific Evolutionary Operators  \n   \u2022 Population Initialization: Generate a diverse pool of indicator pairs via randomized cardinality\u2010preserving perturbations of known high\u2010quality seeds (e.g., Conway\u2019s MSTD).  \n   \u2022 Structured Crossover: Exchange contiguous blocks of indices between parents to preserve beneficial sub\u2010structures and accelerate convergence into high\u2010ratio regions.  \n   \u2022 Adaptive Mutation: Introduce multi\u2010point swaps whose count and selection distribution are governed by a success\u2010driven probability model, allowing dynamic intensification and diversification.\n\n2. Surrogate\u2010Guided Candidate Screening  \n   \u2022 Gaussian Process Regression (GPR) surrogate is trained online on computed (A,B,ratio) triples, enabling cheap prediction of the sum\u2010to\u2010difference ratio.  \n   \u2022 In each generation, a large batch of offspring is proposed, but only the top-k candidates by surrogate\u2010predicted ratio proceed to exact convolution/correlation evaluation, reducing expensive FFT/ direct convolution calls by \u226570%.\n\n3. Dynamic Resource Allocation  \n   \u2022 Population and batch sizes self\u2010adjust in response to search progress: early stages favor broad exploration (larger population, larger surrogate batch), ...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are targeted edits in the requested diff format.  Overall we\n\n\u2022 cache all ratio\u2010computations to avoid redundant FFT/direct convolutions  \n\u2022 move the FFT imports up front (so we don\u2019t pay for repeated imports)  \n\u2022 pre\u2010bind math.exp and the RNG methods for cheaper tight\u2010loop calls  \n\nThese small changes should substantially reduce your per\u2010iteration cost without changing the high\u2010level search logic.\n\n```diff\n<<<<<<< SEARCH\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n=======\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\nimport numpy.fft as fft_module\n# Pre\u2010bind exponential and initialize a cache for compute_ratio\nexp = math.exp\nratio_cache = {}\n>>>>>>> REPLACE\n```\n\n```diff\n<<<<<<< SEARCH\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    if not A_ind.any() or not B_ind.any():\n        return -1.0\n    A_arr = A_ind.view(np.int8)\n    B_arr = B_ind.view(np.int8)\n    n = A_arr.size\n    if n > 512:\n        # FFT\u2010based convolution/correlation for large arrays\n        from numpy.fft import next_fast_len, fft, ifft\n        m = 2*n - 1\n        fsize = next_fast_len(m)\n        fa = fft(A_arr, fsize)\n        fb = fft(B_arr, fsize)\n        conv = ifft(fa * fb).real[:m]\n        corr = ifft(fa * np.conj(fb)).real[:m]\n        num_sums = np.count_nonzero(np.round(conv).astype(int))\n        num_diffs = np.count_nonzero(np.round(corr).astype(int))\n    else:\n        # Direct methods for small arrays\n        num_sums = np.count_nonzero(np.convolve(A_arr, B_arr))\n        num_diffs = np.count_nonzero(np.correlate(A_arr, B_arr, mode='full'))\n    if num_diffs == 0:\n        return -1.0\n    return num_sums / num_diffs\n=======\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in ratio_cache:\n        return ratio_cache[key]\n    if not A_ind.any() or not B_ind.any():\n        ratio_cache[key] = -1.0\n        return -1.0\n    A_arr = A_ind.view(np.int8)\n    B_arr = B_ind.view(np.int8)\n    n = A_arr.size\n    if n > 512:\n        # FFT\u2010based convolution/correlation (pre\u2010imported)\n        m = 2*n - 1\n        fsize = fft_module.next_fast_len(m)\n        fa = fft_module.fft(A_arr, fsize)\n        fb = fft_module.fft(B_arr, fsize)\n        conv = fft_module.ifft(fa * fb).real[:m]\n        corr = fft_module.ifft(fa * np.conj(fb)).real[:m]\n        num_sums = np.count_nonzero(np.round(conv).astype(int))\n        num_diffs = np.count_nonzero(np.round(corr).astype(int))\n    else:\n        # Direct methods for small arrays\n        num_sums = np.count_nonzero(np.convolve(A_arr, B_arr))\n        num_diffs = np.count_nonzero(np.correlate(A_arr, B_arr, mode='full'))\n    if num_diffs == 0:\n        ratio_cache[key] = -1.0\n        return -1.0\n    ratio = num_sums / num_diffs\n    ratio_cache[key] = ratio\n    return ratio\n>>>>>>> REPLACE\n```\n\n```diff\n<<<<<<< SEARCH\n# For reproducibility\nrng = default_rng(42)\n=======\n# For reproducibility\nrng = default_rng(42)\n# Pre\u2010bind RNG methods to avoid attribute lookups in tight loops\nrng_random = rng.random\nrng_choice = rng.choice\n>>>>>>> REPLACE\n```\n\n```diff\n<<<<<<< SEARCH\ndef propose_move(ind: np.ndarray, multi_swap_prob: float = 0.1) -> np.ndarray:\n    \"\"\"Perform one or two swap moves to maintain constant cardinality.\"\"\"\n    new_ind = ind.copy()\n    # with small probability, do two swaps instead of one\n-    swaps = 2 if rng.random() < multi_swap_prob else 1\n+    swaps = 2 if rng_random() < multi_swap_prob else 1\n    for _ in range(swaps):\n-        ones_idx = rng.choice(np.flatnonzero(new_ind))\n-        zeros_idx = rng.choice(np.flatnonzero(~new_ind))\n+        ones_idx = rng_choice(np.flatnonzero(new_ind))\n+        zeros_idx = rng_choice(np.flatnonzero(~new_ind))\n        new_ind[ones_idx] = False\n        new_ind[zeros_idx] = True\n    return new_ind\n```\n\n```diff\n<<<<<<< SEARCH\n        for _ in range(50):  # batch size = 50\n            # Generate independent proposals for both state vectors\n            C_ind, D_ind = current_A.copy(), current_B.copy()\n            if rng.random() < 0.5:\n                C_ind = propose_move(C_ind)\n            else:\n                D_ind = propose_move(D_ind)\n            ratio = compute_ratio(C_ind, D_ind)\n            if ratio > local_best_ratio:\n                local_best_ratio = ratio\n                # Store best candidate copies to avoid aliasing\n                local_best_A, local_best_B = C_ind.copy(), D_ind.copy()\n        # simulated annealing acceptance\n        delta = local_best_ratio - current_ratio\n-       if delta > 0 or rng.random() < math.exp(delta / T):\n+       if delta > 0 or rng_random() < exp(delta / T):\n            current_ratio = local_best_ratio\n            current_A, current_B = local_best_A, local_best_B\n```\nExplanation of benefits:\n\n- Caching (`ratio_cache`) avoids repeated\u2014and expensive\u2014FFT or convolution calls on identical states.\n- Moving the FFT import out of the inner routine removes repeated import overhead.\n- Binding `math.exp`, `rng.random`, and `rng.choice` to local names cuts down on attribute lookups under high\u2010iteration counts.\n- None of these changes alter the core hill\u2010climbing or annealing logic, but they should yield a noticeable speedup per iteration, allowing either more iterations in the same time or reduced overhead for the same workload."]}}}