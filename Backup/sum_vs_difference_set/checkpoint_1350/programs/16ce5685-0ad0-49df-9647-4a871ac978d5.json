{"id": "16ce5685-0ad0-49df-9647-4a871ac978d5", "code": "import numpy as np\nconvolve = np.convolve\nflatnonzero = np.flatnonzero\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n# cache to avoid redundant ratio computations\n_ratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # check cache for redundant computations using sparse index tuples\n    idxA = tuple(flatnonzero(A_ind))\n    idxB = tuple(flatnonzero(B_ind))\n    key = (idxA, idxB)\n    if key in _ratio_cache:\n        return _ratio_cache[key]\n    # convert to int8 bit vectors for convolution-based counts\n    A_int = A_ind.astype(np.int8)\n    B_int = B_ind.astype(np.int8)\n    # |A+B| count via direct convolution\n    conv_s = np.convolve(A_int, B_int)\n    sum_count = np.count_nonzero(conv_s)\n    # |A\u2212B| count via convolution with reversed B\n    B_rev = B_int[::-1]\n    conv_d = np.convolve(A_int, B_rev)\n    diff_count = np.count_nonzero(conv_d)\n    if diff_count == 0:\n        _ratio_cache[key] = -1.0\n        return -1.0\n    ratio = sum_count / diff_count\n    _ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = flatnonzero(ind)\n    zeros = flatnonzero(~ind)\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    # pre\u2010bind hot functions\n    random = rng.random\n    exp = math.exp\n    for _ in range(max_iter):\n        # propose a balanced add/remove move by copying only one side\n        if random() < 0.5:\n            C_ind = current_A.copy()\n            D_ind = current_B\n            C_ind = propose_move(C_ind)\n        else:\n            C_ind = current_A\n            D_ind = current_B.copy()\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or random() < exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n", "language": "python", "proposal": ["Title: Adaptive Graph-Surrogate Guided Hierarchical RL with Dynamic Macro-Move Induction and Streaming Ratio Computation for Scalable Sum-to-Difference Maximization\n\nAbstract:\nCurrent hill-climbing and surrogate-guided PPO methods plateau at a sum-to-difference score of \u22481.38, hindered by static surrogates misaligned for large N, greedy one-step move policies, and costly ratio evaluations. We propose an end-to-end framework that overcomes these bottlenecks through four key innovations:\n\n1. Adaptive Graph-Surrogate with Continual Domain Alignment  \n   \u2022 A graph neural network surrogate is meta-trained on a curriculum of small to mid-sized N and continually fine-tuned online via adversarial domain-shift detection.  \n   \u2022 A multi-fidelity evaluation controller engages high-precision ratio computations only in high-uncertainty regions, reducing unnecessary cost.\n\n2. Hierarchical Model-Based RL with Deep Multi-Step Planning  \n   \u2022 A learned transition model of bit-move dynamics is coupled with an MCTS planner accelerated by a value-function critic, enabling k-step lookahead on local flips and grammar-driven macro moves.  \n   \u2022 Intrinsic exploration bonuses are derived from the surrogate\u2019s epistemic uncertainty and novelty in state-action visitation.\n\n3. Dynamic Macro-Move Induction via Motif Priors  \n   \u2022 We automatically mine recurring substructure motifs from elite solutions using Bayesian nonparametrics, forming a compact grammar of block-flip templates.  \n   \u2022 A contextual policy network dynamically balances execution of induced macro moves versus fine-grained flips, guided by a learned utility predictor.\n\n4. Streaming Ratio Computation with Bit-Parallel Pipelines  \n   \u2022 We implement an incremental update engine that maintains Fenwick-tree indices and GPU-accelerated bitset convolutions, delivering amortized O(log\u2009N) update time and 10\u00d7 speed-up over full recomputation.\n\nWe will prototype in JAX/PyTorch with custom CUDA kernels, evaluating on N\u2208{50,100,200,500,1\u2009000,2\u2009000,5\u2009000,10\u2009000}. Benchmarks against hill-climbing, CMA-ES, tabu search, and prior surrogate-PPO aim to surpass a 1.50 ratio while maintaining sublinear runtime scaling."], "idea_reward": 5.5, "parent_id": "77110863-b88b-4dff-a1cb-e39b5b6e63b1", "generation": 14, "timestamp": 1757790868.6804287, "iteration_found": 710, "metrics": {"score": 1.3846153846153846}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace import numpy as np with 2 lines\nChange 2: Replace 14 lines with 12 lines\nChange 3: Replace 8 lines with 11 lines\nChange 4: 'if rng.random() < 0.5:' to 'if random() < 0.5:'\nChange 5: 'if delta > 0 or rng.random() < math.exp(delta / T):' to 'if delta > 0 or random() < exp(delta / T):'", "parent_metrics": {"score": 1.3846153846153846}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n", "user": "# Previous Proposal: \nTitle: Continual Meta-Surrogate Guided Hierarchical RL with Incremental Ratio Computation for Scalable Sum-to-Difference Optimization\n\nAbstract:  \nThe current hill-climbing and surrogate-guided PPO approaches for maximizing the sum-to-difference ratio achieve moderate gains (score\u22481.3846) but suffer from (1) offline surrogates that misalign as N grows, (2) single-step move policies lacking lookahead, and (3) expensive ratio evaluations that bottleneck search. We propose a unified framework that synergistically addresses these shortcomings:\n\n1. Continual Meta-Surrogate with Active Uncertainty Calibration  \n   \u2022 A meta-learning backbone pre-trains a GNN surrogate on a curriculum of small to medium N, then continually fine-tunes online via uncertainty-driven sampling of new instances.  \n   \u2022 A Bayesian uncertainty module flags domain-shifted regions, triggering localized high-fidelity evaluations only when necessary.\n\n2. Hierarchical Model-Based RL with Multi-Step Planning  \n   \u2022 We integrate a learned transition model of set-moves into a Monte-Carlo Tree Search (MCTS) planner, enabling k-step lookahead on both local bit flips and macro moves.  \n   \u2022 Intrinsic bonuses for regions with high surrogate variance steer exploration toward poorly understood landscape areas.\n\n3. Pattern-Driven Macro-Move Grammar  \n   \u2022 We mine recurring sum/difference patterns from elite solutions and encode them as \u201cmove templates\u201d (e.g., block flips, spectral patches).  \n   \u2022 A contextual policy network selects among template-based macro moves and fine-grained flips, dynamically balancing exploration and exploitation.\n\n4. Incremental Ratio Computation Engine  \n   \u2022 We replace full enumeration of sums and differences with a streaming bitset pipeline combining Fenwick-trees for incremental updates and GPU-accelerated bitwise convolutions.  \n   \u2022 A lightweight cache of substructure counts avoids redundant computations, achieving amortized O(log\u2009N) update time per move.\n\nImplementation & Validat...(truncated)\n\n# Previous Program:\n```python\nimport numpy as np\nflatnonzero = np.flatnonzero\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n# cache to avoid redundant ratio computations\n_ratio_cache = {}\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n    # check cache for redundant computations using sparse index tuples\n    idxA = tuple(flatnonzero(A_ind))\n    idxB = tuple(flatnonzero(B_ind))\n    key = (idxA, idxB)\n    if key in _ratio_cache:\n        return _ratio_cache[key]\n    # convert to int8 bit vectors for convolution-based counts\n    A_int = A_ind.astype(np.int8)\n    B_int = B_ind.astype(np.int8)\n    # |A+B| count via direct convolution\n    conv_s = np.convolve(A_int, B_int)\n    sum_count = np.count_nonzero(conv_s)\n    # |A\u2212B| count via convolution with reversed B\n    B_rev = B_int[::-1]\n    conv_d = np.convolve(A_int, B_rev)\n    diff_count = np.count_nonzero(conv_d)\n    if diff_count == 0:\n        _ratio_cache[key] = -1.0\n        return -1.0\n    ratio = sum_count / diff_count\n    _ratio_cache[key] = ratio\n    return ratio\n\n# Helper: perform one balanced swap/add/remove on a boolean indicator array\ndef propose_move(ind: np.ndarray) -> np.ndarray:\n    \"\"\"Randomly add or remove one bit, ensuring at least one \u20181\u2019 remains.\"\"\"\n    ones = flatnonzero(ind)\n    zeros = flatnonzero(~ind)\n    # if too few ones or by random choice, add\n    if zeros.size > 0 and (ones.size <= 1 or rng.random() < 0.5):\n        ind[rng.choice(zeros)] = True\n    # else remove (only if more than one one)\n    elif ones.size > 1:\n        ind[rng.choice(ones)] = False\n    return ind\n\n# Configuration constants\nDEFAULT_N = 30\nCONWAY_MSTD_INIT = [0, 2, 3, 4, 7, 11, 12, 14]\n\ndef main(N: int = DEFAULT_N) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Perform hill\u2010climbing search starting from the Conway MSTD set of size N.\"\"\"\n    A_ind = np.zeros(N, dtype=bool)\n    B_ind = np.zeros(N, dtype=bool)\n    A_ind[CONWAY_MSTD_INIT] = True\n    B_ind[:] = A_ind\n\n    # Evaluate initial ratio\n    # Evaluate initial ratio and initialize both current and best solutions\n    best_ratio = compute_ratio(A_ind, B_ind)\n    best_A, best_B = A_ind.copy(), B_ind.copy()\n    current_A, current_B = best_A.copy(), best_B.copy()\n    current_ratio = best_ratio\n\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        # propose a balanced add/remove move by copying only one side\n        if rng.random() < 0.5:\n            C_ind = current_A.copy()\n            D_ind = current_B\n            C_ind = propose_move(C_ind)\n        else:\n            C_ind = current_A\n            D_ind = current_B.copy()\n            D_ind = propose_move(D_ind)\n\n        ratio = compute_ratio(C_ind, D_ind)\n        # skip invalid proposals immediately\n        if ratio < 0:\n            continue\n        # simulated\u2010annealing acceptance criterion\n        delta = ratio - current_ratio\n        if delta > 0 or rng.random() < math.exp(delta / T):\n            # accept as new current\n            current_A, current_B, current_ratio = C_ind.copy(), D_ind.copy(), ratio\n            # update global best if improved\n            if ratio > best_ratio:\n                best_ratio, best_A, best_B = ratio, C_ind.copy(), D_ind.copy()\n        # cool down\n        T *= alpha\n\n    print(f\"N={N}, best ratio={best_ratio:.4f}\")\n    return best_A, best_B\n\nif __name__ == \"__main__\":\n    A_ind, B_ind = main()\n    print(\"A_ind:\", A_ind)\n    print(\"B_ind:\", B_ind)\n\n```\n\n# Previous Performance Metrics: \n- score: 1.3846\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: score: 1.5161\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: score: 1.5161\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 1.5312\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.5312)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_ratio(A_arr: np.ndarray, B_arr: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B|. Return \u22121.0 if infeasible.\"\"\"\n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.5312)\n\n\n### Program 2 (Score: -1.0000)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n# Initialize random number generator for reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors\ndef compute_sum_diff_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    S = np.convolve(A_ind, B_ind)\n    sizeS = np.count_nonzero(S)\n    D = np.convolve(A_ind, B_ind[::-1])\n# ... (truncated for brevity)\n```\nKey features: Performs well on error (-1.0000)\n\n\n### Program 3 (Score: 1.3704)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\nimport math  # for simulated\u2010annealing acceptance\n\n# For reproducibility\nrng = default_rng(42)\n\n# Compute the sum\u2010to\u2010difference ratio for two 0/1 indicator vectors via set operations\ndef compute_ratio(A_ind: np.ndarray, B_ind: np.ndarray) -> float:\n    \"\"\"Compute sum\u2010to\u2010difference ratio |A+B|/|A\u2212B| using set operations. \n# ... (truncated for brevity)\n```\nKey features: Performs well on score (1.3704)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.0294)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n\n# For reproducibility\nrng = default_rng(42)\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n### Program D2 (Score: 1.2105)\n```python\nimport numpy as np\nfrom numpy.random import default_rng\n# Prebind convolution for faster inner-loop calls\nconvolve = np.convolve\n\n# ... (truncated)\n```\nKey features: Alternative approach to score\n\n\n# Current Proposal\nTitle: Adaptive Graph-Surrogate Guided Hierarchical RL with Dynamic Macro-Move Induction and Streaming Ratio Computation for Scalable Sum-to-Difference Maximization\n\nAbstract:\nCurrent hill-climbing and surrogate-guided PPO methods plateau at a sum-to-difference score of \u22481.38, hindered by static surrogates misaligned for large N, greedy one-step move policies, and costly ratio evaluations. We propose an end-to-end framework that overcomes these bottlenecks through four key innovations:\n\n1. Adaptive Graph-Surrogate with Continual Domain Alignment  \n   \u2022 A graph neural network surrogate is meta-trained on a curriculum of small to mid-sized N and continually fine-tuned online via adversarial domain-shift detection.  \n   \u2022 A multi-fidelity evaluation controller engages high-precision ratio computations only in high-uncertainty regions, reducing unnecessary cost.\n\n2. Hierarchical Model-Based RL with Deep Multi-Step Planning  \n   \u2022 A learned transition model of bit-move dynamics is coupled with an MCTS planner accelerated by a value-function critic, enabling k-step lookahead on local flips and grammar-driven macro moves.  \n   \u2022 Intrinsic exploration bonuses are derived from the surrogate\u2019s epistemic uncertainty and novelty in state-action visitation.\n\n3. Dynamic Macro-Move Induction via Motif Priors  \n   \u2022 We automatically mine recurring substructure motifs from elite solutions using Bayesian nonparametrics, forming a compact grammar of block-flip templates.  \n   \u2022 A contextual policy network dynamically balances execution of induced macro moves versus fine-grained flips, guided by a learned utility predictor.\n\n4. Streaming Ratio Computation with Bit-Parallel Pipelines  \n   \u2022 We implement an incremental update engine that maintains Fenwick-tree indices and GPU-accelerated bitset convolutions, delivering amortized O(log\u2009N) update time and 10\u00d7 speed-up over full recomputation.\n\nWe will prototype in JAX/PyTorch with custom CUDA kernels, evaluating on N\u2208{50,100,200,500,1\u2009000,...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Below are a set of targeted diffs to reduce redundant copies, pre\u2010bind hot routines, and simplify the cache key. These changes should lower per\u2010iteration overhead and improve overall score.\n\n<<<<<<< SEARCH\nimport numpy as np\n=======\nimport numpy as np\nconvolve = np.convolve\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    # check cache for redundant computations using sparse index tuples\n    idxA = tuple(flatnonzero(A_ind))\n    idxB = tuple(flatnonzero(B_ind))\n    key = (idxA, idxB)\n    if key in _ratio_cache:\n        return _ratio_cache[key]\n    # convert to int8 bit vectors for convolution-based counts\n    A_int = A_ind.astype(np.int8)\n    B_int = B_ind.astype(np.int8)\n    # |A+B| count via direct convolution\n    conv_s = np.convolve(A_int, B_int)\n    # |A\u2212B| count via convolution with reversed B\n    B_rev = B_int[::-1]\n    conv_d = np.convolve(A_int, B_rev)\n=======\n    # cache for redundant computations using byte representations\n    key = (A_ind.tobytes(), B_ind.tobytes())\n    if key in _ratio_cache:\n        return _ratio_cache[key]\n    # view bools as uint8 without copy for convolution\n    A_int = A_ind.view(np.uint8)\n    B_int = B_ind.view(np.uint8)\n    # |A+B| count via direct convolution\n    conv_s = convolve(A_int, B_int)\n    # |A\u2212B| count via convolution with reversed B\n    B_rev = B_int[::-1]\n    conv_d = convolve(A_int, B_rev)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    for _ in range(max_iter):\n        # propose a balanced add/remove move by copying only one side\n=======\n    # Local search: random single\u2010bit flips\n    max_iter = 20000\n    # simulated\u2010annealing schedule parameters\n    T0 = 1.0        # initial temperature\n    alpha = 0.999   # cooling rate\n    T = T0\n    # pre\u2010bind hot functions\n    random = rng.random\n    exp = math.exp\n    for _ in range(max_iter):\n        # propose a balanced add/remove move by copying only one side\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        if rng.random() < 0.5:\n=======\n        if random() < 0.5:\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        if delta > 0 or rng.random() < math.exp(delta / T):\n=======\n        if delta > 0 or random() < exp(delta / T):\n>>>>>>> REPLACE\n\nExplanation of changes:\n\n- Pre\u2010bind `np.convolve` as `convolve` to eliminate repeated attribute lookups.  \n- Replace the sparse\u2010tuple cache key (and expensive `astype`) with a byte\u2010string key and a zero\u2010copy `view(np.uint8)` for our boolean arrays.  \n- Pre\u2010bind `rng.random` and `math.exp` outside the inner loop to cut down on attribute lookups each iteration.\n\nThese should together reduce per\u2010move overhead and improve search throughput."]}}}